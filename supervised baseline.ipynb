{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "from create_dataset import AnimalDatasetEmbedding, UnpairedEmbeddingsDataset\n",
    "from generators import  OneHotGenerator, SkipOneHotGenerator, SkipTensorEmbeddingGen, TensorEmbeddingGen, OneHotResNetGenerator\n",
    "from discriminators import PatchDiscriminator, SampleDiscriminator\n",
    "import os\n",
    "import glob\n",
    "import generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = [1,2,3,4,5]\n",
    "UNPAIRED = False \n",
    "SUPERVISED = True\n",
    "\n",
    "\n",
    "UNET = True\n",
    "SKIPCONNECTIONS = False \n",
    "\n",
    "RESNET = False\n",
    "\n",
    "EMBEDDING = True           # Use intervention and phase information during training\n",
    "if EMBEDDING == True:\n",
    "    DOWN = False\n",
    "    BOTTLENECK = True\n",
    "ONEHOTENCODING = True\n",
    "\n",
    "PATCH = True                # False = Patch Discriminator, True = Sample Discriminator\n",
    "SMALLER_TARGET = True       # True = train with target dataset size = PERCENTAGE\n",
    "PERCENTAGE = 0.5            # percentage of target data\n",
    "\n",
    "\n",
    "# LR scheduler\n",
    "MultiStepLR = False\n",
    "GAMMA = 0.1\n",
    "\n",
    "ReduceLROnPlateau = False\n",
    "FACTOR = 0.001\n",
    "PATIENCE = 2\n",
    "\n",
    "PolinomialLR = True\n",
    "power = 1\n",
    "LR_DECAY_AFTER_EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0002 \n",
    "NUM_WORKERS = 16\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] â€“ Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"  # RVtot_kalibriert existiert auch\n",
    "source_signals = [SIG_D]\n",
    "CHANNELS = len(source_signals)\n",
    "WINDOW = 256\n",
    "\n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "\n",
    "# Use adversarial loss\n",
    "LAMBDA_DISC = 0.05\n",
    "GAN_LOSS = True   # adversarial loss\n",
    "LAMBDA_GAN = 1.0\n",
    "# Use cycle consistency loss\n",
    "CYCLE = False\n",
    "LAMBDA_CYCLE = 1.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = True \n",
    "LAMBDA_SUPERVISED = 5.0\n",
    "# Use Identity loss\n",
    "IDENTITY = False\n",
    "LAMBDA_IDENTITY = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files\" \n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "df = pd.DataFrame()\n",
    "scaler = StandardScaler() \n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df_temp = pd.read_csv(f, sep=\";\")\n",
    "    df_temp = utils.drop_cols(df_temp)\n",
    "    df_temp = df_temp.dropna()\n",
    "    df_temp = utils.remove_strings(df_temp)\n",
    "    df_temp = utils.subsample(df_temp, 10)\n",
    "    df_temp = utils.normalize(df_temp, scaler, phase1 = True)  \n",
    "      \n",
    "    # print the content\n",
    "    df = pd.concat([df, df_temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which phases to use\n",
    "df = df.loc[df['Phasenzuordnung'].isin(PHASES)]\n",
    "print('Size of the dataset',df.shape)\n",
    "\n",
    "# print('Size of the dataset with data from phase 1',df.shape)\n",
    "# print('Size of Phase 1: ', df.loc[df['Phasenzuordnung'] == 1].shape)\n",
    "# print('Size of Phase 2: ', df.loc[df['Phasenzuordnung'] == 2].shape)\n",
    "# print('Size of Phase 3: ', df.loc[df['Phasenzuordnung'] == 3].shape)\n",
    "# print('Size of Phase 4: ', df.loc[df['Phasenzuordnung'] == 4].shape)\n",
    "# print('SIze of Phase 5: ', df.loc[df['Phasenzuordnung'] == 5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Phasenzuordnung'] == 1:\n",
    "        df.at[index, 'intervention'] = 0\n",
    "    elif row['intervention'] == 10:\n",
    "        if row['contractility'] == 1.0:\n",
    "            df.at[index, 'intervention'] = 0      # contractility = 1.0 - could be ignored? - phase 0?\n",
    "        if row['contractility'] == 3.0:\n",
    "            df.at[index, 'intervention'] = 5      # contractility = 3.0                                        \n",
    "        if row['contractility'] == 4.0:\n",
    "            df.at[index, 'intervention'] = 6      # contractility = 4.0\n",
    "\n",
    "\n",
    "#remove rows with intervention 2, 4 and 6 -> nearly no data\n",
    "df = df[df.intervention != 2]\n",
    "df = df[df.intervention != 4]\n",
    "df = df[df.intervention != 5]\n",
    "print(df['intervention'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['animal'].unique()))\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('Number of animals after removing those with less than 10 data points: ', len(df['animal'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select animals 3,4,8,11,17 as test animals\n",
    "test_animals = [3,4,8,11,17]\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# change the length of the test data to a multiple of the Window size\n",
    "df_test = df_test.iloc[:len(df_test) - (len(df_test) % WINDOW)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random number between 0 and Percentage\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# get random number between 0 and Percentage (PERCENTAGE refers to the target data set size)\n",
    "random_number = random.uniform(0, PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERCENTAGE == 0.001:\n",
    "    \n",
    "\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "    for animal in df_train['animal'].unique():\n",
    "        for phase in df_train['Phasenzuordnung'].unique():\n",
    "            df_temp = df_train.loc[(df_train['animal'] == animal) & (df_train['Phasenzuordnung'] == phase)]\n",
    "            # get random window\n",
    "            random_number = int(random.uniform(0, len(df_temp)))\n",
    "            df_temp = df_temp.iloc[random_number:256+random_number]\n",
    "            df_target = pd.concat([df_target, df_temp], axis=0)\n",
    "            if df_train.shape[0] >= 100/PERCENTAGE * df_target.shape[0]:\n",
    "                break\n",
    "\n",
    "# print(df_train.shape[0] / df_target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERCENTAGE > 0.001:\n",
    "\n",
    "    df_target = pd.DataFrame()\n",
    "\n",
    "    for animal in df_train['animal'].unique():\n",
    "        for phase in df_train['Phasenzuordnung'].unique():\n",
    "            df_temp = df_train.loc[(df_train['animal'] == animal) & (df_train['Phasenzuordnung'] == phase)]\n",
    "            df_temp = df_temp.iloc[:int(len(df_temp)*PERCENTAGE)]\n",
    "            df_target = pd.concat([df_target, df_temp], axis=0)\n",
    "\n",
    "# print(df_train.shape[0] / df_target.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "if UNET:\n",
    "    if ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = OneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = OneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipOneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipOneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipTensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipTensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = TensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = TensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "if not UNET:\n",
    "    if ONEHOTENCODING:\n",
    "        gen_target = OneHotResNetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS= 1, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "        gen_source = OneHotResNetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS= CHANNELS, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "if PATCH:\n",
    "    disc_target = PatchDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = PatchDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "if not PATCH:\n",
    "    disc_target = SampleDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = SampleDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# Optimizers \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_source.parameters()) + list(disc_target.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_source.parameters()) + list(gen_target.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "if ReduceLROnPlateau:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_gen,\n",
    "                                                           factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                           min_lr=1e-6,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_disc,\n",
    "                                                            factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                            min_lr=1e-6,\n",
    "                                                    )\n",
    "if MultiStepLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_gen, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "                                                        \n",
    "    disc_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_disc, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "\n",
    "if PolinomialLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = power,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = power,\n",
    "                                                    )\n",
    "\n",
    "# losses\n",
    "l1 = nn.L1Loss() \n",
    "mse = nn.MSELoss() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_GEN_target = 'Checkpoints/Supervised/exp_1_all_data_gen_target.pth.tar'\n",
    "# CHECKPOINT_GEN_source = 'Checkpoints/Supervised/exp_1_all_data_gen_source.pth.tar'\n",
    "\n",
    "# utils.load_checkpoint(CHECKPOINT_GEN_target, gen_target, opt_gen, LEARNING_RATE)\n",
    "# utils.load_checkpoint(CHECKPOINT_GEN_source, gen_source, opt_gen, LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUPERVISED:\n",
    "    train_dataset = AnimalDatasetEmbedding(df_target, source_signals, target_name = TARGET,  window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc, reals, fakes):\n",
    "    # calculate how close reals are to being classified as real\n",
    "    real_loss = mse(disc(reals), torch.ones_like(disc(reals)))\n",
    "    # calculate how close fakes are to being classified as fake\n",
    "    fake_loss = mse(disc(fakes), torch.zeros_like(disc(fakes)))\n",
    "    # return the average of real and fake loss\n",
    "    return (real_loss + fake_loss) \n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # discriminator loss\n",
    "    disc_target_loss = discriminator_loss(disc_target, target, fake_target) * LAMBDA_DISC\n",
    "    disc_source_loss = discriminator_loss(disc_source, source, fake_source) * LAMBDA_DISC\n",
    "    disc_loss = (disc_source_loss + disc_target_loss) #/ 2\n",
    "\n",
    "    return disc_loss, disc_source_loss, disc_target_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for source, target, source_phase, source_intervention, target_phase, target_intervention in loader:\n",
    "        # convert to float16\n",
    "        source = source.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        target = target.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        source_phase = source_phase.to(DEVICE)\n",
    "        source_intervention = source_intervention.to(DEVICE)\n",
    "        target_phase = target_phase.to(DEVICE)\n",
    "        target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "        #  ------------------------------- #\n",
    "        #  ----- train discriminators ---- #\n",
    "        #  ------------------------------- #\n",
    "        with torch.no_grad():\n",
    "            fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "            fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "\n",
    "        d_loss, disc_source_loss, disc_target_loss = get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target)\n",
    "\n",
    "        # update gradients of discriminator \n",
    "        opt_disc.zero_grad() \n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "\n",
    "        g_loss = 0\n",
    "\n",
    "        g_source_loss = mse(disc_source(fake_source), torch.ones_like(disc_source(fake_source))) \n",
    "        g_target_loss = mse(disc_target(fake_target), torch.ones_like(disc_target(fake_target))) \n",
    "\n",
    "        g_loss += g_source_loss * LAMBDA_GAN + g_target_loss * LAMBDA_GAN\n",
    "        \n",
    "        sup_source_loss = l1(gen_source(target, source_phase, source_intervention, target_phase, target_intervention), source)\n",
    "        sup_target_loss = l1(gen_target(source, source_phase, source_intervention, target_phase, target_intervention), target)\n",
    "\n",
    "        g_loss += sup_source_loss * LAMBDA_SUPERVISED + sup_target_loss * LAMBDA_SUPERVISED\n",
    "    \n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "    if ReduceLROnPlateau == True:  \n",
    "        disc_scheduler.step(d_loss)\n",
    "        gen_scheduler.step(g_loss)\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if PolinomialLR == True and (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "\n",
    "disc_source.eval()  # set discriminator to evaluation mode\n",
    "disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "gen_target.eval()\n",
    "gen_source.eval()\n",
    "\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate samples\n",
    "idx = 0\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "    if idx == 5:\n",
    "        break              \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "\n",
    "    l1_source_loss = l1(source, fake_source)\n",
    "    l1_target_loss = l1(target, fake_target)\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source[:256].cpu().detach().numpy(), label= 'Real source sample')\n",
    "    ax[0].plot(fake_source[:256].cpu().detach().numpy(), label= 'Generated source sample')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target[:256].cpu().detach().numpy(), label= 'Real target sample')\n",
    "    ax[1].plot(fake_target[:256].cpu().detach().numpy(), label= 'Generated target sample')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    print(l1_target_loss, l1_source_loss)\n",
    "\n",
    "    idx += 1    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify L1 loss of reconstructed and real signals, per phase and intervention\n",
    "\n",
    "df_phase_1 = df_test.loc[df_test['Phasenzuordnung'].isin([1])]\n",
    "df_phase_2 = df_test.loc[df_test['Phasenzuordnung'].isin([2])]\n",
    "df_phase_3 = df_test.loc[df_test['Phasenzuordnung'].isin([3])]\n",
    "df_phase_4 = df_test.loc[df_test['Phasenzuordnung'].isin([4])]\n",
    "df_phase_5 = df_test.loc[df_test['Phasenzuordnung'].isin([5])]\n",
    "\n",
    "df_int_0 = df_test.loc[df_test['intervention'].isin([0])]\n",
    "df_int_1 = df_test.loc[df_test['intervention'].isin([1])]\n",
    "df_int_2 = df_test.loc[df_test['intervention'].isin([2])]\n",
    "df_int_3 = df_test.loc[df_test['intervention'].isin([3])]\n",
    "df_int_4 = df_test.loc[df_test['intervention'].isin([4])]\n",
    "df_int_5 = df_test.loc[df_test['intervention'].isin([5])]\n",
    "df_int_6 = df_test.loc[df_test['intervention'].isin([6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_dataset = AnimalDatasetEmbedding(df_int_6, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "verify_loader = DataLoader(verify_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "\n",
    "disc_source.eval()  # set discriminator to evaluation mode\n",
    "disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "gen_target.eval()\n",
    "gen_source.eval()\n",
    "\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in verify_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\\\n",
    "        \n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
