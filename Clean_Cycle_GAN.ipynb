{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "from create_dataset import AnimalDatasetEmbedding, UnpairedEmbeddingsDataset\n",
    "from generators import  OneHotGenerator, SkipOneHotGenerator, SkipTensorEmbeddingGen, TensorEmbeddingGen, OneHotResNetGenerator\n",
    "from discriminators import PatchDiscriminator, SampleDiscriminator\n",
    "import os\n",
    "import glob\n",
    "import generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "PHASES = [1, 3]\n",
    "UNPAIRED = True \n",
    "\n",
    "\n",
    "UNET = False\n",
    "SKIPCONNECTIONS = False \n",
    "\n",
    "RESNET = True\n",
    "\n",
    "EMBEDDING = True            # Use intervention and phase information during training\n",
    "if EMBEDDING == True:\n",
    "    DOWN = False\n",
    "    BOTTLENECK = True\n",
    "ONEHOTENCODING = True\n",
    "\n",
    "PATCH = False          # False = Patch Discriminator, True = Sample Discriminator\n",
    "SMALLER_TARGET = False   # True = train with target dataset size = PERCENTAGE\n",
    "PERCENTAGE = 0.5        # percentage of target data\n",
    "\n",
    "\n",
    "# LR scheduler\n",
    "MultiStepLR = False\n",
    "GAMMA = 0.1\n",
    "\n",
    "ReduceLROnPlateau = False\n",
    "FACTOR = 0.001\n",
    "PATIENCE = 2\n",
    "\n",
    "PolinomialLR = True\n",
    "power = 1\n",
    "LR_DECAY_AFTER_EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0002 #1e-2 \n",
    "NUM_WORKERS = 16\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] â€“ Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"  # RVtot_kalibriert existiert auch\n",
    "source_signals = [SIG_D]\n",
    "CHANNELS = len(source_signals)\n",
    "WINDOW = 256\n",
    "\n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "\n",
    "# Use adversarial loss\n",
    "LAMBDA_DISC = 0.05\n",
    "GAN_LOSS = True   # adversarial loss\n",
    "LAMBDA_GAN = 1.0\n",
    "# Use cycle consistency loss\n",
    "CYCLE = True\n",
    "LAMBDA_CYCLE = 1.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = False \n",
    "LAMBDA_SUPERVISED = 1.0\n",
    "# Use Identity loss\n",
    "IDENTITY = False\n",
    "LAMBDA_IDENTITY = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all data and preprocess\n",
    "\n",
    "Subsamplen, normalisieren pro Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6022044, 14)\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files\" \n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "df = pd.DataFrame()\n",
    "scaler = StandardScaler() \n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df_temp = pd.read_csv(f, sep=\";\")\n",
    "    df_temp = utils.drop_cols(df_temp)\n",
    "    df_temp = df_temp.dropna()\n",
    "    df_temp = utils.remove_strings(df_temp)\n",
    "    df_temp = utils.subsample(df_temp, 10)\n",
    "    df_temp = utils.normalize(df_temp, scaler, phase1 = True)  \n",
    "      \n",
    "    # print the content\n",
    "    df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select part of data to use in experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset after selecting Phasenzuordnung 1 (2122212, 14)\n",
      "Size of the dataset with data from phase 1 (2122212, 14)\n",
      "Size of Phase 1:  (751120, 14)\n",
      "Size of Phase 2:  (0, 14)\n",
      "Size of Phase 3:  (1371092, 14)\n",
      "Size of Phase 4:  (0, 14)\n",
      "SIze of Phase 5:  (0, 14)\n"
     ]
    }
   ],
   "source": [
    "# select which phases to use\n",
    "df = df.loc[df['Phasenzuordnung'].isin(PHASES)]\n",
    "print('Size of the dataset after selecting Phasenzuordnung 1',df.shape)\n",
    "\n",
    "print('Size of the dataset with data from phase 1',df.shape)\n",
    "print('Size of Phase 1: ', df.loc[df['Phasenzuordnung'] == 1].shape)\n",
    "print('Size of Phase 2: ', df.loc[df['Phasenzuordnung'] == 2].shape)\n",
    "print('Size of Phase 3: ', df.loc[df['Phasenzuordnung'] == 3].shape)\n",
    "print('Size of Phase 4: ', df.loc[df['Phasenzuordnung'] == 4].shape)\n",
    "print('SIze of Phase 5: ', df.loc[df['Phasenzuordnung'] == 5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 3. 6. 2. 4. 5.]\n",
      "Size of intervention 1:  (553750, 14)\n",
      "Size of intervention 2:  (9601, 14)\n",
      "Size of intervention 3:  (522847, 14)\n",
      "Size of intervention 4:  (12915, 14)\n",
      "Size of intervention 5:  (8126, 14)\n",
      "Size of intervention 6:  (217192, 14)\n",
      "[0. 1. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Phasenzuordnung'] == 1:\n",
    "        df.at[index, 'intervention'] = 0\n",
    "    elif row['intervention'] == 10:\n",
    "        if row['contractility'] == 1.0:\n",
    "            df.at[index, 'intervention'] = 0      # contractility = 1.0 - could be ignored? - phase 0?\n",
    "        if row['contractility'] == 3.0:\n",
    "            df.at[index, 'intervention'] = 5      # contractility = 3.0                                        \n",
    "        if row['contractility'] == 4.0:\n",
    "            df.at[index, 'intervention'] = 6    # contractility = 4.0\n",
    "\n",
    "#get unique intervention\n",
    "print(df['intervention'].unique())\n",
    "\n",
    "int_1 = df.loc[df['intervention'] == 1]\n",
    "int_2 = df.loc[df['intervention'] == 2]\n",
    "int_3 = df.loc[df['intervention'] == 3]\n",
    "int_4 = df.loc[df['intervention'] == 4]\n",
    "int_5 = df.loc[df['intervention'] == 5]\n",
    "int_6 = df.loc[df['intervention'] == 6]\n",
    "\n",
    "print('Size of intervention 1: ', int_1.shape)\n",
    "print('Size of intervention 2: ', int_2.shape)\n",
    "print('Size of intervention 3: ', int_3.shape)\n",
    "print('Size of intervention 4: ', int_4.shape)\n",
    "print('Size of intervention 5: ', int_5.shape)\n",
    "print('Size of intervention 6: ', int_6.shape)\n",
    "\n",
    "#remove rows with intervention 2, 4 and 6, nearly no data\n",
    "df = df[df.intervention != 2]\n",
    "df = df[df.intervention != 4]\n",
    "df = df[df.intervention != 5]\n",
    "print(df['intervention'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop animals with less than 10 data points\n",
    "We drop animals from the dataframe, if they have less than 10 data points. Initially, we have 56 animals and after dropping those with close to no data points, we are left with 25 animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Number of animals after removing those with less than 10 data points:  25\n",
      "[  8  12  13 108   7  14  11   2  16  10 105 111  17  19   3  15   5  20\n",
      " 113   4 102   1   9 107 101]\n"
     ]
    }
   ],
   "source": [
    "print(len(df['animal'].unique()))\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('Number of animals after removing those with less than 10 data points: ', len(df['animal'].unique()))\n",
    "\n",
    "# get all differnent animals\n",
    "animals = df['animal'].unique()\n",
    "print(animals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test split\n",
    "\n",
    "The 5 test animals represent 20.20924% of the whole data. If we don't use all phases, this number might be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test animal(s): [3, 4, 8, 11, 17]\n",
      "\n",
      "Different animal IDs after removing those that are in the test dataset:  20\n",
      "\n",
      "Train data shape: (1484447, 14)\n",
      "\n",
      "Test data shape: (606976, 14)\n",
      "\n",
      "The test dataset is 29.022153815846913 percent of the whole data: \n"
     ]
    }
   ],
   "source": [
    "# select animals 3,4,8,11,17 as test animals\n",
    "test_animals = [3,4,8,11,17]\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# change the length of the test data to a multiple of the Window size\n",
    "df_test = df_test.iloc[:len(df_test) - (len(df_test) % WINDOW)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create target dataframe with a fraction of dfs data\n",
    "\n",
    "Option 1: 10% of data of each animal -> most likely no data from the last phases\n",
    "\n",
    "Option 2: Take first 10% of data of each phase per animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of df_target: (742215, 14)\n",
      "\n",
      "Number of animals in df_target: 20\n",
      "\n",
      "Number of phases in df_target: 2\n",
      "2.000022904414489\n"
     ]
    }
   ],
   "source": [
    "# get first 10% of data of each phase per animal\n",
    "df_target = pd.DataFrame()\n",
    "\n",
    "for animal in df_train['animal'].unique():\n",
    "    for phase in df_train['Phasenzuordnung'].unique():\n",
    "        df_temp = df_train.loc[(df_train['animal'] == animal) & (df_train['Phasenzuordnung'] == phase)]\n",
    "        df_temp = df_temp.iloc[:int(len(df_temp)*PERCENTAGE)]\n",
    "        df_target = pd.concat([df_target, df_temp], axis=0)\n",
    "\n",
    "print('\\nShape of df_target:', df_target.shape)\n",
    "print('\\nNumber of animals in df_target:', len(df_target['animal'].unique()))\n",
    "print('\\nNumber of phases in df_target:', len(df_target['Phasenzuordnung'].unique()))\n",
    "print(df_train.shape[0] / df_target.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Generator and Discriminator\n",
    "\n",
    "We also initialize the 2 optimizers, the 2 Learning rate schedulers, the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def double_conv_pad(in_channels, out_channels):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "#         nn.BatchNorm1d(out_channels),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Dropout1d(p=0.1, inplace=False),\n",
    "#         nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "#         nn.BatchNorm1d(out_channels),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Dropout1d(p=0.1, inplace=False),\n",
    "#     )\n",
    "\n",
    "\n",
    "# class TensorEmbeddingGen(nn.Module):\n",
    "#     def __init__(self, INPUTCHANNELS, OUTPUTCHANNELS, Down = True, Bottleneck = True):\n",
    "#         super(TensorEmbeddingGen, self).__init__()\n",
    "#         self.Down = Down\n",
    "#         self.Bottleneck = Bottleneck\n",
    "#         self.maxpool = nn.MaxPool1d((2))  \n",
    "\n",
    "#         self.source_intervention = torch.nn.Embedding(num_embeddings=8, embedding_dim=256)\n",
    "#         self.source_phase = torch.nn.Embedding(num_embeddings=7, embedding_dim=256)\n",
    "\n",
    "#         self.down_conv1 = double_conv_pad(INPUTCHANNELS, 32) \n",
    "#         self.down_conv2 = double_conv_pad(32, 64) \n",
    "#         self.down_conv3 = double_conv_pad(64, 128)\n",
    "#         self.down_conv4 = double_conv_pad(128, 256)\n",
    "\n",
    "#         self.target_intervention = torch.nn.Embedding(num_embeddings=8, embedding_dim=32)\n",
    "#         self.target_phase = torch.nn.Embedding(num_embeddings=7, embedding_dim=32)\n",
    "\n",
    "#         self.up_trans1 = nn.ConvTranspose1d(256, 128, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv1 = double_conv_pad(128, 128)\n",
    "#         self.up_trans2 = nn.ConvTranspose1d(128, 64, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv2 = double_conv_pad(64, 64)\n",
    "#         self.up_trans3 = nn.ConvTranspose1d(64, 32, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv3 = double_conv_pad(32, 32)\n",
    "\n",
    "#         self.out = nn.Conv1d(32, OUTPUTCHANNELS, kernel_size=1) # kernel_size must be == 1\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "        \n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Conv1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.ConvTranspose1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#         elif isinstance(module, nn.BatchNorm1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Linear):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "\n",
    "#     def forward(self, input, source_phase, source_intervention, target_phase, target_intervention):\n",
    "#         if self.Down:\n",
    "#             print(input.shape)\n",
    "#             sp = self.source_phase(source_phase)\n",
    "#             print(sp.shape)\n",
    "#             si = self.source_intervention(source_intervention)\n",
    "#             print(si.shape)\n",
    "#             input = input + sp + si \n",
    "#         x1 = self.down_conv1(input) \n",
    "#         x2 = self.maxpool(x1) \n",
    "#         x3 = self.down_conv2(x2)\n",
    "#         x4 = self.maxpool(x3) \n",
    "#         x5 = self.down_conv3(x4) \n",
    "#         x6 = self.maxpool(x5)  \n",
    "#         x7 = self.down_conv4(x6)\n",
    "\n",
    "#         # # decoder\n",
    "#         if self.Bottleneck:\n",
    "#             tp = self.target_phase(target_phase)  \n",
    "#             ti = self.target_intervention(target_intervention)\n",
    "#             x7 = x7 + tp + ti # target embeddings are added before upsampling  # lieber concatenating und dann fully connected to amtch the dimension\n",
    "#         x = self.up_trans1(x7)\n",
    "#         x = self.up_conv1(x)\n",
    "#         x = self.up_trans2(x)\n",
    "#         x = self.up_conv2(x)\n",
    "#         x = self.up_trans3(x)\n",
    "#         x = self.up_conv3(x)\n",
    "#         x = self.out(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_block(in_channels, out_channels, kernel_size , stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size, stride),\n",
    "        nn.BatchNorm1d(out_channels),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Dropout1d(p=0.1, inplace=False),\n",
    "    )\n",
    "\n",
    "def Conv_block_no_norm(in_channels, out_channels, kernel_size , stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size, stride),\n",
    "        #nn.BatchNorm1d(out_channels),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Dropout1d(p=0.1, inplace=False),\n",
    "    )\n",
    "\n",
    "class SampleDiscriminator(nn.Module):\n",
    "    def __init__(self, CHANNELS):\n",
    "        super(SampleDiscriminator, self).__init__()\n",
    "\n",
    "        \n",
    "        self.conv1 = Conv_block(in_channels= CHANNELS, out_channels= 32, kernel_size = 4, stride = 3) \n",
    "        self.conv2 = Conv_block(in_channels= 32, out_channels= 64, kernel_size = 4, stride = 3)       \n",
    "        self.conv3 = Conv_block(in_channels= 64, out_channels= 128, kernel_size = 4, stride = 3)      \n",
    "        self.conv4 = Conv_block(in_channels= 128, out_channels= 256, kernel_size = 3, stride = 2)     \n",
    "        self.conv5 = Conv_block_no_norm(in_channels= 256, out_channels= 1, kernel_size = 4, stride = 2) \n",
    "        \n",
    "        self.out = nn.Sigmoid()             \n",
    "\n",
    "\n",
    "    # input shape\n",
    "    # torch.Size([1, 1, 256])\n",
    "    # torch.Size([1, 32, 85])\n",
    "    # torch.Size([1, 64, 28])\n",
    "    # torch.Size([1, 128, 9])\n",
    "    # torch.Size([1, 256, 4])\n",
    "    # torch.Size([1, 1, 1])\n",
    "    # torch.Size([1, 1, 1])\n",
    "\n",
    "    def forward(self, input):    \n",
    "        # input: torch.Size([1, 1, 256]) \n",
    "        x = self.conv1(input)  # torch.Size([1, 32, 85])       \n",
    "        x = self.conv2(x)      # torch.Size([1, 64, 28])     \n",
    "        x = self.conv3(x)      # torch.Size([1, 128, 9])      \n",
    "        x = self.conv4(x)      # torch.Size([1, 256, 4]) \n",
    "        x = self.conv5(x)      # torch.Size([1, 1, 1])     \n",
    "        x = self.out(x)        # torch.Size([1, 1, 1])     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "if UNET:\n",
    "    if ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = OneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = OneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipOneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipOneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipTensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipTensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = TensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = TensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "if not UNET:\n",
    "    if ONEHOTENCODING:\n",
    "        gen_target = OneHotResNetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS= 1, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "        gen_source = OneHotResNetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS= CHANNELS, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "if PATCH:\n",
    "    disc_target = PatchDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = PatchDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "if not PATCH:\n",
    "    disc_target = SampleDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = SampleDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# Optimizers \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_source.parameters()) + list(disc_target.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_source.parameters()) + list(gen_target.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "if ReduceLROnPlateau:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_gen,\n",
    "                                                           factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                           min_lr=1e-6,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_disc,\n",
    "                                                            factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                            min_lr=1e-6,\n",
    "                                                    )\n",
    "if MultiStepLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_gen, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "                                                        \n",
    "    disc_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_disc, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "\n",
    "if PolinomialLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = power,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = power,\n",
    "                                                    )\n",
    "\n",
    "# losses\n",
    "l1 = nn.L1Loss() \n",
    "mse = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnequalDataSetSize(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, target_df, feature_names, target_name, window_length=256):\n",
    "        self.df = df\n",
    "        self.feature_names = feature_names\n",
    "        self.target_name = target_name\n",
    "        self.window_length = window_length\n",
    "        self.target_df = target_df\n",
    "        \n",
    "        self.num_animals = len(np.unique(df[\"animal\"]))\n",
    "        self.animal_dfs = [group[1] for group in df.groupby(\"animal\")]\n",
    "        # get statistics for test dataset\n",
    "        self.animal_lens = [len(an_df) // self.window_length for an_df in self.animal_dfs]\n",
    "        self.animal_cumsum = np.cumsum(self.animal_lens)\n",
    "        self.num_windows = sum(self.animal_lens)\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_windows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        animal_idx = int(np.where(self.animal_cumsum >= idx)[0][0])\n",
    "        animal_df = self.animal_dfs[animal_idx]\n",
    "\n",
    "        # take different windows for the source and target -> unpaired examples\n",
    "        start_idx_source = np.random.randint(0, len(animal_df) - self.window_length - 1)\n",
    "        start_idx_target = np.random.randint(0, len(self.target_df) - self.window_length - 1)\n",
    "        end_idx_source = start_idx_source + self.window_length\n",
    "        end_idx_target = start_idx_target + self.window_length\n",
    "        animal_df_source = animal_df.iloc[start_idx_source: end_idx_source]\n",
    "        animal_df_target = self.target_df.iloc[start_idx_target: end_idx_target]\n",
    "\n",
    "        # extract features\n",
    "        input_df = animal_df_source[self.feature_names]\n",
    "        target_df = animal_df_target[self.target_name]\n",
    "        phase_df_source = animal_df_source[\"Phasenzuordnung\"]   # mit 'Phasenzuordnung' klappt es\n",
    "        intervention_df_source = animal_df_source[\"intervention\"]\n",
    "        phase_df_target = animal_df_target[\"Phasenzuordnung\"]\n",
    "        intervention_df_target = animal_df_target[\"intervention\"]\n",
    "\n",
    "        # to torch\n",
    "        inputs = torch.tensor(input_df.to_numpy()).permute(1, 0)\n",
    "        targets = torch.tensor(target_df.to_numpy()).unsqueeze(0)\n",
    "        phase_source = torch.tensor(phase_df_source.to_numpy()).type(torch.LongTensor)\n",
    "        intervention_source = torch.tensor(intervention_df_source.to_numpy()).type(torch.LongTensor)\n",
    "        phase_target = torch.tensor(phase_df_target.to_numpy()).type(torch.LongTensor)\n",
    "        intervention_target = torch.tensor(intervention_df_target.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "        return inputs, targets, phase_source, intervention_source, phase_target, intervention_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SMALLER_TARGET and UNPAIRED and EMBEDDING:\n",
    "    # create dataset with information of the phases and intervention (embedding information)\n",
    "    train_dataset = UnpairedEmbeddingsDataset(df_train, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "if not SMALLER_TARGET and not UNPAIRED and EMBEDDING:\n",
    "    train_dataset = AnimalDatasetEmbedding(df_train, source_signals, target_name = TARGET,  window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "if SMALLER_TARGET and UNPAIRED and EMBEDDING:\n",
    "    train_dataset = UnequalDataSetSize(df_train, df_target, source_signals, target_name = TARGET, window_length=WINDOW)\n",
    "    test_dataset = UnequalDataSetSize(df_test, df_target, source_signals, target_name = TARGET, window_length=WINDOW)\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/johann/Desktop/GitHub repos/Master_thesis/wandb/run-20230719_230838-eqcvk2bd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johannanton/Cycle_GAN/runs/eqcvk2bd' target=\"_blank\">sweet-waterfall-265</a></strong> to <a href='https://wandb.ai/johannanton/Cycle_GAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johannanton/Cycle_GAN' target=\"_blank\">https://wandb.ai/johannanton/Cycle_GAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johannanton/Cycle_GAN/runs/eqcvk2bd' target=\"_blank\">https://wandb.ai/johannanton/Cycle_GAN/runs/eqcvk2bd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/johannanton/Cycle_GAN/runs/eqcvk2bd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f12553aa820>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Cycle_GAN\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_signals(fake_target, fake_source, target, source):\n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "def discriminator_loss(disc, reals, fakes):\n",
    "    # calculate how close reals are to being classified as real\n",
    "    real_loss = mse(disc(reals), torch.ones_like(disc(reals)))\n",
    "    # calculate how close fakes are to being classified as fake\n",
    "    fake_loss = mse(disc(fakes), torch.zeros_like(disc(fakes)))\n",
    "    # return the average of real and fake loss\n",
    "    return (real_loss + fake_loss) \n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # discriminator loss\n",
    "    disc_target_loss = discriminator_loss(disc_target, target, fake_target) * LAMBDA_DISC\n",
    "    disc_source_loss = discriminator_loss(disc_source, source, fake_source) * LAMBDA_DISC\n",
    "    disc_loss = (disc_source_loss + disc_target_loss) #/ 2\n",
    "\n",
    "    return disc_loss, disc_source_loss, disc_target_loss\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                  gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\n",
    "                  ):\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    if GAN_LOSS:\n",
    "        g_source_loss = mse(disc_source(fake_source), torch.ones_like(disc_source(fake_source))) \n",
    "        g_target_loss = mse(disc_target(fake_target), torch.ones_like(disc_target(fake_target))) \n",
    "\n",
    "        loss += g_source_loss * LAMBDA_GAN + g_target_loss * LAMBDA_GAN\n",
    "    else:\n",
    "        g_source_loss = torch.tensor(0)\n",
    "        g_target_loss = torch.tensor(0)\n",
    "\n",
    "    if CYCLE:\n",
    "        rec_target = gen_target(fake_source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "        rec_source = gen_source(fake_target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "        cycle_target_loss = l1(target, rec_target)  # l1 loss: Mean absolute error between each element in the input x and target y\n",
    "        cycle_source_loss = l1(source, rec_source)  # l1 loss in cycle GAN paper\n",
    "\n",
    "        loss += cycle_target_loss * LAMBDA_CYCLE + cycle_source_loss * LAMBDA_CYCLE\n",
    "    else:\n",
    "        cycle_target_loss = torch.tensor(0)\n",
    "        cycle_source_loss = torch.tensor(0)\n",
    "\n",
    "    if SUPERVISED:\n",
    "        sup_source_loss = mse(source, fake_source)\n",
    "        sup_target_loss = mse(target, fake_target)\n",
    "\n",
    "        loss += sup_source_loss * LAMBDA_SUPERVISED + sup_target_loss * LAMBDA_SUPERVISED\n",
    "    else:\n",
    "        sup_source_loss = torch.tensor(0)\n",
    "        sup_target_loss = torch.tensor(0)\n",
    "\n",
    "    if IDENTITY:\n",
    "        id_target_loss = l1(target, gen_target(target, source_phase, source_intervention, target_phase, target_intervention))\n",
    "        id_source_loss = l1(source, gen_source(source, source_phase, source_intervention, target_phase, target_intervention))\n",
    "\n",
    "        loss += id_target_loss * LAMBDA_IDENTITY + id_source_loss * LAMBDA_IDENTITY\n",
    "    else:\n",
    "        id_target_loss = torch.tensor(0)\n",
    "        id_source_loss = torch.tensor(0)\n",
    "\n",
    "    return loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [21:39<35:44:57, 649.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/johann/Desktop/GitHub repos/Master_thesis/Clean_Cycle_GAN.ipynb Cell 25\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m opt_disc\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# d_scaler.scale(d_loss).backward()  \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m        \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# -------------------------------- #\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# ------- train generators ------- #\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# -------------------------------- # \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m out \u001b[39m=\u001b[39m calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                         gen_source, gen_target, disc_source, disc_target, fake_target, fake_source)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                     \u001b[39m# source, target, source_phase, source_intervention, target_phase, target_intervention,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                     \u001b[39m# gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m g_loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss \u001b[39m=\u001b[39m out\n",
      "\u001b[1;32m/home/johann/Desktop/GitHub repos/Master_thesis/Clean_Cycle_GAN.ipynb Cell 25\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mif\u001b[39;00m CYCLE:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     rec_target \u001b[39m=\u001b[39m gen_target(fake_source, source_phase, source_intervention, target_phase, target_intervention)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     rec_source \u001b[39m=\u001b[39m gen_source(fake_target, source_phase, source_intervention, target_phase, target_intervention)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     cycle_target_loss \u001b[39m=\u001b[39m l1(target, rec_target)  \u001b[39m# l1 loss: Mean absolute error between each element in the input x and target y\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/johann/Desktop/GitHub%20repos/Master_thesis/Clean_Cycle_GAN.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     cycle_source_loss \u001b[39m=\u001b[39m l1(source, rec_source)  \u001b[39m# l1 loss in cycle GAN paper\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/GitHub repos/Master_thesis/generators.py:726\u001b[0m, in \u001b[0;36mOneHotResNetGenerator.forward\u001b[0;34m(self, input, source_phase, source_intervention, target_phase, target_intervention)\u001b[0m\n\u001b[1;32m    723\u001b[0m     x5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBottleNeckFCLinear(x5)   \u001b[39m# x5.shape =  torch.Size([1, 256, 64])\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# # decoder\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup_trans1(x5)\n\u001b[1;32m    727\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_conv1(x)\n\u001b[1;32m    728\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_trans2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:801\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    797\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    798\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    799\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    800\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose1d(\n\u001b[1;32m    802\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    803\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for source, target, source_phase, source_intervention, target_phase, target_intervention in loader:\n",
    "        # convert to float16\n",
    "        source = source.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        target = target.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        source_phase = source_phase.to(DEVICE)\n",
    "        source_intervention = source_intervention.to(DEVICE)\n",
    "        target_phase = target_phase.to(DEVICE)\n",
    "        target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "        #  ------------------------------- #\n",
    "        #  ----- train discriminators ---- #\n",
    "        #  ------------------------------- #\n",
    "        with torch.no_grad():\n",
    "            fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "            fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "\n",
    "        d_loss, disc_source_loss, disc_target_loss = get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target)\n",
    "                                                        # source, target, disc_source, disc_target, fake_source, fake_target\n",
    "\n",
    "        # update gradients of discriminator \n",
    "        opt_disc.zero_grad() \n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "        # d_scaler.scale(d_loss).backward()  \n",
    "               \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "\n",
    "        out = calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                                gen_source, gen_target, disc_source, disc_target, fake_target, fake_source)\n",
    "                            # source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                            # gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\n",
    "\n",
    "        g_loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss = out\n",
    "        # loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        # g_scaler.scale(g_loss).backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    # d_scaler.step(opt_disc)  \n",
    "    # d_scaler.update()\n",
    "\n",
    "    # g_scaler.step(opt_gen) \n",
    "    # g_scaler.update()\n",
    "\n",
    "    wandb.log({'Train/Discriminator source loss': disc_source_loss.item(),\n",
    "                'Train/Discriminator target loss': disc_target_loss.item(),\n",
    "                'Train/Total Discriminator loss': d_loss.item(),\n",
    "                'Train/Total Generator loss': g_loss.item(),\n",
    "                'Train/Adversarial loss source': g_source_loss.item(),\n",
    "                'Train/Adversarial loss target': g_target_loss.item(),\n",
    "                'Train/Cycle consistency loss source': cycle_source_loss.item(),\n",
    "                'Train/Cycle consistency loss target': cycle_target_loss.item(),\n",
    "                'Train/Supervised loss source': sup_source_loss.item(),\n",
    "                'Train/Supervised loss target': sup_target_loss.item(),\n",
    "                'Learning rate': opt_gen.param_groups[0][\"lr\"],\n",
    "                # 'Train/Identity loss A': id_A_loss.item(),\n",
    "                # 'Train/Identity loss B': id_B_loss.item()\n",
    "                })\n",
    "        \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    # source_losses = []\n",
    "    # target_losses = []\n",
    "    if (epoch+1) % 10 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_source.eval()  # set discriminator to evaluation mode\n",
    "            disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_target.eval()\n",
    "            gen_source.eval()\n",
    "\n",
    "            for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "                # convert to float16\n",
    "                source = source.float()\n",
    "                target = target.float()\n",
    "\n",
    "                # move to GPU\n",
    "                source = source.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "                source_phase = source_phase.to(DEVICE)\n",
    "                source_intervention = source_intervention.to(DEVICE)\n",
    "                target_phase = target_phase.to(DEVICE)\n",
    "                target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "\n",
    "                fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention).detach() # already torch.no_grad()\n",
    "                fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "\n",
    "                # Get validation loss\n",
    "                l1_source = l1(source, fake_source)\n",
    "                l1_target = l1(target, fake_target)\n",
    "                # source_losses.append(l1_source.item())\n",
    "                # target_losses.append(l1_target.item())\n",
    "\n",
    "                \n",
    "                # generate signals during validation\n",
    "                #gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                # test_real_fake_lossB = l1(target, fake_target)   # l1(sig_B, fake_B)\n",
    "                # test_real_fake_lossA = l1(source, fake_source)\n",
    "\n",
    "                #  ------------------------------- #\n",
    "                #  ----- test discriminators ----- #\n",
    "                #  ------------------------------- #\n",
    "\n",
    "                test_d_loss, test_disc_A_loss, test_disc_B_loss = get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target)\n",
    "                \n",
    "                # -------------------------------- #\n",
    "                # ------- test generators -------- #\n",
    "                # -------------------------------- # \n",
    "\n",
    "                out = calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                                gen_source, gen_target, disc_source, disc_target, \n",
    "                                fake_target, fake_source\n",
    "                                )\n",
    "                g_lossT, g_A_lossT, g_B_lossT, cycle_B_lossT, cycle_A_lossT, id_B_lossT, id_A_lossT, sup_A_lossT, sup_B_lossT = out\n",
    "        \n",
    "                # gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "            wandb.log({'Test/Generator loss': g_lossT.item(),\n",
    "                        'Test/Discriminator loss': test_d_loss.item(),\n",
    "                        'Test/L1 loss between real signal A and fake signals A': l1_source.item(),\n",
    "                        'Test/L1 loss between real signal B and fake signals B': l1_target.item(),\n",
    "                        'Test/Discriminator A loss': test_disc_A_loss.item(),\n",
    "                        'Test/Discriminator B loss': test_disc_B_loss.item(),\n",
    "                        'Test/Adversarial or GAN loss A': g_A_lossT.item(),\n",
    "                        'Test/Adversarial or GAN loss B': g_B_lossT.item(),\n",
    "                        'Test/Cycle consistency loss A': cycle_A_lossT.item(),\n",
    "                        'Test/Cycle consistency loss B': cycle_B_lossT.item(),\n",
    "                        'Test/Supervised loss A': sup_A_lossT.item(),\n",
    "                        'Test/Supervised loss B': sup_B_lossT.item(),\n",
    "                        'Test/Epoch': epoch+1,\n",
    "                })      \n",
    "\n",
    "    if ReduceLROnPlateau == True:  \n",
    "        disc_scheduler.step(d_loss)\n",
    "        gen_scheduler.step(g_loss)\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if PolinomialLR == True and (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get losses of the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # source_reconstructed = gen_source(gen_target(source, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # target_reconstructed = gen_target(gen_source(target, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # l1_source = l1(source, fake_source)\n",
    "    # l1_target = l1(target, fake_target)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "# get min and max of losses\n",
    "min_max_loss_source = np.min(source_losses), np.max(source_losses)\n",
    "min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# get median of losses\n",
    "median_loss_source = np.median(source_losses)\n",
    "median_loss_target = np.median(target_losses)\n",
    "print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate signals\n",
    "idx = 0\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "    if idx == 5:\n",
    "        break              \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source[:256].cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source[:256].cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target[:256].cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target[:256].cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals\n",
    "idx = 0             \n",
    "phases = df['Phasenzuordnung'].unique()\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "    if idx == 5:\n",
    "        break                 \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    # fake_target = gen_source(gen_target(source, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # fake_source = gen_target(gen_source(target, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify L1 loss of reconstructed and real signals, per phase and intervention\n",
    "# array([ 0.,  1.,  3., 10.,  2.,  4.,  9.])\n",
    "# df of phase 1\n",
    "df_phase_1 = df_test.loc[df_test['Phasenzuordnung'].isin([1])]\n",
    "df_phase_2 = df_test.loc[df_test['Phasenzuordnung'].isin([2])]\n",
    "df_phase_3 = df_test.loc[df_test['Phasenzuordnung'].isin([3])]\n",
    "df_phase_4 = df_test.loc[df_test['Phasenzuordnung'].isin([4])]\n",
    "df_phase_5 = df_test.loc[df_test['Phasenzuordnung'].isin([5])]\n",
    "\n",
    "# 1.,  3., 10.,  2.,  4.\n",
    "df_int_0 = df_test.loc[df_test['intervention'].isin([0])]\n",
    "df_int_1 = df_test.loc[df_test['intervention'].isin([1])]\n",
    "df_int_2 = df_test.loc[df_test['intervention'].isin([2])]\n",
    "df_int_3 = df_test.loc[df_test['intervention'].isin([3])]\n",
    "df_int_4 = df_test.loc[df_test['intervention'].isin([4])]\n",
    "df_int_5 = df_test.loc[df_test['intervention'].isin([5])]\n",
    "df_int_6 = df_test.loc[df_test['intervention'].isin([6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_dataset = AnimalDatasetEmbedding(df_int_6, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "verify_loader = DataLoader(verify_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in verify_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "# get min and max of losses\n",
    "min_max_loss_source = np.min(source_losses), np.max(source_losses)\n",
    "min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# get median of losses\n",
    "median_loss_source = np.median(source_losses)\n",
    "median_loss_target = np.median(target_losses)\n",
    "print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.save_checkpoint(gen_source, opt_gen, path=\"gen_source.pth.tar\")\n",
    "# utils.save_checkpoint(gen_target, opt_gen, path=\"gen_target.pth.tar\")\n",
    "# utils.save_checkpoint(disc_source, opt_disc, path=\"disc_source.pth.tar\")\n",
    "# utils.save_checkpoint(disc_target, opt_disc, path=\"disc_target.pth.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_checkpoint(\"Checkpoints/gen_source.pth.tar\", gen_source, opt_gen, LEARNING_RATE)\n",
    "# utils.load_checkpoint(\"Checkpoints/gen_target.pth.tar\", gen_target, opt_gen, LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
