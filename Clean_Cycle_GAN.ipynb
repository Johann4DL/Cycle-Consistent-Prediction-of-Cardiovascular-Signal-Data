{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize_df\n",
    "from create_dataset import AnimalDataset, AnimalDatasetEmbedding, UnpairedDataset\n",
    "from generators import EmbeddingEncoderDecoderGenerator, EmbeddingUnetGenerator, OneHotGenerator\n",
    "from discriminators import MultiChannelDiscriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "ALLDATA = True # if False -> smaller dataset\n",
    "SKIPCONNECTIONS = True\n",
    "EMBEDDING = True\n",
    "ONEHOTENCODING = True\n",
    "UNPAIRED = False\n",
    "\n",
    "# DISCRIMINATOR = MultiChannelDiscriminator\n",
    "# GENERATOR = EmbeddingUnetGenerator\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-2  # 1e-5 was too small for 'LVtot_kalibriert' and 'LVtot' \n",
    "NUM_WORKERS = 16\n",
    "NUM_EPOCHS = 30\n",
    "LR_DECAY_AFTER_EPOCH = 200  \n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] â€“ Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"\n",
    "source_signals = [SIG_A, SIG_C, SIG_D]\n",
    "CHANNELS = len(source_signals)\n",
    "WINDOW = 256\n",
    "# target = TARGET\n",
    "\n",
    "# Use adversarial loss\n",
    "GAN_LOSS = True   # adversarial loss\n",
    "LAMBDA_GAN = 1.0\n",
    "# Use cycle consistency loss\n",
    "CYCLE = True\n",
    "LAMBDA_CYCLE = 1.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = False \n",
    "LAMBDA_SUPERVISED = 1.0\n",
    "# Use Identity loss\n",
    "IDENTITY = False\n",
    "LAMBDA_IDENTITY = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_df(path):\n",
    "#     df = utils.load_csv(path)\n",
    "#     df = utils.drop_cols(df)\n",
    "#     df = df.dropna()\n",
    "#     return df\n",
    "\n",
    "# def load_data(alldata=ALLDATA):\n",
    "#     if not alldata:\n",
    "#         path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\" \n",
    "#         df = load_df(path)\n",
    "#         print(df.shape)\n",
    "#         df = utils.remove_strings(df)\n",
    "#         df = utils.subsample(df, 10)\n",
    "#         print(df.shape)\n",
    "#         return df\n",
    "\n",
    "#     else:\n",
    "#         path_1 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_1\"  \n",
    "#         path_2 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_2\"\n",
    "#         # path_3 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_3\"\n",
    "#         # path_4 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_4\"\n",
    "        \n",
    "#         df_1 = load_csv(path_1)\n",
    "#         df_2 = load_csv(path_2)\n",
    "#         # df_3 = load_csv(path_3)\n",
    "#         # df_4 = load_csv(path_4)\n",
    "#         df = pd.concat([df_1, df_2], axis=0, ignore_index=True)\n",
    "\n",
    "#         print(df.shape)\n",
    "#         df = utils.remove_strings(df)\n",
    "#         df = utils.subsample(df, 10)\n",
    "#         print(df.shape)\n",
    "#         return df\n",
    "\n",
    "# df = load_data(alldata=ALLDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8072873, 13)\n"
     ]
    }
   ],
   "source": [
    "# if ALLDATA == False:\n",
    "#     # load only a small part of the data and drop the unnecessary columns\n",
    "#     # paths of only a small part of the data\n",
    "#     path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\" \n",
    "#     df = utils.load_csv(path)\n",
    "#     df = utils.drop_cols(df)\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # select only rows where 'Phasenzuordnung' is 1\n",
    "#     # df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "\n",
    "#     print(df.shape) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALLDATA == True:\n",
    "    # Load all the data and drop unnecessary columns\n",
    "    # We load the data separately, to avoid a Runtime error\n",
    "\n",
    "    # all the data\n",
    "    path_1 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_1\"  \n",
    "    path_2 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_2\"\n",
    "    path_3 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_3\"\n",
    "    path_4 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_4\"\n",
    "\n",
    "    df_1 = utils.load_csv(path_1)\n",
    "    df_1 = utils.drop_cols(df_1)\n",
    "\n",
    "    df_2 = utils.load_csv(path_2)\n",
    "    df_2 = utils.drop_cols(df_2)\n",
    "\n",
    "    df_3 = utils.load_csv(path_3)\n",
    "    df_3 = utils.drop_cols(df_3)\n",
    "\n",
    "    df_4 = utils.load_csv(path_4)\n",
    "    df_4 = utils.drop_cols(df_4)\n",
    "\n",
    "    # concatenate the separate dataframes\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4], axis=0, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    print('Size of the whole dataset',df.shape)\n",
    "    df = utils.remove_strings(df)\n",
    "    df = utils.subsample(df, 10)\n",
    "    print('Size of the dataset after subsampling',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "\n",
    "# def load_df(path):\n",
    "#     df = utils.load_csv(path)\n",
    "#     df = utils.drop_cols(df)\n",
    "#     df = df.dropna()\n",
    "#     return df\n",
    "\n",
    "\n",
    "# if ALLDATA == True:\n",
    "#     # Load all the data and drop unnecessary columns\n",
    "#     # We load the data separately, to avoid a Runtime error\n",
    "\n",
    "#     # all the data\n",
    "#     base_path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/\"\n",
    "#     paths = [os.path.join(base_path, f\"Data_split_{i}\") for i in range(1, 5)]\n",
    "\n",
    "#     df = pd.concat([load_df(path) for path in paths], axis=0, ignore_index=True)\n",
    "    \n",
    "#     print('Size of the whole dataset',df.shape)\n",
    "#     # select only rows where 'Phasenzuordnung' is 1\n",
    "#     # df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "#     # print('Size of dataset with only the first phase',df.shape)\n",
    "# else:\n",
    "#     # load only a small part of the data and drop the unnecessary columns\n",
    "#     # paths of only a small part of the data\n",
    "#     path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\" \n",
    "\n",
    "#     df = load_df(path)\n",
    "\n",
    "#     # select only rows where 'Phasenzuordnung' is 1\n",
    "#     # df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "\n",
    "#     print(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['animal'].unique()))\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('Number o fanimals after removing those with less than 10 data points', df['animal'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_by_phase1(df, scaler):\n",
    "#     '''\n",
    "#     Normalize the data by the first phase\n",
    "#     '''\n",
    "#     # column names\n",
    "#     cols = df.columns.tolist()\n",
    "#     # select data von Phasenzuordnung == 1\n",
    "#     phase_1 = df.loc[df['Phasenzuordnung'] == 1]\n",
    "#     phase_1 = phase_1.to_numpy() \n",
    "#     df = df.to_numpy()  #convert to numpy\n",
    "#     # scale the data only with the first phase\n",
    "#     scaler.fit(phase_1)\n",
    "#     transformed_data = scaler.transform(df)\n",
    "#     df = pd.DataFrame(transformed_data, columns=cols)  # convert to dataframe\n",
    "#     # df = df.join(df_IPA) \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def normalize_df(df, scaler):\n",
    "#     df_IPA = df[['intervention', 'Phasenzuordnung', 'animal']]\n",
    "#     df_temp = pd.DataFrame()\n",
    "#     scaler = StandardScaler()\n",
    "\n",
    "#     for animal in df['animal'].unique():\n",
    "#         # split df into separate dataframes for each animal\n",
    "#         df_animal = df.loc[df['animal'] == animal]\n",
    "#         df_animal = utils.normalize_by_phase1(df_animal, scaler) # normalize by phase 1\n",
    "#         # append df_animal to df_temp\n",
    "#         df_temp = pd.concat([df_temp, df_animal], axis=0, ignore_index=True)\n",
    "\n",
    "#     df = df_temp\n",
    "#     df = df.drop(columns=['intervention', 'Phasenzuordnung', 'animal'])\n",
    "#     df.dropna(inplace=True)\n",
    "#     df = df.join(df_IPA)\n",
    "#     return df\n",
    "\n",
    "df = utils.normalize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_IPA = df[['intervention', 'Phasenzuordnung', 'animal']]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# df_temp = pd.DataFrame()\n",
    "\n",
    "# for animal in df['animal'].unique():\n",
    "#     # split df into separate dataframes for each animal\n",
    "#     df_animal = df.loc[df['animal'] == animal]\n",
    "#     df_animal = utils.normalize_by_phase1(df_animal, scaler) # normalize by phase 1\n",
    "#     # append df_animal to df_temp\n",
    "#     df_temp = pd.concat([df_temp, df_animal], axis=0, ignore_index=True)\n",
    "\n",
    "# df = df_temp\n",
    "# df = df.drop(columns=['intervention', 'Phasenzuordnung', 'animal'])\n",
    "# df.dropna(inplace=True)\n",
    "# df = df.join(df_IPA)\n",
    "utils.visualize(df, [SIG_A, SIG_B, SIG_C, SIG_D, 'intervention', 'Phasenzuordnung', 'animal'], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by animal size in percent\n",
    "df_grouped = df.groupby('animal').size().reset_index(name='counts')\n",
    "df_grouped['counts'] = df_grouped['counts'] / df_grouped['counts'].sum() * 100\n",
    "# df_grouped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick always the same test animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 1 random animals for test data\n",
    "test_animals = df['animal'].sample(n=1, random_state=42).unique()\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def double_conv_pad(in_channels, out_channels):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "#         nn.InstanceNorm1d(out_channels),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Dropout1d(p=0.1, inplace=False),\n",
    "#         nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "#         nn.InstanceNorm1d(out_channels),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Dropout1d(p=0.1, inplace=False),\n",
    "#     )\n",
    "\n",
    "# class OneHotGenerator(nn.Module):\n",
    "#     def __init__(self, INPUTCHANNELS, OUTPUTCHANNELS):\n",
    "#         super(OneHotGenerator, self).__init__()\n",
    "#         self.maxpool = nn.MaxPool1d((2))  \n",
    "\n",
    "#         self.down_conv1 = double_conv_pad(INPUTCHANNELS, 32) \n",
    "#         self.down_conv2 = double_conv_pad(32, 64) \n",
    "#         self.down_conv3 = double_conv_pad(64, 128)\n",
    "#         self.down_conv4 = double_conv_pad(128, 256)\n",
    "#         self.down_conv5 = double_conv_pad(256, 512)\n",
    "#         self.down_conv6 = double_conv_pad(512, 1024)\n",
    "        \n",
    "#         self.phaseLinear = nn.Linear(6, 32)\n",
    "#         self.interventionLinear = nn.Linear(11, 32)\n",
    "#         self.FCLinear = nn.Linear(96, 32)\n",
    "\n",
    "#         self.up_trans1 = nn.ConvTranspose1d(1024, 512, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv1 = double_conv_pad(1024, 512)\n",
    "#         self.up_trans2 = nn.ConvTranspose1d(512, 256, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv2 = double_conv_pad(512, 256)\n",
    "#         self.up_trans3 = nn.ConvTranspose1d(256, 128, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv3 = double_conv_pad(256, 128)\n",
    "#         self.up_trans4 = nn.ConvTranspose1d(128, 64, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv4 = double_conv_pad(128, 64)\n",
    "#         self.up_trans5 = nn.ConvTranspose1d(64, 32, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv5 = double_conv_pad(64, 32)\n",
    "\n",
    "#         self.out = nn.Conv1d(32, OUTPUTCHANNELS, kernel_size=1) # kernel_size must be == 1\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "        \n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Conv1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.ConvTranspose1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#         elif isinstance(module, nn.BatchNorm1d):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Linear):\n",
    "#             module.weight.data.normal_(mean=0.0, std=1)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "\n",
    "\n",
    "#     def forward(self, input, phase, intervention):\n",
    "#         # [Batch size, Channels in, Height, Width]\n",
    "        \n",
    "#         # downsampling\n",
    "#         x1 = self.down_conv1(input)   \n",
    "#         x2 = self.maxpool(x1) \n",
    "#         x3 = self.down_conv2(x2)  \n",
    "#         x4 = self.maxpool(x3) \n",
    "#         x5 = self.down_conv3(x4) \n",
    "#         x6 = self.maxpool(x5) \n",
    "#         x7 = self.down_conv4(x6)\n",
    "#         x8 = self.maxpool(x7)\n",
    "#         x9 = self.down_conv5(x8)\n",
    "#         x10 = self.maxpool(x9)\n",
    "#         x11 = self.down_conv6(x10)     \n",
    "\n",
    "#         # one hot encoding\n",
    "#         p = F.one_hot(phase, num_classes=6).type(torch.FloatTensor).to(DEVICE)\n",
    "#         i = F.one_hot(intervention, num_classes=11).type(torch.FloatTensor).to(DEVICE)\n",
    "#         # Fully connected Layers\n",
    "#         p = self.phaseLinear(p).to(DEVICE)\n",
    "#         i = self.interventionLinear(i).to(DEVICE)\n",
    "#         # Concatenate the phase and intervention one hot encodings with the output of the last convolutional layer and reshape\n",
    "#         x7 = torch.cat([x7, p, i], 1) \n",
    "#         x7 = x7.reshape(x7.shape[0], WINDOW, 96)\n",
    "#         x7 = self.FCLinear(x7).to(DEVICE) \n",
    "        \n",
    "\n",
    "#         x = self.up_trans1(x11)\n",
    "#         x = self.up_conv1(torch.cat([x, x9], 1))  # skip connection\n",
    "#         x = self.up_trans2(x)\n",
    "#         x = self.up_conv2(torch.cat([x, x7], 1))  # skip connection\n",
    "#         x = self.up_trans3(x)\n",
    "#         x = self.up_conv3(torch.cat([x, x5], 1))  # skip connection\n",
    "#         x = self.up_trans4(x)\n",
    "#         x = self.up_conv4(torch.cat([x, x3], 1))  # skip connection\n",
    "#         x = self.up_trans5(x)\n",
    "#         x = self.up_conv5(torch.cat([x, x1], 1))  # skip connection\n",
    "#         x = self.out(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIPCONNECTIONS and not ONEHOTENCODING: # we always use embeddings\n",
    "    gen_B = EmbeddingUnetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "    gen_A = EmbeddingUnetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "if not SKIPCONNECTIONS and not ONEHOTENCODING:\n",
    "    gen_B = EmbeddingEncoderDecoderGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "    gen_A = EmbeddingEncoderDecoderGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "if SKIPCONNECTIONS and ONEHOTENCODING:\n",
    "    gen_B = OneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "    gen_A = OneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "disc_B = MultiChannelDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "disc_A = MultiChannelDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# optimizers for discriminator and generator \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_A.parameters()) + list(disc_B.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_A.parameters()) + list(gen_B.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# maybe a step learning rate would be a good idea 1e-4 -> 1e-5\n",
    "# scheduler\n",
    "# gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_gen, milestones=[10, 30, 50, 80], gamma=0.1)\n",
    "                                                    \n",
    "# disc_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_disc, milestones=[10, 30, 50, 80], gamma=0.1)\n",
    "\n",
    "gen_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_gen,\n",
    "                                                           factor=0.1, patience=3, threshold=1e-4,\n",
    "                                                           min_lr=1e-6,\n",
    "                                                    )\n",
    "disc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_disc,\n",
    "                                                            factor=0.1, patience=3, threshold=1e-4,\n",
    "                                                            min_lr=1e-6,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "# losses\n",
    "l1 = nn.L1Loss() \n",
    "mse = nn.MSELoss() \n",
    "\n",
    "if UNPAIRED:\n",
    "    train_dataset = UnpairedDataset(df_train, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "    test_dataset = UnpairedDataset(df_test, source_signals, target_name = TARGET, test = True, window_length = WINDOW)\n",
    "    gen_dataset = UnpairedDataset(df_test, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "\n",
    "\n",
    "if not UNPAIRED:\n",
    "    # create datasets\n",
    "    train_dataset = AnimalDatasetEmbedding(df_train, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, test = True, window_length = WINDOW)\n",
    "    gen_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)\n",
    "gen_loader = DataLoader(gen_dataset, batch_size=1, shuffle=True, pin_memory=True,)\n",
    "\n",
    "# run in float16\n",
    "# g_scaler = torch.cuda.amp.GradScaler()\n",
    "# d_scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Cycle_GAN\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_signals(fake_target, fake_source, target, source):\n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "def discriminator_loss(disc, reals, fakes):\n",
    "    # calculate how close reals are to being classified as real\n",
    "    real_loss = mse(disc(reals), torch.ones_like(disc(reals)))\n",
    "    # calculate how close fakes are to being classified as fake\n",
    "    fake_loss = mse(disc(fakes), torch.zeros_like(disc(fakes)))\n",
    "    # return the average of real and fake loss\n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_disc_loss(sig_A, sig_B, phase, intervention, \n",
    "                    disc_A, disc_B, gen_A, gen_B, fake_A, fake_B\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    \"\"\"\n",
    "    # generate fakes\n",
    "    # with torch.no_grad():\n",
    "    #     fake_B = gen_B(sig_A, phase, intervention).detach()\n",
    "    #     fake_A = gen_A(sig_B, phase, intervention).detach()\n",
    "    \n",
    "    # discriminator loss\n",
    "    disc_B_loss = discriminator_loss(disc_B, sig_B, fake_B)\n",
    "    disc_A_loss = discriminator_loss(disc_A, sig_A, fake_A)\n",
    "    disc_loss = (disc_A_loss + disc_B_loss) / 2\n",
    "\n",
    "    return disc_loss, disc_A_loss, disc_B_loss\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def calc_gen_loss(sig_A, sig_B, phase, intervention,\n",
    "                  gen_A, gen_B, disc_A, disc_B, fake_B, fake_A\n",
    "                  ):\n",
    "    loss = 0\n",
    "\n",
    "    if GAN_LOSS:\n",
    "        g_A_loss = mse(disc_A(fake_A), torch.ones_like(disc_A(fake_A))) \n",
    "        g_B_loss = mse(disc_B(fake_B), torch.ones_like(disc_B(fake_B))) \n",
    "\n",
    "        loss += g_A_loss * LAMBDA_GAN + g_B_loss * LAMBDA_GAN\n",
    "    else:\n",
    "        g_A_loss = torch.tensor(0)\n",
    "        g_B_loss = torch.tensor(0)\n",
    "\n",
    "    if CYCLE:\n",
    "        rec_B = gen_B(fake_A, phase, intervention)\n",
    "        rec_A = gen_A(fake_B, phase, intervention)\n",
    "        cycle_B_loss = l1(sig_B, rec_B)  # l1 loss: Mean absolute error between each element in the input x and target y\n",
    "        cycle_A_loss = l1(sig_A, rec_A)  # l1 loss in cycle GAN paper\n",
    "\n",
    "        loss += cycle_B_loss * LAMBDA_CYCLE + cycle_A_loss * LAMBDA_CYCLE\n",
    "    else:\n",
    "        cycle_B_loss = torch.tensor(0)\n",
    "        cycle_A_loss = torch.tensor(0)\n",
    "\n",
    "    if SUPERVISED:\n",
    "        sup_A_loss = mse(sig_A, fake_A)\n",
    "        sup_B_loss = mse(sig_B, fake_B)\n",
    "\n",
    "        loss += sup_A_loss * LAMBDA_SUPERVISED + sup_B_loss * LAMBDA_SUPERVISED\n",
    "    else:\n",
    "        sup_A_loss = torch.tensor(0)\n",
    "        sup_B_loss = torch.tensor(0)\n",
    "\n",
    "    if IDENTITY:\n",
    "        id_B_loss = l1(sig_B, gen_B(sig_B, phase, intervention))\n",
    "        id_A_loss = l1(sig_A, gen_A(sig_A, phase, intervention))\n",
    "\n",
    "        loss += id_B_loss * LAMBDA_IDENTITY + id_A_loss * LAMBDA_IDENTITY\n",
    "    else:\n",
    "        id_B_loss = torch.tensor(0)\n",
    "        id_A_loss = torch.tensor(0)\n",
    "\n",
    "    return loss, g_A_loss, g_B_loss, cycle_B_loss, cycle_A_loss, id_B_loss, id_A_loss, sup_A_loss, sup_B_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for source, target, phase, intervention in loader:\n",
    "        # convert to float16\n",
    "        source = source.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        target = target.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        phase = phase.to(DEVICE)\n",
    "        intervention = intervention.to(DEVICE)\n",
    "\n",
    "        #  ------------------------------- #\n",
    "        #  ----- train discriminators ---- #\n",
    "        #  ------------------------------- #\n",
    "        with torch.no_grad():\n",
    "            fake_target = gen_B(source, phase, intervention).detach()\n",
    "            fake_source = gen_A(target, phase, intervention).detach()\n",
    "\n",
    "        d_loss, disc_A_loss, disc_B_loss = get_disc_loss(source, target, phase, intervention,\n",
    "                                                            disc_A, disc_B, gen_A, gen_B, \n",
    "                                                            fake_source, fake_target\n",
    "                                                        )\n",
    "\n",
    "        # update gradients of discriminator \n",
    "        opt_disc.zero_grad() \n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "        # d_scaler.scale(d_loss).backward()  \n",
    "               \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "\n",
    "        out = calc_gen_loss(source, target, phase, intervention,\n",
    "                                gen_A, gen_B, disc_A, disc_B, \n",
    "                                fake_target, fake_source\n",
    "                                )\n",
    "        g_loss, g_A_loss, g_B_loss, cycle_B_loss, cycle_A_loss, id_B_loss, id_A_loss, sup_A_loss, sup_B_loss = out\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        # g_scaler.scale(g_loss).backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    # d_scaler.step(opt_disc)  \n",
    "    # d_scaler.update()\n",
    "\n",
    "    # g_scaler.step(opt_gen) \n",
    "    # g_scaler.update()\n",
    "\n",
    "    wandb.log({'Train/Discriminator A loss': disc_A_loss.item(),\n",
    "                'Train/Discriminator B loss': disc_B_loss.item(),\n",
    "                'Train/Total Discriminator loss': d_loss.item(),\n",
    "                'Train/Total Generator loss': g_loss.item(),\n",
    "                'Train/Adversarial loss A': g_A_loss.item(),\n",
    "                'Train/Adversarial loss B': g_B_loss.item(),\n",
    "                'Train/Cycle consistency loss A': cycle_A_loss.item(),\n",
    "                'Train/Cycle consistency loss B': cycle_B_loss.item(),\n",
    "                'Train/Supervised loss A': sup_A_loss.item(),\n",
    "                'Train/Supervised loss B': sup_B_loss.item(),\n",
    "                'Learning rate': opt_gen.param_groups[0][\"lr\"],\n",
    "                # 'Train/Identity loss A': id_A_loss.item(),\n",
    "                # 'Train/Identity loss B': id_B_loss.item()\n",
    "                })\n",
    "        \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    if (epoch+1) % 1 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_A.eval()  # set discriminator to evaluation mode\n",
    "            disc_B.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_B.eval()\n",
    "            gen_A.eval()\n",
    "\n",
    "            for source, target, phase, intervention in test_loader:\n",
    "                # convert to float16\n",
    "                source = source.float()\n",
    "                target = target.float()\n",
    "\n",
    "                # move to GPU\n",
    "                source = source.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "                phase = phase.to(DEVICE)\n",
    "                intervention = intervention.to(DEVICE)\n",
    "\n",
    "                fake_target = gen_B(source, phase, intervention).detach() # already torch.no_grad()\n",
    "                fake_source = gen_A(target, phase, intervention).detach()\n",
    "\n",
    "                # generate signals during validation\n",
    "                #gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                test_real_fake_lossB= l1(target, fake_target)   # l1(sig_B, fake_B)\n",
    "                test_real_fake_lossA = l1(source, fake_source)\n",
    "\n",
    "                #  ------------------------------- #\n",
    "                #  ----- test discriminators ----- #\n",
    "                #  ------------------------------- #\n",
    "\n",
    "                test_d_loss, test_disc_A_loss, test_disc_B_loss = get_disc_loss(source, target, phase, intervention,\n",
    "                                                                    disc_A, disc_B, gen_A, gen_B, \n",
    "                                                                    fake_source, fake_target\n",
    "                                                                )\n",
    "                \n",
    "                # -------------------------------- #\n",
    "                # ------- test generators -------- #\n",
    "                # -------------------------------- # \n",
    "\n",
    "                out = calc_gen_loss(source, target, phase, intervention,\n",
    "                                        gen_A, gen_B, disc_A, disc_B, \n",
    "                                        fake_target, fake_source\n",
    "                                        )\n",
    "                g_lossT, g_A_lossT, g_B_lossT, cycle_B_lossT, cycle_A_lossT, id_B_lossT, id_A_lossT, sup_A_lossT, sup_B_lossT = out\n",
    "        \n",
    "                # gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "            wandb.log({'Test/Generator loss': g_lossT.item(),\n",
    "                        'Test/Discriminator loss': test_d_loss.item(),\n",
    "                        'Test/L1 loss between real signal A and fake signals A': test_real_fake_lossA.item(),\n",
    "                        'Test/L1 loss between real signal B and fake signals B': test_real_fake_lossB.item(),\n",
    "                        'Test/Discriminator A loss': test_disc_A_loss.item(),\n",
    "                        'Test/Discriminator B loss': test_disc_B_loss.item(),\n",
    "                        'Test/Adversarial or GAN loss A': g_A_lossT.item(),\n",
    "                        'Test/Adversarial or GAN loss B': g_B_lossT.item(),\n",
    "                        'Test/Cycle consistency loss A': cycle_A_lossT.item(),\n",
    "                        'Test/Cycle consistency loss B': cycle_B_lossT.item(),\n",
    "                        'Test/Supervised loss A': sup_A_lossT.item(),\n",
    "                        'Test/Supervised loss B': sup_B_lossT.item(),\n",
    "                        'Test/Epoch': epoch+1,\n",
    "                })\n",
    "            \n",
    "            \n",
    "      \n",
    "    disc_scheduler.step(test_d_loss)\n",
    "    gen_scheduler.step(g_lossT)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals\n",
    "idx = 0             \n",
    "phases = df['Phasenzuordnung'].unique()\n",
    "for source, target, phase, intervention in gen_loader:\n",
    "    if idx == 3:\n",
    "        break                 \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    phase = phase.to(DEVICE)\n",
    "    intervention = intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_B(source, phase, intervention)\n",
    "    fake_source = gen_A(target, phase, intervention)\n",
    "\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = []\n",
    "target_list = []\n",
    "\n",
    "for source, target, phase, intervention in test_loader:\n",
    "    # convert to float16\n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "\n",
    "    # move to GPU\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    phase = phase.to(DEVICE)\n",
    "    intervention = intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_B(source, phase, intervention).detach() # already torch.no_grad()\n",
    "    fake_source = gen_A(target, phase, intervention).detach()\n",
    "\n",
    "    # calculate l1 loss of fake signals and real signals\n",
    "    target_loss= mse(target, fake_target)   # l1(sig_B, fake_B)\n",
    "    source_loss = l1(source, fake_source)\n",
    "\n",
    "    source_list.append(source_loss.item())\n",
    "    target_list.append(target_loss.item())\n",
    "\n",
    "    \n",
    "\n",
    "print(len(source_list))\n",
    "print(len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(source_list)/len(source_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
