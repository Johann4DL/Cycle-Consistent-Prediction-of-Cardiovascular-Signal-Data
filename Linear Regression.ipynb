{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "BATCH_SIZE = 512  \n",
    "LEARNING_RATE = 2e-4  # 1e-5 was too small for 'LVtot_kalibriert' and 'LVtot' \n",
    "NUM_WORKERS = 10\n",
    "NUM_EPOCHS = 500\n",
    "LR_DECAY_AFTER_EPOCH = 300  \n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] â€“ Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"\n",
    "feature_names = [SIG_A, SIG_C, SIG_D]\n",
    "CHANNELS = len(feature_names)\n",
    "WINDOW = 256\n",
    "target = TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files\" \n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "df = pd.DataFrame()\n",
    "scaler = StandardScaler() \n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df_temp = pd.read_csv(f, sep=\";\")\n",
    "    df_temp = utils.drop_cols(df_temp)\n",
    "    df_temp = df_temp.dropna()\n",
    "    df_temp = utils.remove_strings(df_temp)\n",
    "    df_temp = utils.subsample(df_temp, 10)\n",
    "    df_temp = utils.normalize(df_temp, scaler, phase1 = True)  \n",
    "      \n",
    "    # print the content\n",
    "    df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Phasenzuordnung'] == 1:\n",
    "        df.at[index, 'intervention'] = 0\n",
    "    elif row['intervention'] == 10:\n",
    "        if row['contractility'] == 1.0:\n",
    "            df.at[index, 'intervention'] = 0      # contractility = 1.0 - could be ignored? - phase 0?\n",
    "        if row['contractility'] == 3.0:\n",
    "            df.at[index, 'intervention'] = 5      # contractility = 3.0                                        \n",
    "        if row['contractility'] == 4.0:\n",
    "            df.at[index, 'intervention'] = 6      # contractility = 4.0\n",
    "\n",
    "# get unique intervention\n",
    "# print(df['intervention'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IPA = df[['intervention', 'Phasenzuordnung', 'animal']]\n",
    "\n",
    "# split df into separate dataframes for each animal\n",
    "scaler = StandardScaler()\n",
    "df_temp = pd.DataFrame()\n",
    "\n",
    "#utils.visualize(df, [SIG_A, SIG_B, SIG_C, SIG_D, 'intervention', 'animal'], 72094)\n",
    "\n",
    "for animal in df['animal'].unique():\n",
    "    # split df into separate dataframes for each animal\n",
    "    df_animal = df.loc[df['animal'] == animal]\n",
    "    df_animal = utils.normalize(df_animal, scaler)\n",
    "    # append df_animal to df_temp\n",
    "    df_temp = pd.concat([df_temp, df_animal], axis=0, ignore_index=True)\n",
    "\n",
    "print(df_temp.shape)\n",
    "df = df_temp\n",
    "df = df.drop(columns=['intervention', 'Phasenzuordnung', 'animal'])\n",
    "df = df.join(df_IPA)\n",
    "# utils.visualize(df, [SIG_A, SIG_B, SIG_C, SIG_D, 'intervention', 'animal'], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select animals 3,4,8,11,17 as test animals\n",
    "test_animals = [3,4,8,11,17] \n",
    "\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# change the length of the test data to a multiple of the Window size\n",
    "df_test = df_test.iloc[:len(df_test) - (len(df_test) % WINDOW)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LR = df_train[[SIG_A, SIG_B, SIG_C, SIG_D, 'intervention', 'Phasenzuordnung']]\n",
    "target = df_train[[TARGET]]\n",
    "\n",
    "# convert to numpy array\n",
    "X_train = df_LR.to_numpy()\n",
    "y_train = target.to_numpy()\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "print(reg.score(X_train, y_train))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LR_test = df_test[[SIG_A, SIG_B, SIG_C, SIG_D, 'intervention', 'Phasenzuordnung']]\n",
    "target_test = df_test[[TARGET]]\n",
    "\n",
    "# convert to numpy array\n",
    "X_test = df_LR_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "\n",
    "\n",
    "#calculate the mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE: ', mean_squared_error(y_test, reg.predict(X_test)))\n",
    "\n",
    "# calculate l1 loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('L1: ', mean_absolute_error(y_test, reg.predict(X_test)))\n",
    "\n",
    "\n",
    "# plot the first 1000 results \n",
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "plt.plot(y_test[:1000], label='true')\n",
    "plt.plot(reg.predict(X_test[:1000]), label='predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
