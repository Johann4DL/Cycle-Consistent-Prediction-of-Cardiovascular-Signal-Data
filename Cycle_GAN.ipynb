{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import config\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (disc): Sequential(\n",
      "    (0): Conv1d(1, 1, kernel_size=(4,), stride=(2,))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv1d(1, 1, kernel_size=(4,), stride=(2,))\n",
      "    (3): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv1d(1, 1, kernel_size=(4,), stride=(2,))\n",
      "    (6): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv1d(1, 1, kernel_size=(4,), stride=(2,))\n",
      "    (9): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv1d(1, 1, kernel_size=(4,), stride=(2,))\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (13): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(1, 1, 4, 2, 0),\n",
    "            nn.InstanceNorm1d(1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(1, 1, 4, 2, 0),\n",
    "            nn.InstanceNorm1d(1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(1, 1, 4, 2, 0),\n",
    "            nn.InstanceNorm1d(1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(1, 1, 4, 2, 0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "# pass 1d tensor through the discriminator and return the output\n",
    "#LVP_tiny = LVP[0:100]\n",
    "#LVP_tiny = torch.tensor(LVP_tiny).double()\n",
    "#LVP_tiny = LVP_tiny.reshape(1, 1, 100)\n",
    "#LVP_tiny = LVP_tiny.float()\n",
    "#print(LVP_tiny)\n",
    "disc = Discriminator(1)\n",
    "#y = disc(LVP_tiny)\n",
    "print(disc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (gen): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (1): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (4): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (7): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (11): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (12): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
      "    (15): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv1d(64, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (18): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, 3, 1, 1),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels, channels, 3, 1, 1),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator class with 2 Residual blocks\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels=64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 3, 2, 1),\n",
    "            nn.InstanceNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 256, 3, 2, 1),\n",
    "            nn.InstanceNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            nn.ConvTranspose1d(256, 128, 3, 2, 1, 1),\n",
    "            nn.InstanceNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, 3, 2, 1, 1),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, out_channels, 7, 1, 3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "# pass 1d tensor through the block and return the output\n",
    "gen = Generator(1, 1)\n",
    "#y = gen( LVP_tiny)\n",
    "#print(\"Output of Generator: \", y.shape, '\\n', y, '\\n')\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116300, 39)\n",
      "(65800, 39)\n"
     ]
    }
   ],
   "source": [
    "# read csv file semi-colon separated\n",
    "df_1 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_2 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "#df_3 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "#df_4 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_5 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "#df_6 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_10_2.csv\", sep=\";\")\n",
    "#df_7 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_8 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "#df_9 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_10_2.csv\", sep=\";\")\n",
    "#df_10 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_4_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_11 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_4_1_1_1_3_2.csv\", sep=\";\")\n",
    "#df_12 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_13 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "#df_14 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "#df_15 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "#df_16 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "\n",
    "# concatenate all dataframes\n",
    "df = df_1\n",
    "#df = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13, df_14, df_15, df_16], ignore_index=True)\n",
    "print(df.shape)\n",
    "\n",
    "# access columns by name (e.g. df['LVP']) or by index (e.g. df.iloc[:, 0])\n",
    "\n",
    "#df_57 = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_58 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "\n",
    "df_test = df_58 #pd.concat([df_57, df_58], ignore_index=True)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116300, 39)\n",
      "(65800, 39)\n"
     ]
    }
   ],
   "source": [
    "# subsample data by a factor of 10\n",
    "#df = df.sample(frac=0.1, random_state=1)\n",
    "#df_test = df_test.sample(frac=0.1, random_state=1)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataframe to make it easier to test the code\n",
    "# df = pd.read_csv(\"LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_1_2.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, signal_A, signal_B, df):\n",
    "        self.df = df\n",
    "        self.signal_A = self.df[signal_A]\n",
    "        self.signal_B = self.df[signal_B]\n",
    "\n",
    "        # creating tensor from df \n",
    "        self.tensor_A = torch.tensor(self.df[signal_A].values).double()\n",
    "        self.tensor_B = torch.tensor(self.df[signal_B].values).double()\n",
    "\n",
    "        # split tensor into tensors of size 100\n",
    "        self.tensor_A = self.tensor_A.split(100)\n",
    "        self.tensor_B = self.tensor_B.split(100)\n",
    "\n",
    "        for el in self.tensor_A:\n",
    "            el = el.reshape(1, 1, 100)\n",
    "            el = el.float()\n",
    "            \n",
    "        for el in self.tensor_B:\n",
    "            el = el.reshape(1, 1, 100)\n",
    "            el = el.float()\n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        # signal_A and signal_B should have the same length\n",
    "        return len(self.tensor_A)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return the signal at the given index\n",
    "        return self.tensor_A[index], self.tensor_B[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint - only during train phase\n",
    "#model_A2B = utils.load_checkpoint(config.CHECKPOINT_GEN_A2B, gen_A2B, opt_gen, config.LEARNING_RATE)\n",
    "#model_B2A = utils.load_checkpoint(config.CHECKPOINT_GEN_B2A, gen_B2A, opt_gen, config.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train df:  (116300, 39) \n",
      "\n",
      "Shape of test dataframe:  (175800, 39)\n",
      "\n",
      "Epoch [1/5], Loss D: 0.4390, loss G: 845.9126\n",
      "\n",
      "Epoch [2/5], Loss D: 0.4278, loss G: 493.8284\n",
      "\n",
      "Epoch [3/5], Loss D: 0.3816, loss G: 483.7133\n",
      "\n",
      "Epoch [4/5], Loss D: 0.3756, loss G: 629.4152\n",
      "\n",
      "Epoch [5/5], Loss D: 0.3648, loss G: 457.9908\n",
      "=> Saving checkpoint at location:  Checkpoints/LVtot_kalibriert_to_RVtot_kalibriert/gen_LVtot.pth.tar\n",
      "=> Saving checkpoint at location:  Checkpoints/LVtot_kalibriert_to_RVtot_kalibriert/gen_RVtot.pth.tar\n",
      "=> Saving checkpoint at location:  Checkpoints/LVtot_kalibriert_to_RVtot_kalibriert/disc_LVtot.pth.tar\n",
      "=> Saving checkpoint at location:  Checkpoints/LVtot_kalibriert_to_RVtot_kalibriert/disc_RVtot.pth.tar\n",
      "Epoch [5/5]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # initialize generator and discriminator\n",
    "    gen_A2B = Generator(in_channels=1, out_channels=1).to(\"cuda\")\n",
    "    gen_B2A = Generator(in_channels=1, out_channels=1).to(\"cuda\")\n",
    "    disc_A = Discriminator(in_channels=1).to(\"cuda\")\n",
    "    disc_B = Discriminator(in_channels=1).to(\"cuda\")\n",
    "\n",
    "    # optimizers for discriminator and generator \n",
    "    opt_disc = torch.optim.Adam(\n",
    "        list(disc_A.parameters()) + list(disc_B.parameters()), \n",
    "        lr=0.0002, \n",
    "        betas=(0.5, 0.999) \n",
    "    )\n",
    "    opt_gen = torch.optim.Adam(\n",
    "        list(gen_A2B.parameters()) + list(gen_B2A.parameters()),\n",
    "        lr=0.0002,\n",
    "        betas=(0.5, 0.999)\n",
    "    )\n",
    "\n",
    "    l1 = nn.L1Loss() # L1 loss for cycle consistency and identity loss\n",
    "    mse = nn.MSELoss() # MSE loss for adversarial loss\n",
    "\n",
    "    # load checkpoint if required\n",
    "    load_checkpoint = False\n",
    "\n",
    "    if load_checkpoint:\n",
    "        utils.load_checkpoint(\n",
    "            config.CHECKPOINT_GEN_B2A, gen_A2B, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        utils.load_checkpoint(\n",
    "            config.CHECKPOINT_GEN_A2B, gen_B2A, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        utils.load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_A, disc_A, opt_disc, config.LEARNING_RATE,\n",
    "        )\n",
    "        utils.load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_B, disc_B, opt_disc, config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    # train data\n",
    "    print('Shape of train df: ', df.shape, '\\n') # df is a global variable that contains the data\n",
    "    df_train = df\n",
    "\n",
    "    # test data\n",
    "    print('Shape of test dataframe: ', df_test.shape)\n",
    "\n",
    "    # create datasets with class SignalDataset\n",
    "    dataset = SignalDataset(signal_A='LVP', signal_B='AoP', df=df_train)\n",
    "    test_dataset = SignalDataset(signal_A='LVP', signal_B='AoP', df=df_test)\n",
    "    \n",
    "    # Data loader\n",
    "    batch_size = 1  # best batch size according to cycle GAN paper\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True,)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True,)\n",
    "\n",
    "    # run in float16\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # training loop\n",
    "\n",
    "    # tqdm for progress bar\n",
    "    # loop = tqdm(loader, leave=True) # does not work\n",
    "    NUM_EPOCHS = 5\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        for sig_A, sig_B in loader:\n",
    "            # convert to float16\n",
    "            sig_A = sig_A.float()\n",
    "            sig_B = sig_B.float()\n",
    "\n",
    "            # move to GPU\n",
    "            sig_A = sig_A.to(config.DEVICE)\n",
    "            sig_B = sig_B.to(config.DEVICE)\n",
    "\n",
    "            # train discriminators\n",
    "            with torch.cuda.amp.autocast(): #necessary for float16\n",
    "\n",
    "                fake_B = gen_A2B(sig_A)\n",
    "                d_B_real = disc_B(sig_B)\n",
    "                d_B_fake = disc_B(fake_B.detach())\n",
    "                d_B_real_loss = mse(d_B_real, torch.ones_like(d_B_real)) # why ones? Mentioned in cycle GAN paper\n",
    "                d_B_fake_loss = mse(d_B_fake, torch.zeros_like(d_B_fake)) # why zeros? Mentioned in cycle GAN paper\n",
    "                d_B_loss = d_B_real_loss + d_B_fake_loss\n",
    "\n",
    "                fake_A = gen_B2A(sig_B)\n",
    "                d_A_real = disc_A(sig_A)\n",
    "                d_A_fake = disc_B(fake_A.detach()) \n",
    "                d_A_real_loss = mse(d_A_real, torch.ones_like(d_A_real)) # why ones? Mentioned in cycle GAN paper\n",
    "                d_A_fake_loss = mse(d_A_fake, torch.zeros_like(d_A_fake)) # why zeros? Mentioned in cycle GAN paper\n",
    "                d_A_loss = d_A_real_loss + d_A_fake_loss\n",
    "\n",
    "                # put it all together\n",
    "                d_loss = (d_A_loss + d_B_loss) / 2 # why average? Mentioned in cycle GAN paper\n",
    "\n",
    "            # exit amp.auto_cast() context manager and backpropagate \n",
    "            opt_disc.zero_grad()   \n",
    "            d_scaler.scale(d_loss).backward() \n",
    "            d_scaler.step(opt_disc)\n",
    "            d_scaler.update()\n",
    "\n",
    "            # train generators\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                # adversarial loss for both generators\n",
    "                d_A_fake = disc_A(fake_A) # d_A_fake is the output of the discriminator for the fake_A signal\n",
    "                d_B_fake = disc_B(fake_B)\n",
    "                g_A_loss = mse(d_A_fake, torch.ones_like(d_A_fake))\n",
    "                g_B_loss = mse(d_B_fake, torch.ones_like(d_B_fake))\n",
    "\n",
    "                # cycle consistency loss\n",
    "                cycle_B = gen_A2B(fake_A)\n",
    "                cycle_A = gen_B2A(fake_B)\n",
    "                cycle_B_loss = l1(sig_B, cycle_B)\n",
    "                cycle_A_loss = l1(sig_A, cycle_A)\n",
    "\n",
    "                # identity loss\n",
    "                id_B = gen_A2B(sig_B) # id_B is the output of the generator for the sig_B signal\n",
    "                id_A = gen_B2A(sig_A)\n",
    "                id_B_loss = l1(sig_B, id_B)\n",
    "                id_A_loss = l1(sig_A, id_A)\n",
    "\n",
    "                # put it all together\n",
    "                g_loss = (\n",
    "                    g_A_loss +\n",
    "                    g_B_loss +\n",
    "                    cycle_B_loss * config.LAMBDA_CYCLE +\n",
    "                    cycle_A_loss * config.LAMBDA_CYCLE +\n",
    "                    id_B_loss * config.LAMBDA_IDENTITY +  # config.LAMBDA_IDENTITY = 0.0 -> no identity loss \n",
    "                    id_A_loss * config.LAMBDA_IDENTITY    # we could remove it to increase training speed\n",
    "                )\n",
    "\n",
    "            opt_gen.zero_grad()\n",
    "            g_scaler.scale(g_loss).backward()\n",
    "            g_scaler.step(opt_gen)\n",
    "            g_scaler.update()\n",
    "        \n",
    "        # print(f\"Loss D: {d_loss.item():.4f}, loss G: {g_loss.item():.4f}\")\n",
    "        print('\\nEpoch [{}/{}], Loss D: {:.4f}, loss G: {:.4f}'.format(epoch+1, NUM_EPOCHS, d_loss.item(), g_loss.item()))\n",
    "        \n",
    "        #  validation\n",
    "        disc_A.eval()  # set discriminator to evaluation mode\n",
    "        disc_B.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "        gen_A2B.eval()\n",
    "        gen_B2A.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mse_G_A2B = 0\n",
    "            mse_G_B2A = 0\n",
    "            for sig_A, sig_B in test_loader:\n",
    "                # convert to float16\n",
    "                sig_A = sig_A.float()\n",
    "                sig_B = sig_B.float()\n",
    "\n",
    "                # move to GPU\n",
    "                sig_A = sig_A.to(config.DEVICE)\n",
    "                sig_B = sig_B.to(config.DEVICE)\n",
    "\n",
    "                fake_B = gen_A2B(sig_A)\n",
    "                fake_A = gen_B2A(sig_B)\n",
    "                \n",
    "                # calculate mse loss of fake signals and real signals\n",
    "                mse_A2B = mse(sig_B, fake_B)\n",
    "                mse_B2A = mse(sig_A, fake_A)\n",
    "                mse_G_A2B += mse_A2B\n",
    "                mse_G_B2A += mse_B2A\n",
    "\n",
    "            # print('\\nTest:\\nMSE loss Generator A2B: {:.4f}, MSE loss Generator B2A: {:.4f}'.format(mse_G_A2B/len(test_loader), mse_G_B2A/len(test_loader)))\n",
    "\n",
    "    if config.SAVE_MODEL:\n",
    "        utils.save_checkpoint(gen_A2B, opt_gen, path=config.CHECKPOINT_GEN_A2B)\n",
    "        utils.save_checkpoint(gen_B2A, opt_gen, path=config.CHECKPOINT_GEN_B2A)\n",
    "        utils.save_checkpoint(disc_A, opt_disc, path=config.CHECKPOINT_DISC_A)\n",
    "        utils.save_checkpoint(disc_B, opt_disc, path=config.CHECKPOINT_DISC_B)\n",
    "\n",
    "        # print progress\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LVP to AoP\n",
    "\n",
    "Epoch [1/5], Loss D: 0.3012, loss G: 619.1838\n",
    "\n",
    "Epoch [2/5], Loss D: 0.3729, loss G: 1133.2758\n",
    "\n",
    "Epoch [3/5], Loss D: 0.2997, loss G: 818.2902\n",
    "\n",
    "Epoch [4/5], Loss D: 0.3086, loss G: 794.9103\n",
    "\n",
    "Epoch [5/5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AoP to AoQ\n",
    "\n",
    "Epoch [1/10], Loss D: 0.3331, loss G: 763.7483\n",
    "\n",
    "Epoch [2/10], Loss D: 0.3324, loss G: 528.8701\n",
    "\n",
    "Epoch [3/10], Loss D: 0.3328, loss G: 416.0652\n",
    "\n",
    "Epoch [4/10], Loss D: 0.3331, loss G: 541.8179\n",
    "\n",
    "Epoch [5/10], Loss D: 0.3312, loss G: 500.3121\n",
    "\n",
    "Epoch [6/10], Loss D: 0.3270, loss G: 490.1998\n",
    "\n",
    "Epoch [7/10], Loss D: 0.3597, loss G: 390.4794\n",
    "\n",
    "Epoch [8/10], Loss D: 0.3440, loss G: 403.8621\n",
    "\n",
    "Epoch [9/10], Loss D: 0.3366, loss G: 566.6377\n",
    "\n",
    "Epoch [10/10], Loss D: 0.3217, loss G: 452.6429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load a saved version of the model:\n",
    "\n",
    "#saved_model = GarmentClassifier()\n",
    "#saved_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do: Use batch size > 1, mit Anton den code durchgehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(gen_A2B, gen_B2A, test_loader):\n",
    "\n",
    "    for sig_A, sig_B in test_loader:\n",
    "        # convert to float16\n",
    "        sig_A = sig_A.float()\n",
    "        sig_B = sig_B.float()\n",
    "\n",
    "        # move to GPU\n",
    "        sig_A = sig_A.to(config.DEVICE)\n",
    "        sig_B = sig_B.to(config.DEVICE)\n",
    "\n",
    "        # generate fake signals\n",
    "        fake_B = gen_A2B(sig_A)\n",
    "        fake_A = gen_B2A(sig_B)\n",
    "\n",
    "        # store fake signals in dataframe\n",
    "        fake_B = fake_B.cpu().detach().numpy()\n",
    "        fake_A = fake_A.cpu().detach().numpy()\n",
    "        fake_B = pd.DataFrame(fake_B)\n",
    "        fake_A = pd.DataFrame(fake_A)\n",
    "        # append both dataframes\n",
    "        fake_B = fake_B.append(fake_A) \n",
    "        fake_B.to_csv('fake_signals.csv')\n",
    "\n",
    "        #calculate mse loss between fake and real signals\n",
    "        mse_loss = mse(sig_B, fake_B)\n",
    "        print(f\"MSE loss: {mse_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below works, but is split into a main() and train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df:  (116300, 39) \n",
      "\n",
      "Shape of df_test:  (175800, 39)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_checkpoint() got an unexpected keyword argument 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 167\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mNUM_EPOCHS\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn [12], line 157\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m train_fn(\n\u001b[1;32m    143\u001b[0m     disc_A,\n\u001b[1;32m    144\u001b[0m     disc_B,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     g_scaler,\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mSAVE_MODEL:\n\u001b[0;32m--> 157\u001b[0m     utils\u001b[39m.\u001b[39;49msave_checkpoint(gen_A2B, opt_gen, filename\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mCHECKPOINT_GEN_A2B)\n\u001b[1;32m    158\u001b[0m     utils\u001b[39m.\u001b[39msave_checkpoint(gen_B2A, opt_gen, filename\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mCHECKPOINT_GEN_B2A)\n\u001b[1;32m    159\u001b[0m     utils\u001b[39m.\u001b[39msave_checkpoint(disc_A, opt_disc, filename\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mCHECKPOINT_DISC_A)\n",
      "\u001b[0;31mTypeError\u001b[0m: save_checkpoint() got an unexpected keyword argument 'filename'"
     ]
    }
   ],
   "source": [
    "def train_fn(disc_A, disc_B, gen_A2B, gen_B2A, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "\n",
    "   #loop = tqdm(loader) # progress bar for training loop \n",
    "\n",
    "   for sig_A, sig_B in loader:\n",
    "        # convert to float16\n",
    "        sig_A = sig_A.float()\n",
    "        sig_B = sig_B.float()\n",
    "\n",
    "        # move to GPU\n",
    "        sig_A = sig_A.to(config.DEVICE)\n",
    "        sig_B = sig_B.to(config.DEVICE)\n",
    "\n",
    "        # train discriminators\n",
    "        with torch.cuda.amp.autocast(): #necessary for float16\n",
    "\n",
    "            fake_B = gen_A2B(sig_A)\n",
    "            d_B_real = disc_B(sig_B)\n",
    "            d_B_fake = disc_B(fake_B.detach())\n",
    "            d_B_real_loss = mse(d_B_real, torch.ones_like(d_B_real)) # why ones? Mentioned in cycle GAN paper\n",
    "            d_B_fake_loss = mse(d_B_fake, torch.zeros_like(d_B_fake)) # why zeros? Mentioned in cycle GAN paper\n",
    "            d_B_loss = d_B_real_loss + d_B_fake_loss\n",
    "\n",
    "            fake_A = gen_B2A(sig_B)\n",
    "            d_A_real = disc_A(sig_A)\n",
    "            d_A_fake = disc_B(fake_A.detach()) \n",
    "            d_A_real_loss = mse(d_A_real, torch.ones_like(d_A_real)) # why ones? Mentioned in cycle GAN paper\n",
    "            d_A_fake_loss = mse(d_A_fake, torch.zeros_like(d_A_fake)) # why zeros? Mentioned in cycle GAN paper\n",
    "            d_A_loss = d_A_real_loss + d_A_fake_loss\n",
    "\n",
    "            # put it together\n",
    "            d_loss = (d_A_loss + d_B_loss) / 2 # why average? Mentioned in cycle GAN paper\n",
    "\n",
    "        # exit amp.auto_cast() context manager and backpropagate \n",
    "        opt_disc.zero_grad()   \n",
    "        d_scaler.scale(d_loss).backward() \n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # train generators\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            # adversarial loss for both generators\n",
    "            d_A_fake = disc_A(fake_A) # d_A_fake is the output of the discriminator for the fake_A signal\n",
    "            d_B_fake = disc_B(fake_B)\n",
    "            g_A_loss = mse(d_A_fake, torch.ones_like(d_A_fake))\n",
    "            g_B_loss = mse(d_B_fake, torch.ones_like(d_B_fake))\n",
    "\n",
    "            # cycle consistency loss\n",
    "            cycle_B = gen_A2B(fake_A)\n",
    "            cycle_A = gen_B2A(fake_B)\n",
    "            cycle_B_loss = l1(sig_B, cycle_B)\n",
    "            cycle_A_loss = l1(sig_A, cycle_A)\n",
    "\n",
    "            # identity loss\n",
    "            id_B = gen_A2B(sig_B) # id_B is the output of the generator for the sig_B signal\n",
    "            id_A = gen_B2A(sig_A)\n",
    "            id_B_loss = l1(sig_B, id_B)\n",
    "            id_A_loss = l1(sig_A, id_A)\n",
    "\n",
    "            # put it together\n",
    "            g_loss = (\n",
    "                g_A_loss +\n",
    "                g_B_loss +\n",
    "                cycle_B_loss * config.LAMBDA_CYCLE +\n",
    "                cycle_A_loss * config.LAMBDA_CYCLE +\n",
    "                id_B_loss * config.LAMBDA_IDENTITY +  # LAMBDA_IDENTITY = 0.0 -> no identity loss \n",
    "                id_A_loss * config.LAMBDA_IDENTITY    # we could remove it t increase training speed\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(g_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "        \n",
    "        # print(f\"Loss D: {d_loss.item():.4f}, loss G: {g_loss.item():.4f}\")\n",
    "        \n",
    "        #  validation\n",
    "        \n",
    "\n",
    "def main():\n",
    "    disc_A = Discriminator(in_channels=1).to(\"cuda\")\n",
    "    disc_B = Discriminator(in_channels=1).to(\"cuda\")\n",
    "    gen_A2B = Generator(in_channels=1, out_channels=1).to(\"cuda\")\n",
    "    gen_B2A = Generator(in_channels=1, out_channels=1).to(\"cuda\")\n",
    "\n",
    "    # optimizers for discriminator and generator \n",
    "    opt_disc = torch.optim.Adam(\n",
    "        list(disc_A.parameters()) + list(disc_B.parameters()), \n",
    "        lr=0.0002, \n",
    "        betas=(0.5, 0.999) \n",
    "    )\n",
    "    opt_gen = torch.optim.Adam(\n",
    "        list(gen_A2B.parameters()) + list(gen_B2A.parameters()),\n",
    "        lr=0.0002,\n",
    "        betas=(0.5, 0.999)\n",
    "    )\n",
    "\n",
    "    L1_loss = nn.L1Loss() # L1 loss for cycle consistency and identity loss\n",
    "    MSE_loss = nn.MSELoss() # MSE loss for adversarial loss\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN_B2A, gen_A2B, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN_A2B, gen_B2A, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_A, disc_A, opt_disc, config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_B, disc_B, opt_disc, config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    # dataset\n",
    "    # df is a global variable that contains the data\n",
    "    print('Shape of df: ', df.shape, '\\n')\n",
    "    df_train = df\n",
    "    # test data\n",
    "    print('Shape of df_test: ', df_test.shape)\n",
    "\n",
    "    # create dataset\n",
    "    dataset = SignalDataset(signal_A='LVP', signal_B='AoP', df=df_train)\n",
    "    test_dataset = SignalDataset(signal_A='LVP', signal_B='AoP', df=df_test)\n",
    "    \n",
    "    # Data loader\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True,)\n",
    "\n",
    "    # run in float16\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # training loop\n",
    "    NUM_EPOCHS = 1\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(\n",
    "            disc_A,\n",
    "            disc_B,\n",
    "            gen_A2B,\n",
    "            gen_B2A,\n",
    "            loader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1_loss,\n",
    "            MSE_loss,\n",
    "            d_scaler,\n",
    "            g_scaler,\n",
    "        )\n",
    "\n",
    "        if config.SAVE_MODEL:\n",
    "            utils.save_checkpoint(gen_A2B, opt_gen, filename=config.CHECKPOINT_GEN_A2B)\n",
    "            utils.save_checkpoint(gen_B2A, opt_gen, filename=config.CHECKPOINT_GEN_B2A)\n",
    "            utils.save_checkpoint(disc_A, opt_disc, filename=config.CHECKPOINT_DISC_A)\n",
    "            utils.save_checkpoint(disc_B, opt_disc, filename=config.CHECKPOINT_DISC_B)\n",
    "\n",
    "        # print progress\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
