{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "from create_dataset import AnimalDataset\n",
    "from generators import OneChannelUnetGenerator, MultiChannelUnetGenerator, SkipConnectionsMultiChannelUnetGenerator\n",
    "from discriminators import OneChannelDiscriminator, MultiChannelDiscriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "ALLDATA = True # if False -> smaller dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 512 \n",
    "LEARNING_RATE = 1e-4  # 1e-5 was too small for 'LVtot_kalibriert' and 'LVtot' \n",
    "NUM_WORKERS = 10\n",
    "NUM_EPOCHS = 100\n",
    "LR_DECAY_AFTER_EPOCH = 250  \n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] – Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"  # Kalibriertes Volumensignal der linken Herzkammer [ml] - Ventrikelvolumen\n",
    "feature_names = [SIG_A]  # one signal works\n",
    "CHANNELS = len(feature_names)\n",
    "WINDOW = 256\n",
    "EMBEDDING = False\n",
    "target = TARGET\n",
    "\n",
    "SKIPCONNECTIONS = True\n",
    "\n",
    "# Use cycle consistency loss\n",
    "CYCLE = True\n",
    "LAMBDA_CYCLE = 10.0\n",
    "FORWARD_CYCLE = 10.0\n",
    "BACKWARD_CYCLE = 10.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = False\n",
    "LAMBDA_SUPERVISED = 10.0\n",
    "# Use adversarial loss\n",
    "ADVERSARIAL = False\n",
    "LAMBDA_ADVERSARIAL = 100.0\n",
    "# Use Identity loss\n",
    "IDENTITY = True\n",
    "LAMBDA_IDENTITY = 10.0\n",
    "\n",
    "# Abailable Generators\n",
    "# GENERATOR = OneChannelUnetGenerator or MultiChannelUnetGenerator or SkipConnectionsMultiChannelUnetGenerator\n",
    "\n",
    "# Available Discminators\n",
    "# DISCRIMINTOR = OneChannelDiscriminator or MultiChannelDiscriminator\n",
    "\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = False\n",
    "SAVE_EPOCH = 100\n",
    "load_from_epoch = 10  # select which Generator to load\n",
    "\n",
    "gen_B_filename = 'Checkpoints/GenB_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, load_from_epoch)\n",
    "gen_A_filename = 'Checkpoints/GenA_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, load_from_epoch)\n",
    "disc_B_filename = 'Checkpoints/DiscB_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, load_from_epoch)\n",
    "disc_A_filename = 'Checkpoints/DiscA_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, load_from_epoch)\n",
    "\n",
    "CHECKPOINT_GEN_B = gen_B_filename\n",
    "CHECKPOINT_GEN_A = gen_A_filename\n",
    "CHECKPOINT_DISC_B =  disc_B_filename\n",
    "CHECKPOINT_DISC_A =  disc_A_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALLDATA == False:\n",
    "    # load only a small part of the data and drop the unnecessary columns\n",
    "    # paths of only a small part of the data\n",
    "    path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\" \n",
    "    df = utils.load_csv(path)\n",
    "    df = utils.drop_cols(df)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # select only rows where 'Phasenzuordnung' is 1\n",
    "    # df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "\n",
    "    print(df.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the whole dataset (60221949, 13)\n"
     ]
    }
   ],
   "source": [
    "if ALLDATA == True:\n",
    "    # Load all the data and drop unnecessary columns\n",
    "    # We load the data separately, to avoid a Runtime error\n",
    "\n",
    "    # all the data\n",
    "    path_1 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_1\"  \n",
    "    path_2 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_2\"\n",
    "    path_3 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_3\"\n",
    "    path_4 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_4\"\n",
    "\n",
    "    df_1 = utils.load_csv(path_1)\n",
    "    df_1 = utils.drop_cols(df_1)\n",
    "\n",
    "    df_2 = utils.load_csv(path_2)\n",
    "    df_2 = utils.drop_cols(df_2)\n",
    "\n",
    "    df_3 = utils.load_csv(path_3)\n",
    "    df_3 = utils.drop_cols(df_3)\n",
    "\n",
    "    df_4 = utils.load_csv(path_4)\n",
    "    df_4 = utils.drop_cols(df_4)\n",
    "\n",
    "    # concatenate the separate dataframes\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4], axis=0, ignore_index=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    print('Size of the whole dataset',df.shape)\n",
    "    # select only rows where 'Phasenzuordnung' is 1\n",
    "    # df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "    # print('Size of dataset with only the first phase',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesssing\n",
    "### Removing strings from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.remove_strings(df)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the data by the factor 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.subsample(df, 10)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.normalize(df)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicht invasive Messgroessen: \n",
    "#### Pumpenfluss, Pumpenstrom, Aortendruck , Ventrikeldruck \n",
    "\n",
    "***AoP*** = Drucksignal Hauptschlagader = Aortendruck\n",
    "\n",
    "***LVP*** = Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "\n",
    "***VADQ*** = Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "\n",
    "***VadCurrent*** = VAD Strom [A] – Pumpemstrom in Ampere\n",
    "\n",
    "#### Diese hier sind eventuell ebenfalls nicht invasiv\n",
    "\n",
    "***AoQ*** = Flusssignal Hauptschlagader/ Aorta [l/min] - Aortenfluss\n",
    "\n",
    "***VADspeed*** = VAD Geschwindigkeit [rpm] (rpm = drehzahl). Kann man nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot AoP, LVP, VadQ, VADcurrent in different plots\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot AoQ and VADspeed in differents plots\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(20, 10))\n",
    "# axs[0].plot(df['AoQ'][:5000])\n",
    "# axs[0].set_title('AoQ')\n",
    "# axs[1].plot(df['VADspeed'][:5000])\n",
    "# axs[1].set_title('VADspeed')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Invasive Regelgroessen: \n",
    "#### Ventrikelvolumen, Schlagarbeit\n",
    "\n",
    "***LVtot*** = unkalibriertes Volumensignal der linken Herzkammer [ml] (Ventrikelvolumen)\n",
    "\n",
    "***RVtot*** = unkalibriertes Volumensignal der rechten Herzkammer [ml] (Ventrikelvolumen der rechten Herzkammer. Erstmal das der lnken anschauen)\n",
    "\n",
    "***LVtot_kalibriert*** = kalibriertes Ventrikelvolumen\n",
    "\n",
    "***RVtot_kalibriert*** = Kalibriertes Volumensignal der rechten Herzkammer = kalibriertes Ventrikelvolumen (wenn vorhanden – kalibriert nutzen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot LVtot, LVtot_kalibriert, RVtot, RVtot_kalibriert in different plots\n",
    "\n",
    "#utils.visualize(df, 'LVtot', 'LVtot_kalibriert', 'RVtot', 'RVtot_kalibriert', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andere relevante Variablen\n",
    "\n",
    "***intervention*** = Gibt an welche Intervention im aktuellen Ausschnitt durchgeführt wurde: 1 = Vorlastreduktion (Googlen), 2 & 3 = Nachlasterhöhung (Googlen) , 4 = Speedramp (Pumpenstrom angehoben)\n",
    "\n",
    "***Phasenzuordnung***\n",
    "\n",
    "***animal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hom many different intervention ids are there?\n",
    "print('\\nDifferent interventions: \\n',df['intervention'].unique())\n",
    "\n",
    "# hom many different animal ids are there?\n",
    "print('\\nDifferent animal IDs: \\n',len(df['animal'].unique()))\n",
    "\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('\\nDifferent animal IDs after removing those with less than 10 data points: \\n',len(df['animal'].unique()))\n",
    "\n",
    "# length of data per animal\n",
    "#print(df.groupby('animal').size())\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# pick 2 random animals for test data\n",
    "test_animals = df['animal'].sample(n=1, random_state=1).unique()\n",
    "\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "print('\\nTest animals:', test_animals)\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: \\n',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "#print('Unique animals in df_train: ',df_train['animal'].unique())\n",
    "#print('Unique animals in df_test: ',df_test['animal'].unique())\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify that the data is loaded properly\n",
    "\n",
    "# SIG_A = \"VadQ\"\n",
    "# SIG_B = \"LVP\"\n",
    "# SIG_C = \"AoQ\"\n",
    "# SIG_D = \"AoP\"\n",
    "# feature_names = [SIG_A, SIG_B, SIG_C, SIG_D]\n",
    "# target = \"LVtot_kalibriert\" \n",
    "\n",
    "# train_dataset = AnimalDataset(df_train, feature_names, target_name = target, test = False, window_length = 256)\n",
    "# test_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = True, window_length = 256)\n",
    "\n",
    "# # Data loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=10, pin_memory=True,)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=10, pin_memory=True,)\n",
    "\n",
    "# # length of train and test dataset\n",
    "# print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify OneChannelUnetGenerator generator\n",
    "\n",
    "# x = torch.rand(4,1,256)\n",
    "# model = OneChannelUnetGenerator()\n",
    "# print(model(x).shape)\n",
    "# gen_output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify MultiChannelUnetGenerator generator\n",
    "\n",
    "# CHANNELS = 4 # Channel anpassen!!\n",
    "\n",
    "# x1 = torch.rand(512, 1, 256)\n",
    "# x2 = torch.rand(512, 1, 256)\n",
    "# x3 = torch.rand(512, 1, 256)\n",
    "# x4 = torch.rand(512, 1, 256)\n",
    "# input = torch.cat([x1, x2, x3, x4], 1)\n",
    "# # print('Input shpe: ', input.size())\n",
    "# model = MultiChannelUnetGenerator(INPUTCHANNELS=CHANNELS, OUTPUTCHANNELS = 1)\n",
    "# print('\\nOutput for {} chanels: '.format(CHANNELS), model(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify SkipConnectionsMultiChannelUnetGenerator\n",
    "\n",
    "# CHANNELS = 4 # Channel anpassen!!\n",
    "\n",
    "# x1 = torch.rand(512, 1, 256)\n",
    "# x2 = torch.rand(512, 1, 256)\n",
    "# x3 = torch.rand(512, 1, 256)\n",
    "# x4 = torch.rand(512, 1, 256)\n",
    "# input = torch.cat([x1, x2, x3, x4], 1)\n",
    "# # print('Input shpe: ', input.size())\n",
    "# model = SkipConnectionsMultiChannelUnetGenerator(CHANNELS=CHANNELS)\n",
    "# print('\\nOutput for {} chanels: '.format(CHANNELS), model(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify MultiChannelDiscriminator \n",
    "\n",
    "# CHANNELS = 4\n",
    "# x1 = torch.rand(512, 1, 256)\n",
    "# x2 = torch.rand(512, 1, 256)\n",
    "# x3 = torch.rand(512, 1, 256)\n",
    "# x4 = torch.rand(512, 1, 256)\n",
    "# combined_input = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "# print(combined_input.shape)\n",
    "# discriminator = MultiChannelDiscriminator(CHANNELS)\n",
    "# print(discriminator(combined_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify Discriminator\n",
    "\n",
    "# x = torch.randn((4, 1, 256))\n",
    "# disc = OneChannelDiscriminator()\n",
    "# print(disc(x).shape)\n",
    "# print(disc(gen_output).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator and discriminator\n",
    "if CHANNELS == 1:\n",
    "    gen_A = OneChannelUnetGenerator().to(DEVICE)  # this uses Skipconnections\n",
    "    gen_B = OneChannelUnetGenerator().to(DEVICE)\n",
    "    disc_A = OneChannelDiscriminator().to(DEVICE)\n",
    "    disc_B = OneChannelDiscriminator().to(DEVICE)\n",
    "\n",
    "if CHANNELS > 1 and SKIPCONNECTIONS == False:\n",
    "    gen_B = MultiChannelUnetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "    gen_A = MultiChannelUnetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "    disc_B = MultiChannelDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_A = MultiChannelDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "if CHANNELS > 1 and SKIPCONNECTIONS == True:\n",
    "    gen_B = SkipConnectionsMultiChannelUnetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "    gen_A = SkipConnectionsMultiChannelUnetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "    disc_B = MultiChannelDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_A = MultiChannelDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# optimizers for discriminator and generator \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_A.parameters()) + list(disc_B.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_A.parameters()) + list(gen_B.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "\n",
    "# maybe a step learning rate would be a good idea 1e-5 -> 1e-4\n",
    "# scheduler\n",
    "gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = 1,\n",
    "                                                    )\n",
    "disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = 1,\n",
    "                                                    )\n",
    "# losses\n",
    "l1 = nn.L1Loss() # L1 loss for cycle consistency and identity loss\n",
    "mse = nn.MSELoss() # MSE loss for adversarial loss\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    utils.load_checkpoint(CHECKPOINT_GEN_B, gen_B, opt_gen, LEARNING_RATE)\n",
    "    utils.load_checkpoint(CHECKPOINT_GEN_A, gen_A, opt_gen, LEARNING_RATE)\n",
    "    utils.load_checkpoint(CHECKPOINT_DISC_B, disc_B, opt_disc, LEARNING_RATE)\n",
    "    utils.load_checkpoint(CHECKPOINT_DISC_A, disc_A, opt_disc, LEARNING_RATE)\n",
    "        \n",
    "\n",
    "\n",
    "if EMBEDDING == False:\n",
    "    # create datasets  \n",
    "    train_dataset = AnimalDataset(df_train, feature_names, target_name = target, test = False, window_length = WINDOW)\n",
    "    test_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = True, window_length = WINDOW)\n",
    "    gen_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = False, window_length = WINDOW)\n",
    "\n",
    "if EMBEDDING == True:\n",
    "    train_dataset = AnimalDataset(df_train, feature_names, target_name = target, test = False, window_length = WINDOW)\n",
    "    test_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = True, window_length = WINDOW)\n",
    "    gen_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = False, window_length = WINDOW)\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)\n",
    "gen_loader = DataLoader(gen_dataset, batch_size=1, shuffle=True, pin_memory=True,)\n",
    "\n",
    "# run in float16\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "train_losses = {\n",
    "            'Discriminator A loss' : [],\n",
    "            'Discriminator B loss' : [],\n",
    "            'Total Discriminator loss' : [],\n",
    "            'Adversaral loss A' : [],\n",
    "            'Adversaral loss B' : [],\n",
    "            'Cycle consistency loss A' : [],\n",
    "            'Cycle consistency loss B' : [],\n",
    "            'Total Generator loss' : [],\n",
    "            'Supervised loss A' : [],\n",
    "            'Supervised loss B' : [],\n",
    "            'identity_loss' : [],\n",
    "        }\n",
    "\n",
    "test_losses = {\n",
    "            'Discriminator A loss' : [],\n",
    "            'Discriminator B loss' : [],\n",
    "            'Total Discriminator loss' : [],\n",
    "            'Adversaral loss A' : [],\n",
    "            'Adversaral loss B' : [],\n",
    "            'Cycle consistency loss A' : [],\n",
    "            'Cycle consistency loss B' : [],\n",
    "            'Total Generator loss' : [],\n",
    "            'L1 loss between real signal A and fake signals A' : [],\n",
    "            'L1 loss between real signal B and fake signals B' : [],\n",
    "            'Supervised loss A' : [],\n",
    "            'Supervised loss B' : [],\n",
    "            'identity_loss' : [],\n",
    "            'L1_realA_fakeB': [],\n",
    "            'L1_realB_fakeA': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader.dataset.animal_dfs[0].shape[0] / 256\n",
    "# (np.where(loader.dataset.animal_cumsum >= 1)[0][0])\n",
    "\n",
    "# inputs, targets= next(iter(loader))\n",
    "# print(inputs.shape, targets.shape)\n",
    "\n",
    "# train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for sig_A, sig_B in loader:\n",
    "        # convert to float16\n",
    "        sig_A = sig_A.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        sig_B = sig_B.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        sig_A = sig_A.to(DEVICE)\n",
    "        sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "        #  -------------------------------- #\n",
    "        #  ----- train discriminators ----- #\n",
    "        #  -------------------------------- #\n",
    "        with torch.cuda.amp.autocast(): # necessary for float16\n",
    "            \n",
    "            # Discriminators: D_A: gen_B(A) vs. B; D_B: gen_B(A) vs. A.\n",
    "            \n",
    "            fake_B = gen_B(sig_A) # generate fake signal B\n",
    "            d_B_real = disc_B(sig_B) # output of discriminator B for real signal B\n",
    "            d_B_fake = disc_B(fake_B.detach()) # output of discriminator B for fake signal B (detached from generator)\n",
    "\n",
    "            # B_reals += d_B_real.mean().item()\n",
    "            # B_fakes += d_B_fake.mean().item()\n",
    "\n",
    "            # Loss between dicriminator (with real signal) output and 1 - The discriminator should output 1 for real signals\n",
    "            d_B_real_loss = mse(d_B_real, torch.ones_like(d_B_real))  \n",
    "            # Loss between dicriminator (with fake signal) output and 0 - The discriminator should output 0 for fake signals\n",
    "            d_B_fake_loss = mse(d_B_fake, torch.zeros_like(d_B_fake)) \n",
    "            # Total loss for discriminator B\n",
    "            d_B_loss = d_B_real_loss + d_B_fake_loss\n",
    "\n",
    "            fake_A = gen_A(sig_B)\n",
    "            d_A_real = disc_A(sig_A)\n",
    "            d_A_fake = disc_A(fake_A.detach()) \n",
    "            d_A_real_loss = mse(d_A_real, torch.ones_like(d_A_real)) \n",
    "            d_A_fake_loss = mse(d_A_fake, torch.zeros_like(d_A_fake))  \n",
    "            d_A_loss = d_A_real_loss + d_A_fake_loss\n",
    "\n",
    "            # Total loss for discriminator A\n",
    "            d_loss = d_A_loss + d_B_loss  # in cycle GAN paper they halve the loss\n",
    "\n",
    "        # exit amp.auto_cast() context manager and backpropagate \n",
    "        opt_disc.zero_grad() \n",
    "        d_scaler.scale(d_loss).backward()  \n",
    "        d_scaler.step(opt_disc)  \n",
    "        d_scaler.update()\n",
    "        \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            g_loss = 0  #initialize Generator loss and add the different losses as needed\n",
    "\n",
    "            if ADVERSARIAL:\n",
    "                # ----- adversarial loss for both generators ----- #\n",
    "                d_A_fake = disc_A(fake_A) # disc_A should output 0 for fake signal A\n",
    "                d_B_fake = disc_B(fake_B) # disc_B should output 0 for fake signal B\n",
    "                # loss between discriminator output and 0 - The discriminator should output 0 for fake signals\n",
    "                g_A_loss = mse(d_A_fake, torch.ones_like(d_A_fake))   \n",
    "                g_B_loss = mse(d_B_fake, torch.ones_like(d_B_fake)) \n",
    "\n",
    "                g_loss += LAMBDA_ADVERSARIAL * (g_A_loss + g_B_loss)\n",
    "\n",
    "            if SUPERVISED:\n",
    "                # supervised A loss\n",
    "                sup_A_loss = l1(sig_A, fake_A)\n",
    "                sup_B_loss = l1(sig_B, fake_B)\n",
    "\n",
    "                g_loss += LAMBDA_SUPERVISED * (sup_B_loss  + sup_A_loss) \n",
    "                \n",
    "\n",
    "            if CYCLE:\n",
    "                # ----- cycle consistency loss ----- #\n",
    "                \n",
    "                # Forward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)\n",
    "                \n",
    "                cycle_A_loss = l1(gen_A(gen_B(sig_A)), sig_A)  # forward cycle loss\n",
    "                cycle_B_loss = l1(gen_B(gen_A(sig_B)), sig_B)  # Backward cycle loss\n",
    "                \n",
    "                # cycle_B = gen_B(fake_A) # fake_A = gen_A(sig_B)  \n",
    "                # cycle_A = gen_A(fake_B) # fake_B = gen_B(sig_A)\n",
    "                # cycle_B_loss = l1(sig_B, cycle_B)  # l1 loss: Mean absolute error between each element in the input x and target y.\n",
    "                # cycle_A_loss = l1(sig_A, cycle_A)\n",
    "\n",
    "                g_loss += LAMBDA_CYCLE * (BACKWARD_CYCLE * cycle_B_loss + FORWARD_CYCLE * cycle_A_loss) \n",
    "\n",
    "            if IDENTITY:\n",
    "            # ----- identity loss ----- #\n",
    "            \n",
    "            # Identity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) \n",
    "            # (Sec 5.2 \" Photo generation from paintings\" in the paper)\n",
    "                \n",
    "                identity_loss = LAMBDA_IDENTITY * (LAMBDA_CYCLE * (l1(gen_B(sig_B), sig_B) + l1(gen_A(sig_A), sig_A)))\n",
    "                g_loss += identity_loss\n",
    "                # id_B = gen_B(sig_B) \n",
    "                # id_A = gen_A(sig_A)\n",
    "                # id_B_loss = l1(sig_A, id_B)\n",
    "                # id_A_loss = l1(sig_B, id_A)\n",
    "\n",
    "                # g_loss += id_B_loss * LAMBDA_IDENTITY + id_A_loss * LAMBDA_IDENTITY\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(g_loss).backward() \n",
    "        g_scaler.step(opt_gen) \n",
    "        g_scaler.update()\n",
    "\n",
    "    # save losses\n",
    "    train_losses['Discriminator A loss'].append(d_A_loss.item())\n",
    "    train_losses['Discriminator B loss'].append(d_B_loss.item())\n",
    "    train_losses['Total Discriminator loss'].append(d_loss.item())\n",
    "    if ADVERSARIAL:\n",
    "        train_losses['Adversaral loss A'].append(g_A_loss.item())\n",
    "        train_losses['Adversaral loss B'].append(g_B_loss.item())\n",
    "    if CYCLE: \n",
    "        train_losses['Cycle consistency loss A'].append(cycle_A_loss.item())\n",
    "        train_losses['Cycle consistency loss B'].append(cycle_B_loss.item())\n",
    "    if SUPERVISED: \n",
    "        train_losses['Supervised loss A'].append(sup_A_loss.item())\n",
    "        train_losses['Supervised loss B'].append(sup_B_loss.item())\n",
    "    \n",
    "    if IDENTITY:\n",
    "        train_losses['identity_loss'].append(identity_loss.item())\n",
    "        # train_losses['Identity loss A'].append(id_A_loss.item())\n",
    "        # train_losses['Identity loss B'].append(id_B_loss.item())\n",
    "        \n",
    "    train_losses['Total Generator loss'].append(g_loss.item())\n",
    "    \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    #  validation every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_A.eval()  # set discriminator to evaluation mode\n",
    "            disc_B.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_B.eval()\n",
    "            gen_A.eval()\n",
    "\n",
    "            # store losses for testing\n",
    "            test_Discrminator_A_loss = 0 #\n",
    "            test_Discrminator_B_loss = 0 #\n",
    "            test_Total_Discrminator_loss = 0 #\n",
    "            test_Adversaral_loss_A = 0  #\n",
    "            test_Adversaral_loss_B = 0  #\n",
    "            test_Cycle_consistency_loss_A = 0 #\n",
    "            test_Cycle_consistency_loss_B = 0 #\n",
    "            test_sup_A_loss = 0\n",
    "            test_sup_B_loss = 0\n",
    "            test_Total_Generator_loss = 0 #\n",
    "            test_L1_real_fake_A = 0  # L1 loss between real signal A and fake signal A\n",
    "            test_L1_real_fake_B = 0  # L1 loss between real signal B and fake signal B\n",
    "\n",
    "            for sig_A, sig_B in test_loader:\n",
    "                # convert to float16\n",
    "                sig_A = sig_A.float()\n",
    "                sig_B = sig_B.float()\n",
    "\n",
    "                # move to GPU\n",
    "                sig_A = sig_A.to(DEVICE)\n",
    "                sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "                fake_B = gen_B(sig_A)\n",
    "                fake_A = gen_A(sig_B)\n",
    "\n",
    "        \n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                test_L1_real_fake_B = l1(sig_B, fake_B)   # l1(sig_B, fake_B)\n",
    "                test_L1_real_fake_A = l1(sig_A, fake_A)   # l1(sig_A, fake_A)\n",
    "                \n",
    "                L1_realA_fakeB = l1(sig_A, fake_B)\n",
    "                L1_realB_fakeA = l1(sig_B, fake_A)\n",
    "                \n",
    "                # ----- discriminator loss ----- #\n",
    "                test_Discriminator_A_loss = mse(disc_A(sig_A), torch.ones_like(disc_A(sig_A))) + mse(disc_A(fake_A), torch.zeros_like(disc_A(fake_A)))\n",
    "                test_Discriminator_B_loss = mse(disc_B(sig_B), torch.ones_like(disc_B(sig_B))) + mse(disc_B(fake_B), torch.zeros_like(disc_B(fake_B)))\n",
    "                \n",
    "                # ----- total discriminator loss ----- #\n",
    "                test_Total_Discriminator_loss = test_Discriminator_A_loss + test_Discriminator_B_loss\n",
    "\n",
    "                test_Total_Generator_loss = 0 # initialize\n",
    "                # calculate adversarial loss\n",
    "                if ADVERSARIAL:\n",
    "                    test_Adversaral_loss_B = mse(disc_B(fake_B), torch.ones_like(disc_B(fake_B))) #was ones_like before\n",
    "                    test_Adversaral_loss_A = mse(disc_A(fake_A), torch.ones_like(disc_A(fake_A)))\n",
    "\n",
    "                    test_Total_Generator_loss += test_Adversaral_loss_A * LAMBDA_ADVERSARIAL + test_Adversaral_loss_B * LAMBDA_ADVERSARIAL \n",
    "\n",
    "                # ----- cycle loss ----- #\n",
    "                if CYCLE:\n",
    "                    \n",
    "                    test_Cycle_consistency_loss_A = l1(gen_A(gen_B(sig_A)), sig_A)  # forward cycle loss\n",
    "                    test_Cycle_consistency_loss_B = l1(gen_B(gen_A(sig_B)), sig_B)  # Backward cycle loss\n",
    "\n",
    "\n",
    "                    test_Total_Generator_loss += LAMBDA_CYCLE * (cycle_B_loss + cycle_A_loss)\n",
    "                    \n",
    "                    # cycle_B = gen_B(fake_A)  # fake_A = gen_B2A(sig_B)\n",
    "                    # cycle_A = gen_A(fake_B)  # fake_B = gen_A2B(sig_A)\n",
    "                    # test_Cycle_consistency_loss_B = l1(sig_B, cycle_B)\n",
    "                    # test_Cycle_consistency_loss_A = l1(sig_A, cycle_A)\n",
    "\n",
    "                    # test_Total_Generator_loss += test_Cycle_consistency_loss_B * LAMBDA_CYCLE + test_Cycle_consistency_loss_A * LAMBDA_CYCLE\n",
    "\n",
    "                # ----- supervise loss ------ #\n",
    "                if SUPERVISED:\n",
    "                    test_sup_A_loss = mse(sig_A, fake_A)\n",
    "                    test_sup_B_loss = mse(sig_B, fake_B)\n",
    "\n",
    "                    test_Total_Generator_loss += LAMBDA_SUPERVISED * (test_sup_A_loss + test_sup_B_loss)\n",
    "\n",
    "                # ----- identity loss ----- #\n",
    "                if IDENTITY:\n",
    "                    \n",
    "                    identity_loss = LAMBDA_IDENTITY * (LAMBDA_CYCLE * (l1(gen_B(sig_B), sig_B) + l1(gen_A(sig_A), sig_A)))\n",
    "                    test_Total_Generator_loss += identity_loss\n",
    "                #     id_B = gen_B(sig_B)\n",
    "                #     id_A = gen_A(sig_A)\n",
    "                #     id_B_loss = l1(sig_A, id_B)\n",
    "                #     id_A_loss = l1(sig_B, id_A)\n",
    "\n",
    "                #     test_Total_Generator_loss += id_B_loss * LAMBDA_IDENTITY + id_A_loss * LAMBDA_IDENTITY\n",
    "                    \n",
    "                # # ----- discriminator loss ----- #\n",
    "                # test_Discriminator_A_loss = mse(disc_A(sig_A), torch.ones_like(disc_A(sig_A))) + mse(disc_A(fake_A), torch.zeros_like(disc_A(fake_A)))\n",
    "                # test_Discriminator_B_loss = mse(disc_B(sig_B), torch.ones_like(disc_B(sig_B))) + mse(disc_B(fake_B), torch.zeros_like(disc_B(fake_B)))\n",
    "                \n",
    "                # # ----- total discriminator loss ----- #\n",
    "                # test_Total_Discriminator_loss = test_Discriminator_A_loss + test_Discriminator_B_loss\n",
    "\n",
    "                # save losses\n",
    "                test_losses['Discriminator A loss'].append(test_Discriminator_A_loss.item())\n",
    "                test_losses['Discriminator B loss'].append(test_Discriminator_B_loss.item())\n",
    "                test_losses['Total Discriminator loss'].append(test_Total_Discriminator_loss.item())\n",
    "                if ADVERSARIAL:\n",
    "                    test_losses['Adversaral loss A'].append(test_Adversaral_loss_A.item())\n",
    "                    test_losses['Adversaral loss B'].append(test_Adversaral_loss_B.item())\n",
    "                if CYCLE:\n",
    "                    test_losses['Cycle consistency loss A'].append(test_Cycle_consistency_loss_A.item())\n",
    "                    test_losses['Cycle consistency loss B'].append(test_Cycle_consistency_loss_B.item())\n",
    "                if SUPERVISED:\n",
    "                    test_losses['Supervised loss A'].append(test_sup_A_loss.item())\n",
    "                    test_losses['Supervised loss B'].append(test_sup_B_loss.item())\n",
    "                if IDENTITY:\n",
    "                    test_losses['identity_loss'].append(identity_loss.item())\n",
    "                \n",
    "                test_losses['Total Generator loss'].append(test_Total_Generator_loss.item())\n",
    "                test_losses['L1 loss between real signal A and fake signals A'].append(test_L1_real_fake_A.item())\n",
    "                test_losses['L1 loss between real signal B and fake signals B'].append(test_L1_real_fake_B.item())\n",
    "                test_losses['L1_realA_fakeB'].append(L1_realA_fakeB.item())\n",
    "                test_losses['L1_realB_fakeA'].append(L1_realB_fakeA.item())\n",
    "\n",
    "\n",
    "            #  ------------------------------------- #   \n",
    "            #  ------- Generate fake signals ------- #\n",
    "            #  ------------------------------------- #\n",
    "            \n",
    "            # Generate fake signals after the last epoch\n",
    "            \n",
    "            if (epoch+1) % GENERATION_AFTER_EPOCH == 0:\n",
    "                print('Generate fake signals')\n",
    "                # generate fake signals 10 times\n",
    "                #utils.save_predictions(gen_loader, gen_B, gen_A, fake_A, fake_B, DEVICE, mse)\n",
    "                idx =0\n",
    "                for sig_A, sig_B in gen_loader:\n",
    "                    if idx == 2:\n",
    "                        break\n",
    "\n",
    "                    sig_A = sig_A.float()\n",
    "                    sig_B = sig_B.float()\n",
    "                    sig_A = sig_A.to(DEVICE)\n",
    "                    sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "                    fake_B = gen_B(sig_A)\n",
    "                    fake_A = gen_A(sig_B)\n",
    "\n",
    "                    # plot generated signals and real signals\n",
    "                    #reshape to 1D\n",
    "                    fake_B = fake_B.reshape(-1)\n",
    "                    fake_A = fake_A.reshape(-1)\n",
    "                    sig_A = sig_A.reshape(-1)\n",
    "                    sig_B = sig_B.reshape(-1)\n",
    "\n",
    "                    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "                    ax[0].plot(sig_A.cpu().numpy(), label= 'Real signal A')\n",
    "                    ax[0].plot(fake_A.cpu().numpy(), label= 'Generated signal A')\n",
    "                    ax[0].set_xlabel('Epoch')\n",
    "                    ax[0].set_ylabel('Loss')\n",
    "                    ax[0].legend()\n",
    "\n",
    "                    ax[1].plot(sig_B.cpu().numpy(), label= 'Real signal B')\n",
    "                    ax[1].plot(fake_B.cpu().numpy(), label= 'Generated signal B')\n",
    "                    ax[1].set_xlabel('Epoch')\n",
    "                    ax[1].set_ylabel('Loss')\n",
    "                    ax[1].legend()\n",
    "\n",
    "                    # plot generated signals and real signals\n",
    "                    plt.figure(figsize=(15, 5))\n",
    "                    plt.plot(sig_A.cpu().detach().numpy(), label= 'Real signal A')\n",
    "                    plt.plot(sig_B.cpu().detach().numpy(), label= 'Real signal B')\n",
    "                    plt.plot(fake_A.cpu().detach().numpy(), label= 'Generated signal A')\n",
    "                    plt.plot(fake_B.cpu().detach().numpy(), label= 'Generated signal B')\n",
    "                    plt.title('Generated signals vs real signals')\n",
    "                    plt.legend()\n",
    "\n",
    "                    idx += 1\n",
    "\n",
    "        # activate training mode again\n",
    "        disc_A.train()  \n",
    "        disc_B.train()\n",
    "        gen_B.train()\n",
    "        gen_A.train()\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "\n",
    "    #  ------------------------------------- #   \n",
    "    #  ------------- Checkpoint ------------ #\n",
    "    #  ------------------------------------- #\n",
    "\n",
    "    if (epoch+1) % SAVE_EPOCH == 0 and SAVE_MODEL == True:\n",
    "\n",
    "        gen_B_filename = 'Checkpoints/GenB_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, epoch+1)\n",
    "        gen_A_filename = 'Checkpoints/GenA_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, epoch+1)\n",
    "        disc_B_filename = 'Checkpoints/DiscB_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, epoch+1)\n",
    "        disc_A_filename = 'Checkpoints/DiscA_FeatureNames_{}_Target_[{}]_BatchSize_[{}]_LR_[{}]_Epoch_[{}].pth.tar'.format(feature_names, target, BATCH_SIZE, LEARNING_RATE, epoch+1)\n",
    "\n",
    "        utils.save_checkpoint(gen_B, opt_gen, path=gen_B_filename)\n",
    "        utils.save_checkpoint(gen_A, opt_gen, path=gen_A_filename)\n",
    "        utils.save_checkpoint(disc_B, opt_disc, path=disc_B_filename)\n",
    "        utils.save_checkpoint(disc_A, opt_disc, path=disc_A_filename)\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "# -------------- PLOT --------------- #\n",
    "# ----------------------------------- #\n",
    "\n",
    "# Plot training losses in different subplots\n",
    "\n",
    "fig, ax = plt.subplots(11, 1, figsize=(12, 24))\n",
    "ax[0].plot(train_losses['Discriminator A loss'], label= 'Discriminator A loss (Training)')\n",
    "ax[0].plot(train_losses['Discriminator B loss'], label= 'Discriminator B loss (Training)')\n",
    "ax[0].plot(train_losses['Total Discriminator loss'], label= 'Total Discriminator loss (Training)')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "#ax[0].set_title('Training Discriminator Loss')\n",
    "\n",
    "ax[1].plot(train_losses['Adversaral loss A'], label= 'Adversaral loss A (Training)')\n",
    "ax[1].plot(train_losses['Adversaral loss B'], label= 'Adversaral loss B (Training)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "#ax[1].set_title('Training Adversarial Loss')\n",
    "ax[2].plot(train_losses['Cycle consistency loss A'], label= 'Cycle consistency loss A (Training)')\n",
    "ax[2].plot(train_losses['Cycle consistency loss B'], label= 'Cycle consistency loss B (Training)')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend()\n",
    "#ax[2].set_title('Training Cycle Consistency Loss')\n",
    "ax[3].plot(train_losses['Total Generator loss'], label= 'Total Generator loss (Training)')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Loss')\n",
    "ax[3].legend()\n",
    "#ax[3].set_title('Training Total Generator Loss')\n",
    "ax[4].plot(train_losses['Supervised loss A'], label = 'Supervised loss A (Training)')\n",
    "ax[4].plot(train_losses['Supervised loss B'], label = 'Supervised loss B (Training)')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Loss')\n",
    "ax[4].legend()\n",
    "\n",
    "# Plot test losses in different subplots\n",
    "\n",
    "ax[5].plot(test_losses['Discriminator A loss'], label= 'Discriminator A loss (Test)')\n",
    "ax[5].plot(test_losses['Discriminator B loss'], label= 'Discriminator B loss (Test)')\n",
    "ax[5].plot(test_losses['Total Discriminator loss'], label= 'Total Discriminator loss (Test)')\n",
    "ax[5].set_xlabel('Epoch')\n",
    "ax[5].set_ylabel('Loss')\n",
    "ax[5].legend()\n",
    "#ax[4].set_title('Test Discriminator Loss')\n",
    "\n",
    "ax[6].plot(test_losses['Adversaral loss A'], label= 'Adversaral loss A (Test)')\n",
    "ax[6].plot(test_losses['Adversaral loss B'], label= 'Adversaral loss B (Test)')\n",
    "ax[6].set_xlabel('Epoch')\n",
    "ax[6].set_ylabel('Loss')\n",
    "ax[6].legend()\n",
    "#ax[5].set_title('Test Adversarial Loss')\n",
    "\n",
    "ax[7].plot(test_losses['Cycle consistency loss A'], label= 'Cycle consistency loss A (Test)')\n",
    "ax[7].plot(test_losses['Cycle consistency loss B'], label= 'Cycle consistency loss B (Test)')\n",
    "ax[7].set_xlabel('Epoch')\n",
    "ax[7].set_ylabel('Loss')\n",
    "ax[7].legend()\n",
    "#ax[6].set_title('Test Cycle Consistency Loss')\n",
    "\n",
    "ax[8].plot(test_losses['Supervised loss A'], label = 'Supervised loss A (Training)')\n",
    "ax[8].plot(test_losses['Supervised loss B'], label = 'Supervised loss B (Training)')\n",
    "ax[8].set_xlabel('Epoch')\n",
    "ax[8].set_ylabel('Loss')\n",
    "ax[8].legend()\n",
    "\n",
    "ax[9].plot(test_losses['Total Generator loss'], label= 'Total Generator loss (Test)')\n",
    "ax[9].set_xlabel('Epoch')\n",
    "ax[9].set_ylabel('Loss')\n",
    "ax[9].legend()\n",
    "#ax[7].set_title('Test Total Generator Loss')\n",
    "\n",
    "ax[10].plot(test_losses['L1 loss between real signal A and fake signals A'], label= 'L1 loss between real signal A and fake signals A (Test)')\n",
    "ax[10].plot(test_losses['L1 loss between real signal B and fake signals B'], label= 'L1 loss between real signal B and fake signals B (Test)')\n",
    "ax[10].set_xlabel('Epoch')\n",
    "ax[10].set_ylabel('Loss')\n",
    "ax[10].legend()\n",
    "#ax[8].set_title('Test L1 Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Selected losses for the test dataset after the last epoch:\\n')\n",
    "print('\\nL1 loss between real signal A and fake signals A: ', test_losses['L1 loss between real signal A and fake signals A'][-1])\n",
    "print('\\nL1 loss between real signal B and fake signals B: ', test_losses['L1 loss between real signal B and fake signals B'][-1])\n",
    "print('\\nL1 loss between real signal A and fake signal B: ', test_losses['L1_realA_fakeB'][-1])\n",
    "print('\\nL1 loss between real signal B and fake signal A: ', test_losses['L1_realB_fakeA'][-1])\n",
    "\n",
    "print('\\nDiscriminator A loss: ', test_losses['Discriminator A loss'][-1])\n",
    "print('\\nDiscriminator B loss: ', test_losses['Discriminator B loss'][-1])\n",
    "print('\\nTotal Discriminator loss: ', test_losses['Total Discriminator loss'][-1])\n",
    "if ADVERSARIAL:\n",
    "    print('\\nAdversaral loss A: ', test_losses['Adversaral loss A'][-1])\n",
    "    print('\\nAdversaral loss B: ', test_losses['Adversaral loss B'][-1])\n",
    "if CYCLE:\n",
    "    print('\\nCycle consistency loss A: ', test_losses['Cycle consistency loss A'][-1])\n",
    "    print('\\nCycle consistency loss B: ', test_losses['Cycle consistency loss B'][-1])\n",
    "if SUPERVISED:\n",
    "    print('\\nSupervised loss A: ', test_losses['Supervised loss A'][-1])\n",
    "    print('\\nSupervised loss B: ', test_losses['Supervised loss B'][-1])\n",
    "print('\\nTotal Generator loss: ', test_losses['Total Generator loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected losses for the test dataset after the last epoch:\n",
    "\n",
    "\n",
    "L1 loss between real signal A and fake signals A:  0.5614745020866394\n",
    "\n",
    "L1 loss between real signal B and fake signals B:  0.20988085865974426\n",
    "\n",
    "Discrminator A loss:  0.43891608715057373\n",
    "\n",
    "Discrminator B loss:  0.49221402406692505\n",
    "\n",
    "Total Discriminator loss:  0.9311301112174988\n",
    "\n",
    "Adversaral loss A:  0.2902804911136627\n",
    "\n",
    "Adversaral loss B:  0.2568780779838562\n",
    "\n",
    "Cycle consistency loss A:  0.4262011647224426\n",
    "\n",
    "Cycle consistency loss B:  0.06283529847860336\n",
    "\n",
    "Supervised loss A:  0.5173115730285645\n",
    "\n",
    "Supervised loss B:  0.07597765326499939\n",
    "\n",
    "Total Generator loss:  11.370415687561035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load csv\n",
    "# df = pd.read_csv('generated_signals.csv')\n",
    "# df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# # plot df\n",
    "# df.plot(figsize=(12, 6), title='Generated Signals')\n",
    "# # legend\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "\n",
    "# # save plot\n",
    "# plt.savefig('{}_to_{}.png'.format(SIG_A, SIG_B), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load csv\n",
    "# df = pd.read_csv('generated_signals.csv')\n",
    "# df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(20, 5))\n",
    "# plt.plot(df['sig_A'], label= 'sig_A')\n",
    "# plt.plot(df['fake_B'], label= 'fake_B')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
