{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "from create_dataset import AnimalDatasetEmbedding, UnpairedEmbeddingsDataset\n",
    "from generators import  OneHotGenerator, SkipOneHotGenerator, SkipTensorEmbeddingGen, TensorEmbeddingGen, OneHotResNetGenerator\n",
    "from discriminators import PatchDiscriminator, SampleDiscriminator\n",
    "import os\n",
    "import glob\n",
    "import generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phases 1,2,3,4,5, UNET no skipcons, Embedding Bottleneck One Hot, \n",
    "## Patch Discriminator, 50% target set, 200 epochs, LR decay after 100, \n",
    "## Batch Size =1, LR = 2e-4, lambda disc = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "PHASES = [1, 2, 3, 4, 5]\n",
    "UNPAIRED = True \n",
    "\n",
    "\n",
    "UNET = True\n",
    "SKIPCONNECTIONS = False \n",
    "\n",
    "RESNET = False\n",
    "\n",
    "EMBEDDING = True            # Use intervention and phase information during training\n",
    "if EMBEDDING == True:\n",
    "    DOWN = False\n",
    "    BOTTLENECK = True\n",
    "ONEHOTENCODING = True\n",
    "\n",
    "PATCH = True          # False = Patch Discriminator, True = Sample Discriminator\n",
    "SMALLER_TARGET = True   # True = train with target dataset size = PERCENTAGE\n",
    "PERCENTAGE = 0.5        # percentage of target data\n",
    "\n",
    "\n",
    "# LR scheduler\n",
    "MultiStepLR = False\n",
    "GAMMA = 0.1\n",
    "\n",
    "ReduceLROnPlateau = False\n",
    "FACTOR = 0.001\n",
    "PATIENCE = 2\n",
    "\n",
    "PolinomialLR = True\n",
    "power = 1\n",
    "LR_DECAY_AFTER_EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0002 #1e-2 \n",
    "NUM_WORKERS = 16\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] â€“ Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"  # RVtot_kalibriert existiert auch\n",
    "source_signals = [SIG_D]\n",
    "CHANNELS = len(source_signals)\n",
    "WINDOW = 256\n",
    "\n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "\n",
    "# Use adversarial loss\n",
    "LAMBDA_DISC = 0.05\n",
    "GAN_LOSS = True   # adversarial loss\n",
    "LAMBDA_GAN = 1.0\n",
    "# Use cycle consistency loss\n",
    "CYCLE = True\n",
    "LAMBDA_CYCLE = 1.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = False \n",
    "LAMBDA_SUPERVISED = 1.0\n",
    "# Use Identity loss\n",
    "IDENTITY = False\n",
    "LAMBDA_IDENTITY = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all data and preprocess\n",
    "\n",
    "Subsamplen, normalisieren pro Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os. getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6022044, 15)\n"
     ]
    }
   ],
   "source": [
    "#path = \"/home/ubuntu/Master_thesis/Data/\" \n",
    "# csv_file = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Master_thesis/Data/downsampled_normalized_df.csv\" , sep=\";\")\n",
    "  \n",
    "#df = pd.DataFrame()\n",
    "#scaler = StandardScaler() \n",
    "# loop over the list of csv files\n",
    "#for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    #df_temp = pd.read_csv(f, sep=\";\")\n",
    "    #df_temp = utils.drop_cols(df_temp)\n",
    "    #df_temp = df_temp.dropna()\n",
    "    #df_temp = utils.remove_strings(df_temp)\n",
    "    #df_temp = utils.subsample(df_temp, 10)\n",
    "    #df_temp = utils.normalize(df_temp, scaler, phase1 = True)  \n",
    "      \n",
    "    # print the content\n",
    "    #df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "\n",
    "print(df.shape) # (6022044, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select part of data to use in experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset after selecting Phasenzuordnung 1 (6022044, 15)\n",
      "Size of the dataset with data from phase 1 (6022044, 15)\n",
      "Size of Phase 1:  (751120, 15)\n",
      "Size of Phase 2:  (1370162, 15)\n",
      "Size of Phase 3:  (1371092, 15)\n",
      "Size of Phase 4:  (1369343, 15)\n",
      "SIze of Phase 5:  (1160327, 15)\n"
     ]
    }
   ],
   "source": [
    "# select which phases to use\n",
    "df = df.loc[df['Phasenzuordnung'].isin(PHASES)]\n",
    "print('Size of the dataset after selecting Phasenzuordnung 1',df.shape)\n",
    "\n",
    "print('Size of the dataset with data from phase 1',df.shape)\n",
    "print('Size of Phase 1: ', df.loc[df['Phasenzuordnung'] == 1].shape)\n",
    "print('Size of Phase 2: ', df.loc[df['Phasenzuordnung'] == 2].shape)\n",
    "print('Size of Phase 3: ', df.loc[df['Phasenzuordnung'] == 3].shape)\n",
    "print('Size of Phase 4: ', df.loc[df['Phasenzuordnung'] == 4].shape)\n",
    "print('SIze of Phase 5: ', df.loc[df['Phasenzuordnung'] == 5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 3. 6. 2. 4. 5.]\n",
      "Size of intervention 1:  (2162838, 15)\n",
      "Size of intervention 2:  (34544, 15)\n",
      "Size of intervention 3:  (1532302, 15)\n",
      "Size of intervention 4:  (26222, 15)\n",
      "Size of intervention 5:  (19533, 15)\n",
      "Size of intervention 6:  (1212463, 15)\n",
      "[0. 1. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Phasenzuordnung'] == 1:\n",
    "        df.at[index, 'intervention'] = 0\n",
    "    elif row['intervention'] == 10:\n",
    "        if row['contractility'] == 1.0:\n",
    "            df.at[index, 'intervention'] = 0      # contractility = 1.0 - could be ignored? - phase 0?\n",
    "        if row['contractility'] == 3.0:\n",
    "            df.at[index, 'intervention'] = 5      # contractility = 3.0                                        \n",
    "        if row['contractility'] == 4.0:\n",
    "            df.at[index, 'intervention'] = 6    # contractility = 4.0\n",
    "\n",
    "#get unique intervention\n",
    "print(df['intervention'].unique())\n",
    "\n",
    "int_1 = df.loc[df['intervention'] == 1]\n",
    "int_2 = df.loc[df['intervention'] == 2]\n",
    "int_3 = df.loc[df['intervention'] == 3]\n",
    "int_4 = df.loc[df['intervention'] == 4]\n",
    "int_5 = df.loc[df['intervention'] == 5]\n",
    "int_6 = df.loc[df['intervention'] == 6]\n",
    "\n",
    "print('Size of intervention 0: ', int_0.shape)\n",
    "print('Size of intervention 1: ', int_1.shape)\n",
    "print('Size of intervention 2: ', int_2.shape)\n",
    "print('Size of intervention 3: ', int_3.shape)\n",
    "print('Size of intervention 4: ', int_4.shape)\n",
    "print('Size of intervention 5: ', int_5.shape)\n",
    "print('Size of intervention 6: ', int_6.shape)\n",
    "\n",
    "#remove rows with intervention 2, 4 and 6, nearly no data\n",
    "df = df[df.intervention != 2]\n",
    "df = df[df.intervention != 4]\n",
    "df = df[df.intervention != 5]\n",
    "print(df['intervention'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop animals with less than 10 data points\n",
    "We drop animals from the dataframe, if they have less than 10 data points. Initially, we have 56 animals and after dropping those with close to no data points, we are left with 25 animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Number of animals after removing those with less than 10 data points:  25\n",
      "[  8  12  13 108   7  14  11   2  16  10 105 111  17  19   3  15   5  20\n",
      " 113   4 102   1   9 107 101]\n"
     ]
    }
   ],
   "source": [
    "print(len(df['animal'].unique()))\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('Number of animals after removing those with less than 10 data points: ', len(df['animal'].unique()))\n",
    "\n",
    "# get all differnent animals\n",
    "animals = df['animal'].unique()\n",
    "print(animals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test split\n",
    "\n",
    "The 5 test animals represent 20.20924% of the whole data. If we don't use all phases, this number might be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test animal(s): [3, 4, 8, 11, 17]\n",
      "\n",
      "Different animal IDs after removing those that are in the test dataset:  20\n",
      "\n",
      "Train data shape: (4744288, 15)\n",
      "\n",
      "Test data shape: (1197312, 15)\n",
      "\n",
      "The test dataset is 20.15133970647637 percent of the whole data: \n"
     ]
    }
   ],
   "source": [
    "# select animals 3,4,8,11,17 as test animals\n",
    "test_animals = [3,4,8,11,17]\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# change the length of the test data to a multiple of the Window size\n",
    "df_test = df_test.iloc[:len(df_test) - (len(df_test) % WINDOW)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target dataframe with a fraction of dfs data\n",
    "\n",
    "Option 1: 10% of data of each animal -> most likely no data from the last phases\n",
    "\n",
    "Option 2: Take first 10% of data of each phase per animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of df_target: (2372118, 15)\n",
      "\n",
      "Number of animals in df_target: 20\n",
      "\n",
      "Number of phases in df_target: 5\n",
      "2.0000219213378085\n"
     ]
    }
   ],
   "source": [
    "# get first 10% of data of each phase per animal\n",
    "df_target = pd.DataFrame()\n",
    "\n",
    "for animal in df_train['animal'].unique():\n",
    "    for phase in df_train['Phasenzuordnung'].unique():\n",
    "        df_temp = df_train.loc[(df_train['animal'] == animal) & (df_train['Phasenzuordnung'] == phase)]\n",
    "        df_temp = df_temp.iloc[:int(len(df_temp)*PERCENTAGE)]\n",
    "        df_target = pd.concat([df_target, df_temp], axis=0)\n",
    "\n",
    "print('\\nShape of df_target:', df_target.shape)\n",
    "print('\\nNumber of animals in df_target:', len(df_target['animal'].unique()))\n",
    "print('\\nNumber of phases in df_target:', len(df_target['Phasenzuordnung'].unique()))\n",
    "print(df_train.shape[0] / df_target.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Generator and Discriminator\n",
    "\n",
    "We also initialize the 2 optimizers, the 2 Learning rate schedulers, the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "if UNET:\n",
    "    if ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = OneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = OneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipOneHotGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipOneHotGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, WINDOWSIZE=WINDOW, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and SKIPCONNECTIONS:\n",
    "        gen_target = SkipTensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = SkipTensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "    if not ONEHOTENCODING and not SKIPCONNECTIONS:\n",
    "        gen_target = TensorEmbeddingGen(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "        gen_source = TensorEmbeddingGen(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, Down = DOWN, Bottleneck=BOTTLENECK).to(DEVICE)\n",
    "\n",
    "if not UNET:\n",
    "    if ONEHOTENCODING:\n",
    "        gen_target = OneHotResNetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS= 1, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "        gen_source = OneHotResNetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS= CHANNELS, WINDOWSIZE = WINDOW, blocks=6, Down = False, Bottleneck = True)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "if PATCH:\n",
    "    disc_target = PatchDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = PatchDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "if not PATCH:\n",
    "    disc_target = SampleDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = SampleDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# Optimizers \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_source.parameters()) + list(disc_target.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_source.parameters()) + list(gen_target.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "if ReduceLROnPlateau:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_gen,\n",
    "                                                           factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                           min_lr=1e-6,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_disc,\n",
    "                                                            factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                            min_lr=1e-6,\n",
    "                                                    )\n",
    "if MultiStepLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_gen, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "                                                        \n",
    "    disc_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_disc, milestones=[5,6,7,8], gamma=GAMMA)\n",
    "\n",
    "if PolinomialLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = power,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = power,\n",
    "                                                    )\n",
    "\n",
    "# losses\n",
    "l1 = nn.L1Loss() \n",
    "mse = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_poly_lr_decay import PolynomialLRDecay\n",
    "\n",
    "#gen_scheduler = PolynomialLRDecay(opt_gen, max_decay_steps=100, end_learning_rate=0.0000, power=1.0)\n",
    "\n",
    "#disc_scheduler = PolynomialLRDecay(opt_disc, max_decay_steps=100, end_learning_rate=0.0000, power=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnequalDataSetSize(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, target_df, feature_names, target_name, window_length=256):\n",
    "        self.df = df\n",
    "        self.feature_names = feature_names\n",
    "        self.target_name = target_name\n",
    "        self.window_length = window_length\n",
    "        self.target_df = target_df\n",
    "        \n",
    "        self.num_animals = len(np.unique(df[\"animal\"]))\n",
    "        self.animal_dfs = [group[1] for group in df.groupby(\"animal\")]\n",
    "        # get statistics for test dataset\n",
    "        self.animal_lens = [len(an_df) // self.window_length for an_df in self.animal_dfs]\n",
    "        self.animal_cumsum = np.cumsum(self.animal_lens)\n",
    "        self.num_windows = sum(self.animal_lens)\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_windows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        animal_idx = int(np.where(self.animal_cumsum >= idx)[0][0])\n",
    "        animal_df = self.animal_dfs[animal_idx]\n",
    "\n",
    "        # take different windows for the source and target -> unpaired examples\n",
    "        start_idx_source = np.random.randint(0, len(animal_df) - self.window_length - 1)\n",
    "        start_idx_target = np.random.randint(0, len(self.target_df) - self.window_length - 1)\n",
    "        end_idx_source = start_idx_source + self.window_length\n",
    "        end_idx_target = start_idx_target + self.window_length\n",
    "        animal_df_source = animal_df.iloc[start_idx_source: end_idx_source]\n",
    "        animal_df_target = self.target_df.iloc[start_idx_target: end_idx_target]\n",
    "\n",
    "        # extract features\n",
    "        input_df = animal_df_source[self.feature_names]\n",
    "        target_df = animal_df_target[self.target_name]\n",
    "        phase_df_source = animal_df_source[\"Phasenzuordnung\"]   # mit 'Phasenzuordnung' klappt es\n",
    "        intervention_df_source = animal_df_source[\"intervention\"]\n",
    "        phase_df_target = animal_df_target[\"Phasenzuordnung\"]\n",
    "        intervention_df_target = animal_df_target[\"intervention\"]\n",
    "\n",
    "        # to torch\n",
    "        inputs = torch.tensor(input_df.to_numpy()).permute(1, 0)\n",
    "        targets = torch.tensor(target_df.to_numpy()).unsqueeze(0)\n",
    "        phase_source = torch.tensor(phase_df_source.to_numpy()).type(torch.LongTensor)\n",
    "        intervention_source = torch.tensor(intervention_df_source.to_numpy()).type(torch.LongTensor)\n",
    "        phase_target = torch.tensor(phase_df_target.to_numpy()).type(torch.LongTensor)\n",
    "        intervention_target = torch.tensor(intervention_df_target.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "        return inputs, targets, phase_source, intervention_source, phase_target, intervention_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SMALLER_TARGET and UNPAIRED and EMBEDDING:\n",
    "    # create dataset with information of the phases and intervention (embedding information)\n",
    "    train_dataset = UnpairedEmbeddingsDataset(df_train, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "if not SMALLER_TARGET and not UNPAIRED and EMBEDDING:\n",
    "    train_dataset = AnimalDatasetEmbedding(df_train, source_signals, target_name = TARGET,  window_length = WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "if SMALLER_TARGET and UNPAIRED and EMBEDDING:\n",
    "    train_dataset = UnequalDataSetSize(df_train, df_target, source_signals, target_name = TARGET, window_length=WINDOW)\n",
    "    #test_dataset = UnequalDataSetSize(df_test, df_target, source_signals, target_name = TARGET, window_length=WINDOW)\n",
    "    test_dataset = AnimalDatasetEmbedding(df_test, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "#wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "#    project=\"Cycle_GAN\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "#    config={\n",
    "#    \"learning_rate\": LEARNING_RATE,\n",
    "#    \"epochs\": NUM_EPOCHS,\n",
    "#    }\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_signals(fake_target, fake_source, target, source):\n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "def discriminator_loss(disc, reals, fakes):\n",
    "    # calculate how close reals are to being classified as real\n",
    "    real_loss = mse(disc(reals), torch.ones_like(disc(reals)))\n",
    "    # calculate how close fakes are to being classified as fake\n",
    "    fake_loss = mse(disc(fakes), torch.zeros_like(disc(fakes)))\n",
    "    # return the average of real and fake loss\n",
    "    return (real_loss + fake_loss) \n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # discriminator loss\n",
    "    disc_target_loss = discriminator_loss(disc_target, target, fake_target) * LAMBDA_DISC\n",
    "    disc_source_loss = discriminator_loss(disc_source, source, fake_source) * LAMBDA_DISC\n",
    "    disc_loss = (disc_source_loss + disc_target_loss) #/ 2\n",
    "\n",
    "    return disc_loss, disc_source_loss, disc_target_loss\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                  gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\n",
    "                  ):\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    if GAN_LOSS:\n",
    "        g_source_loss = mse(disc_source(fake_source), torch.ones_like(disc_source(fake_source))) \n",
    "        g_target_loss = mse(disc_target(fake_target), torch.ones_like(disc_target(fake_target))) \n",
    "\n",
    "        loss += g_source_loss * LAMBDA_GAN + g_target_loss * LAMBDA_GAN\n",
    "    else:\n",
    "        g_source_loss = torch.tensor(0)\n",
    "        g_target_loss = torch.tensor(0)\n",
    "\n",
    "    if CYCLE:\n",
    "        rec_target = gen_target(fake_source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "        rec_source = gen_source(fake_target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "        cycle_target_loss = l1(target, rec_target)  # l1 loss: Mean absolute error between each element in the input x and target y\n",
    "        cycle_source_loss = l1(source, rec_source)  # l1 loss in cycle GAN paper\n",
    "\n",
    "        loss += cycle_target_loss * LAMBDA_CYCLE + cycle_source_loss * LAMBDA_CYCLE\n",
    "    else:\n",
    "        cycle_target_loss = torch.tensor(0)\n",
    "        cycle_source_loss = torch.tensor(0)\n",
    "\n",
    "    if SUPERVISED:\n",
    "        sup_source_loss = mse(source, fake_source)\n",
    "        sup_target_loss = mse(target, fake_target)\n",
    "\n",
    "        loss += sup_source_loss * LAMBDA_SUPERVISED + sup_target_loss * LAMBDA_SUPERVISED\n",
    "    else:\n",
    "        sup_source_loss = torch.tensor(0)\n",
    "        sup_target_loss = torch.tensor(0)\n",
    "\n",
    "    if IDENTITY:\n",
    "        id_target_loss = l1(target, gen_target(target, source_phase, source_intervention, target_phase, target_intervention))\n",
    "        id_source_loss = l1(source, gen_source(source, source_phase, source_intervention, target_phase, target_intervention))\n",
    "\n",
    "        loss += id_target_loss * LAMBDA_IDENTITY + id_source_loss * LAMBDA_IDENTITY\n",
    "    else:\n",
    "        id_target_loss = torch.tensor(0)\n",
    "        id_source_loss = torch.tensor(0)\n",
    "\n",
    "    return loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6a5adc0390464581e3d1a417b966e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for source, target, source_phase, source_intervention, target_phase, target_intervention in loader:\n",
    "        # convert to float16\n",
    "        source = source.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        target = target.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        source_phase = source_phase.to(DEVICE)\n",
    "        source_intervention = source_intervention.to(DEVICE)\n",
    "        target_phase = target_phase.to(DEVICE)\n",
    "        target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "        #  ------------------------------- #\n",
    "        #  ----- train discriminators ---- #\n",
    "        #  ------------------------------- #\n",
    "        with torch.no_grad():\n",
    "            fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "            fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "\n",
    "        d_loss, disc_source_loss, disc_target_loss = get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target)\n",
    "                                                        # source, target, disc_source, disc_target, fake_source, fake_target\n",
    "\n",
    "        # update gradients of discriminator \n",
    "        opt_disc.zero_grad() \n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "        # d_scaler.scale(d_loss).backward()  \n",
    "               \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "\n",
    "        out = calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                                gen_source, gen_target, disc_source, disc_target, fake_target, fake_source)\n",
    "                            # source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                            # gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\n",
    "\n",
    "        g_loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss = out\n",
    "        # loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        # g_scaler.scale(g_loss).backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    # d_scaler.step(opt_disc)  \n",
    "    # d_scaler.update()\n",
    "\n",
    "    # g_scaler.step(opt_gen) \n",
    "    # g_scaler.update()\n",
    "        \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    # source_losses = []\n",
    "    # target_losses = []\n",
    "    if (epoch+1) % 201 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_source.eval()  # set discriminator to evaluation mode\n",
    "            disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_target.eval()\n",
    "            gen_source.eval()\n",
    "\n",
    "            for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "                # convert to float16\n",
    "                source = source.float()\n",
    "                target = target.float()\n",
    "\n",
    "                # move to GPU\n",
    "                source = source.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "                source_phase = source_phase.to(DEVICE)\n",
    "                source_intervention = source_intervention.to(DEVICE)\n",
    "                target_phase = target_phase.to(DEVICE)\n",
    "                target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "\n",
    "                fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention).detach() # already torch.no_grad()\n",
    "                fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention).detach()\n",
    "\n",
    "                # Get validation loss\n",
    "                l1_source = l1(source, fake_source)\n",
    "                l1_target = l1(target, fake_target)\n",
    "                # source_losses.append(l1_source.item())\n",
    "                # target_losses.append(l1_target.item())\n",
    "\n",
    "                \n",
    "                # generate signals during validation\n",
    "                #gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                # test_real_fake_lossB = l1(target, fake_target)   # l1(sig_B, fake_B)\n",
    "                # test_real_fake_lossA = l1(source, fake_source)\n",
    "\n",
    "                #  ------------------------------- #\n",
    "                #  ----- test discriminators ----- #\n",
    "                #  ------------------------------- #\n",
    "\n",
    "                test_d_loss, test_disc_A_loss, test_disc_B_loss = get_disc_loss(source, target, disc_source, disc_target, fake_source, fake_target)\n",
    "                \n",
    "                # -------------------------------- #\n",
    "                # ------- test generators -------- #\n",
    "                # -------------------------------- # \n",
    "\n",
    "                out = calc_gen_loss(source, target, source_phase, source_intervention, target_phase, target_intervention,\n",
    "                                gen_source, gen_target, disc_source, disc_target, \n",
    "                                fake_target, fake_source\n",
    "                                )\n",
    "                g_lossT, g_A_lossT, g_B_lossT, cycle_B_lossT, cycle_A_lossT, id_B_lossT, id_A_lossT, sup_A_lossT, sup_B_lossT = out\n",
    "        \n",
    "                # gen_signals(fake_target, fake_source, target, source)\n",
    "     \n",
    "\n",
    "    if ReduceLROnPlateau == True:  \n",
    "        disc_scheduler.step(d_loss)\n",
    "        gen_scheduler.step(g_loss)\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if PolinomialLR == True and (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get losses of the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # source_reconstructed = gen_source(gen_target(source, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # target_reconstructed = gen_target(gen_source(target, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # l1_source = l1(source, fake_source)\n",
    "    # l1_target = l1(target, fake_target)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "# get min and max of losses\n",
    "min_max_loss_source = np.min(source_losses), np.max(source_losses)\n",
    "min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# get median of losses\n",
    "median_loss_source = np.median(source_losses)\n",
    "median_loss_target = np.median(target_losses)\n",
    "print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate signals\n",
    "idx = 0\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "    if idx == 5:\n",
    "        break              \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source[:256].cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source[:256].cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target[:256].cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target[:256].cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals\n",
    "idx = 0             \n",
    "phases = df['Phasenzuordnung'].unique()\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in test_loader:\n",
    "    if idx == 5:\n",
    "        break                 \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    # fake_target = gen_source(gen_target(source, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "    # fake_source = gen_target(gen_source(target, source_phase, source_intervention, target_phase, target_intervention), source_phase, source_intervention, target_phase, target_intervention)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source.cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source.cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target.cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target.cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify L1 loss of reconstructed and real signals, per phase and intervention\n",
    "# array([ 0.,  1.,  3., 10.,  2.,  4.,  9.])\n",
    "# df of phase 1\n",
    "df_phase_1 = df_test.loc[df_test['Phasenzuordnung'].isin([1])]\n",
    "df_phase_2 = df_test.loc[df_test['Phasenzuordnung'].isin([2])]\n",
    "df_phase_3 = df_test.loc[df_test['Phasenzuordnung'].isin([3])]\n",
    "df_phase_4 = df_test.loc[df_test['Phasenzuordnung'].isin([4])]\n",
    "df_phase_5 = df_test.loc[df_test['Phasenzuordnung'].isin([5])]\n",
    "\n",
    "# 1.,  3., 10.,  2.,  4.\n",
    "df_int_0 = df_test.loc[df_test['intervention'].isin([0])]\n",
    "df_int_1 = df_test.loc[df_test['intervention'].isin([1])]\n",
    "df_int_2 = df_test.loc[df_test['intervention'].isin([2])]\n",
    "df_int_3 = df_test.loc[df_test['intervention'].isin([3])]\n",
    "df_int_4 = df_test.loc[df_test['intervention'].isin([4])]\n",
    "df_int_5 = df_test.loc[df_test['intervention'].isin([5])]\n",
    "df_int_6 = df_test.loc[df_test['intervention'].isin([6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_dataset = AnimalDatasetEmbedding(df_int_6, source_signals, target_name = TARGET, window_length = WINDOW)\n",
    "verify_loader = DataLoader(verify_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "for source, target, source_phase, source_intervention, target_phase, target_intervention in verify_loader:                \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "    source_phase = source_phase.to(DEVICE)\n",
    "    source_intervention = source_intervention.to(DEVICE)\n",
    "    target_phase = target_phase.to(DEVICE)\n",
    "    target_intervention = target_intervention.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    fake_source = gen_source(target, source_phase, source_intervention, target_phase, target_intervention)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "# get min and max of losses\n",
    "min_max_loss_source = np.min(source_losses), np.max(source_losses)\n",
    "min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# get median of losses\n",
    "median_loss_source = np.median(source_losses)\n",
    "median_loss_target = np.median(target_losses)\n",
    "print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.save_checkpoint(gen_source, opt_gen, path=\"gen_source.pth.tar\")\n",
    "# utils.save_checkpoint(gen_target, opt_gen, path=\"gen_target.pth.tar\")\n",
    "# utils.save_checkpoint(disc_source, opt_disc, path=\"disc_source.pth.tar\")\n",
    "# utils.save_checkpoint(disc_target, opt_disc, path=\"disc_target.pth.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_checkpoint(\"Checkpoints/gen_source.pth.tar\", gen_source, opt_gen, LEARNING_RATE)\n",
    "# utils.load_checkpoint(\"Checkpoints/gen_target.pth.tar\", gen_target, opt_gen, LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
