{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "import create_dataset\n",
    "# from generators import UnetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720947, 13)\n"
     ]
    }
   ],
   "source": [
    "# load only a small part of the data and drop the unnecessary columns\n",
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\"\n",
    "df = utils.load_csv(path)\n",
    "\n",
    "df = utils.drop_cols(df)\n",
    "df = df.dropna()\n",
    "\n",
    "# select only rows where 'Phasenzuordnung' is 1\n",
    "df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "\n",
    "print(df.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDatasetEmbedding(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, feature_names, target_name, test,\n",
    "                 window_length=256):\n",
    "        self.df = df\n",
    "        self.feature_names = feature_names\n",
    "        self.target_name = target_name\n",
    "        self.window_length = window_length\n",
    "        self.test = test\n",
    "        \n",
    "        self.num_animals = len(np.unique(df[\"animal\"]))\n",
    "        self.animal_dfs = [group[1] for group in df.groupby(\"animal\")]\n",
    "        # get statistics for test dataset\n",
    "        self.animal_lens = [len(an_df) // self.window_length for an_df in self.animal_dfs]\n",
    "        self.animal_cumsum = np.cumsum(self.animal_lens)\n",
    "        self.num_windows = sum(self.animal_lens)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        # if self.test:\n",
    "        return self.num_windows\n",
    "        # else:\n",
    "        #     return self.num_animals\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.test:\n",
    "            # look up which test animal the idx corresponds to\n",
    "            animal_idx = int(np.where(self.animal_cumsum >= idx)[0][0])\n",
    "            animal_df = self.animal_dfs[animal_idx]\n",
    "            # look up which part of the test animal the idx corresponds to \n",
    "            if animal_idx > 0:\n",
    "                start_idx = idx - self.animal_cumsum[animal_idx - 1]\n",
    "            else:\n",
    "                start_idx = idx\n",
    "            start_idx *= self.window_length\n",
    "        else:\n",
    "            # animal_df = self.animal_dfs[idx]\n",
    "            animal_idx = int(np.where(self.animal_cumsum >= idx)[0][0])\n",
    "            animal_df = self.animal_dfs[animal_idx]\n",
    "            \n",
    "            # take window\n",
    "            start_idx = np.random.randint(0, len(animal_df) - self.window_length - 1)\n",
    "        end_idx = start_idx + self.window_length\n",
    "        animal_df = animal_df.iloc[start_idx: end_idx]\n",
    "        \n",
    "        # extract features\n",
    "        input_df = animal_df[self.feature_names]\n",
    "        target_df = animal_df[self.target_name]\n",
    "        phase_df = animal_df[\"Phasenzuordnung\"]\n",
    "        \n",
    "        # to torch\n",
    "        inputs = torch.tensor(input_df.to_numpy()).permute(1, 0)\n",
    "        targets = torch.tensor(target_df.to_numpy()).unsqueeze(0)\n",
    "        phase = torch.tensor(phase_df.to_numpy())\n",
    "        \n",
    "        return inputs, targets, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814 2814\n"
     ]
    }
   ],
   "source": [
    "# Verify that the data is loaded properly\n",
    "\n",
    "SIG_A = \"VadQ\"\n",
    "SIG_B = \"LVP\"\n",
    "SIG_C = \"AoQ\"\n",
    "SIG_D = \"AoP\"\n",
    "feature_names = [SIG_A, SIG_B, SIG_C, SIG_D]\n",
    "target = \"LVtot_kalibriert\" \n",
    "\n",
    "train_dataset = AnimalDatasetEmbedding(df, feature_names, target_name = target, test = False, window_length = 256)\n",
    "test_dataset = AnimalDatasetEmbedding(df, feature_names, target_name = target, test = True, window_length = 256)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=10, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=10, pin_memory=True,)\n",
    "\n",
    "# length of train and test dataset\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for 4 chanels:  torch.Size([512, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "def double_conv_pad(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class SkipConnectionsMultiChannelUnetGenerator(nn.Module):\n",
    "    def __init__(self, INPUTCHANNELS, OUTPUTCHANNELS):\n",
    "        super(SkipConnectionsMultiChannelUnetGenerator, self).__init__()\n",
    "        self.maxpool = nn.MaxPool1d((2))  \n",
    "\n",
    "        self.down_conv1 = double_conv_pad(INPUTCHANNELS, 32) \n",
    "        self.down_conv2 = double_conv_pad(32, 64) \n",
    "        self.down_conv3 = double_conv_pad(64, 128)\n",
    "        self.down_conv4 = double_conv_pad(128, 256)\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=5, embedding_dim=32)\n",
    "\n",
    "        self.up_trans1 = nn.ConvTranspose1d(256, 128, kernel_size=(2), stride=2, padding=0)\n",
    "        self.up_conv1 = double_conv_pad(256, 128)\n",
    "        self.up_trans2 = nn.ConvTranspose1d(128, 64, kernel_size=(2), stride=2, padding=0)\n",
    "        self.up_conv2 = double_conv_pad(128, 64)\n",
    "        self.up_trans3 = nn.ConvTranspose1d(64, 32, kernel_size=(2), stride=2, padding=0)\n",
    "        self.up_conv3 = double_conv_pad(64, 32)\n",
    "\n",
    "        self.out = nn.Conv1d(32, OUTPUTCHANNELS, kernel_size=1) # kernel_size must be == 1\n",
    "\n",
    "    def forward(self, input, phase):\n",
    "        # [Batch size, Channels in, Height, Width]\n",
    "        \n",
    "        # downsampling\n",
    "        x1 = self.down_conv1(input)   \n",
    "        x2 = self.maxpool(x1) \n",
    "        x3 = self.down_conv2(x2)  \n",
    "        x4 = self.maxpool(x3) \n",
    "        x5 = self.down_conv3(x4)  \n",
    "        x6 = self.maxpool(x5) \n",
    "        x7 = self.down_conv4(x6)\n",
    "\n",
    "        # upsampling\n",
    "        e = self.embedding(phase)\n",
    "        x7 = x7 + e\n",
    "        x = self.up_trans1(x7)\n",
    "        x = self.up_conv1(torch.cat([x, x5], 1))  # skip connection\n",
    "        x = self.up_trans2(x)\n",
    "        x = self.up_conv2(torch.cat([x, x3], 1))  # skip connection\n",
    "        x = self.up_trans3(x)\n",
    "        x = self.up_conv3(torch.cat([x, x1], 1))  # skip connection\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "CHANNELS = 4 # Channel anpassen!!\n",
    "\n",
    "dummy = torch.LongTensor([[1]])\n",
    "phase = torch.ones_like(dummy)\n",
    "x1 = torch.rand(512, 1, 256)\n",
    "x2 = torch.rand(512, 1, 256)\n",
    "x3 = torch.rand(512, 1, 256)\n",
    "x4 = torch.rand(512, 1, 256)\n",
    "input = torch.cat([x1, x2, x3, x4], 1)\n",
    "# print('Input shpe: ', input.size())\n",
    "model = SkipConnectionsMultiChannelUnetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS= 1)\n",
    "print('\\nOutput for {} chanels: '.format(CHANNELS), model(input, phase).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 4 # Channel anpassen!!\n",
    "\n",
    "def double_conv_pad(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class MultiChannelUnetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiChannelUnetGenerator, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d((1, CHANNELS))\n",
    "\n",
    "        self.down_conv1 = double_conv_pad(CHANNELS, 32) \n",
    "        self.down_conv2 = double_conv_pad(32, 64) \n",
    "        self.down_conv3 = double_conv_pad(64, 128)\n",
    "        self.down_conv4 = double_conv_pad(128, 256)\n",
    "\n",
    "        self.up_trans1 = nn.ConvTranspose1d(256, 128, kernel_size=(CHANNELS), stride=2, padding=0)\n",
    "        self.up_conv1 = double_conv_pad(128, 128)\n",
    "        self.up_trans2 = nn.ConvTranspose1d(128, 64, kernel_size=(CHANNELS), stride=2, padding=0)\n",
    "        self.up_conv2 = double_conv_pad(64, 64)\n",
    "        self.up_trans3 = nn.ConvTranspose1d(64, 32, kernel_size=(CHANNELS), stride=2, padding=0)\n",
    "        self.up_conv3 = double_conv_pad(32, 32)\n",
    "\n",
    "        self.out = nn.Conv1d(32, 1, kernel_size=CHANNELS)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # [Batch size, Channels in, Height, Width]\n",
    "        print(\"Input sizes: \", input.size())\n",
    "        x1 = self.down_conv1(input)\n",
    "        print('x1', x1.size())  \n",
    "        x2 = self.maxpool(x1) \n",
    "        print('x2', x2.size())\n",
    "        x3 = self.down_conv2(x2)  #\n",
    "        print('x3', x3.size())\n",
    "        x4 = self.maxpool(x3) \n",
    "        print('x4', x4.size()) \n",
    "        x5 = self.down_conv3(x4)  #\n",
    "        print('x5', x5.size()) \n",
    "        x6 = self.maxpool(x5)\n",
    "        print('x6', x6.size())  \n",
    "        x7 = self.down_conv4(x6)\n",
    "        print('x7', x7.size())\n",
    "\n",
    "        # # decoder\n",
    "        print(\"Upsampling\")\n",
    "        x = self.up_trans1(x7)\n",
    "        print(x.size())\n",
    "        x = self.up_conv1(x)\n",
    "        x = self.up_trans2(x)\n",
    "        print(x.size())\n",
    "        x = self.up_conv2(x)\n",
    "        x = self.up_trans3(x)\n",
    "        print(x.size())\n",
    "        x = self.up_conv3(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# x1 = torch.rand(512, 1, 1, 256)\n",
    "# x2 = torch.rand(512, 1, 1, 256)\n",
    "\n",
    "x1 = torch.rand(512, 1, 256)\n",
    "x2 = torch.rand(512, 1, 256)\n",
    "x3 = torch.rand(512, 1, 256)\n",
    "x4 = torch.rand(512, 1, 256)\n",
    "input = torch.cat([x1, x2, x3, x4], 1)\n",
    "# print('Input shpe: ', input.size())\n",
    "model = MultiChannelUnetGenerator()\n",
    "print(model(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input sizes:  torch.Size([512, 2, 256])\n",
    "x1 torch.Size([512, 32, 256])\n",
    "x2 torch.Size([512, 32, 128])\n",
    "x3 torch.Size([512, 64, 128])\n",
    "x4 torch.Size([512, 64, 64])\n",
    "x5 torch.Size([512, 128, 64])\n",
    "x6 torch.Size([512, 128, 32])\n",
    "x7 torch.Size([512, 256, 32])\n",
    "Upsampling\n",
    "torch.Size([512, 128, 64])\n",
    "torch.Size([512, 64, 128])\n",
    "torch.Size([512, 32, 256])\n",
    "torch.Size([512, 1, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 512  #1024 didsn't work so well\n",
    "LEARNING_RATE = 1e-5  # 1e-5 was too small for 'LVtot_kalibriert' and 'LVtot'\n",
    "# LAMBDA_IDENTITY = 0.1\n",
    "LAMBDA_CYCLE = 10.0 # try out different values\n",
    "NUM_WORKERS = 10\n",
    "NUM_EPOCHS = 2000\n",
    "LR_DECAY_AFTER_EPOCH = 800\n",
    "GENERATION_AFTER_EPOCH = NUM_EPOCHS # number of epochs after which the model generates a sample\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = False\n",
    "SIG_A = \"AoP\" # \"VADcurrent\" # \"VadQ\" # \"AoP\" # \"LVP\"  \n",
    "SIG_B = \"VadQ\" # \"LVtot\" # \"LVtot_kalibriert\" # \"LVtot\" # \"LVtot_kalibriert\" \n",
    "TARGET = \"LVtot_kalibriert\" \n",
    "CHECKPOINT_GEN_A2B = \"Checkpoints/Generated_data/gen_{}.pth.tar\".format(SIG_B)\n",
    "CHECKPOINT_GEN_B2A = \"Checkpoints/Generated_data/gen_{}.pth.tar\".format(SIG_A)\n",
    "CHECKPOINT_DISC_A =  \"Checkpoints/Generated_data/disc{}.pth.tar\".format(SIG_A)\n",
    "CHECKPOINT_DISC_B =  \"Checkpoints/Generated_data/disc{}.pth.tar\".format(SIG_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths of only a small part of the data\n",
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\" # time: ~ 19 sec\n",
    "\n",
    "# all the data\n",
    "path_1 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_1\"  \n",
    "path_2 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_2\"\n",
    "path_3 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_3\"\n",
    "path_4 = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files/Data_split_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only a small part of the data and drop the unnecessary columns\n",
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports_not_all_the_data/constant_speed_interventions/\"\n",
    "df = utils.load_csv(path)\n",
    "\n",
    "df = utils.drop_cols(df)\n",
    "df = df.dropna()\n",
    "\n",
    "# select only rows where 'Phasenzuordnung' is 1\n",
    "df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "\n",
    "print(df.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data and drop unnecessary columns\n",
    "# We load the data separately, to avoid a Runtime error\n",
    "\n",
    "df_1 = utils.load_csv(path_1)\n",
    "df_1 = utils.drop_cols(df_1)\n",
    "\n",
    "df_2 = utils.load_csv(path_2)\n",
    "df_2 = utils.drop_cols(df_2)\n",
    "\n",
    "df_3 = utils.load_csv(path_3)\n",
    "df_3 = utils.drop_cols(df_3)\n",
    "\n",
    "df_4 = utils.load_csv(path_4)\n",
    "df_4 = utils.drop_cols(df_4)\n",
    "\n",
    "# concatenate the separate dataframes\n",
    "df = pd.concat([df_1, df_2, df_3, df_4], axis=0, ignore_index=True)\n",
    "df = df.dropna()\n",
    "\n",
    "print('Size of the whole dataset',df.shape)\n",
    "# select only rows where 'Phasenzuordnung' is 1\n",
    "df = df.loc[df['Phasenzuordnung'] == 1]\n",
    "print('Size of dataset with only the first phase',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesssing\n",
    "### Removing strings from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.remove_strings(df)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the data by the factor 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.subsample(df, 10)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.normalize(df)\n",
    "# utils.visualize(df, 'AoP', 'LVP', 'VadQ', 'VADcurrent', 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulo 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I apply this currently in the dataframe loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hom many different intervention ids are there?\n",
    "print('\\nDifferent interventions: \\n',df['intervention'].unique())\n",
    "\n",
    "# hom many different animal ids are there?\n",
    "print('\\nDifferent animal IDs: \\n',len(df['animal'].unique()))\n",
    "\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('\\nDifferent animal IDs after removing those with less than 10 data points: \\n',len(df['animal'].unique()))\n",
    "\n",
    "# length of data per animal\n",
    "#print(df.groupby('animal').size())\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# pick 2 random animals for test data\n",
    "test_animals = df['animal'].sample(n=1, random_state=1).unique()\n",
    "\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "print('\\nTest animals:', test_animals)\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: \\n',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "#print('Unique animals in df_train: ',df_train['animal'].unique())\n",
    "#print('Unique animals in df_test: ',df_test['animal'].unique())\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gen_dataset which is a part of the test dataset\n",
    "# df_gen = df_test.sample(frac=0.01, random_state=1)\n",
    "# print('\\nGen data shape:', df_gen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a combined tensor of SIG_A and SIG_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a combined tensor of SIG_A and SIG_B\n",
    "df_A = df[SIG_A]\n",
    "df_B = df[SIG_B]\n",
    "#print(df_A.head)\n",
    "#print(df_B.head)\n",
    "\n",
    "df_A = df_A.to_numpy()\n",
    "df_B = df_B.to_numpy()\n",
    "\n",
    "# split df_A into a 2D array \n",
    "df_A = np.split(df_A, df_A.shape[0], axis=0)\n",
    "df_B = np.split(df_B, df_B.shape[0], axis=0)\n",
    "\n",
    "# create a combined tensor of df_A and df_B\n",
    "combined = np.concatenate((df_A, df_B), axis=1)\n",
    "# to dataframe\n",
    "combined = pd.DataFrame(combined, columns = [SIG_A, SIG_B])\n",
    "print(combined.head)\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the combined tensor in a single plot\n",
    "\n",
    "plt.plot(combined[SIG_A][:256])\n",
    "plt.plot(combined[SIG_B][:256])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_A[:5])\n",
    "print(df_B[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, feature_names, target_name, test,\n",
    "                 window_length=256):\n",
    "        self.df = df\n",
    "        self.feature_names = feature_names\n",
    "        self.target_name = target_name\n",
    "        self.window_length = window_length\n",
    "        self.test = test\n",
    "        \n",
    "        self.num_animals = len(np.unique(df[\"animal\"]))\n",
    "        self.animal_dfs = [group[1] for group in df.groupby(\"animal\")]\n",
    "        # get statistics for test dataset\n",
    "        self.animal_lens = [len(an_df) // self.window_length for an_df in self.animal_dfs]\n",
    "        self.animal_cumsum = np.cumsum(self.animal_lens)\n",
    "        self.num_windows = sum(self.animal_lens)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.test:\n",
    "            return self.num_windows\n",
    "        else:\n",
    "            return self.num_animals\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.test:\n",
    "            # look up which test animal the idx corresponds to\n",
    "            animal_idx = np.where(self.animal_cumsum >= idx)[0]\n",
    "            animal_df = self.animal_dfs[idx]\n",
    "            # look up which part of the test animal the idx corresponds to \n",
    "            if animal_idx > 0:\n",
    "                start_idx = idx - self.animal_cumsum[animal_idx - 1]\n",
    "            else:\n",
    "                start_idx = idx\n",
    "            start_idx *= self.window_length\n",
    "        else:\n",
    "            animal_df = self.animal_dfs[idx]\n",
    "\n",
    "            # take window\n",
    "            start_idx = np.random.randint(0, len(animal_df) - self.window_length - 1)\n",
    "        end_idx = start_idx + self.window_length\n",
    "        animal_df = animal_df.iloc[start_idx: end_idx]\n",
    "        \n",
    "        # extract features\n",
    "        input_df = animal_df[self.feature_names]\n",
    "        target_df = animal_df[self.target_name]\n",
    "        \n",
    "        # to torch\n",
    "        inputs = torch.tensor(input_df.to_numpy())\n",
    "        targets = torch.tensor(target_df.to_numpy())\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "print(len(df_test)/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data is loaded properly\n",
    "\n",
    "SIG_A = \"VadQ\"\n",
    "SIG_B = \"LVP\"\n",
    "SIG_C = \"AoQ\"\n",
    "SIG_D = \"AoP\"\n",
    "feature_names = [SIG_A, SIG_B]\n",
    "target = \"LVtot_kalibriert\" \n",
    "test_F = False\n",
    "test_T = True\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = AnimalDataset(df_train, feature_names, target_name = target, test = False, window_length = 256)\n",
    "test_dataset = AnimalDataset(df_test, feature_names, target_name = target, test = True, window_length = 256)\n",
    "# train_dataset = create_dataset.TestDataset(signal_A=SIG_A, signal_B=SIG_B, df=df_train)\n",
    "# print(len(train_dataset))\n",
    "# train_dataset = TrainDataset(signal_A=SIG_A, signal_B=SIG_B, signal_C = SIG_C, signal_D = SIG_D, target = target, window = 4, df=df_train)\n",
    "# test_dataset = create_dataset.TestDataset(signal_A=SIG_A, signal_B=SIG_B, df=df_test)\n",
    "# print(len(test_dataset))\n",
    "# test_dataset = TrainDataset(signal_A=SIG_A, signal_B=SIG_B, signal_C = SIG_C, signal_D = SIG_D, target = target, window = 4, df=df_test)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=10, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, pin_memory=True,)\n",
    "\n",
    "# length of train and test dataset\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, signal_A, signal_B, signal_C, signal_D, target,  window, df):\n",
    "        self.df = df\n",
    "        self.signal_A = self.df[signal_A]\n",
    "        self.target = target\n",
    "        self.window = window\n",
    "        if window >= 2:\n",
    "            self.signal_B = self.df[signal_B]\n",
    "        if window >= 3:\n",
    "            self.signal_B = self.df[signal_C]\n",
    "        if window >= 4:\n",
    "            self.signal_B = self.df[signal_D]\n",
    "\n",
    "        if window > 4:\n",
    "            print(\"The window determines, how many signals from the input are taken into consideration.\")\n",
    "            print(\"The window should be between 1 and 4\")\n",
    "            \n",
    "\n",
    "        # only data from a single animal per batch \n",
    "        for animal in self.df['animal'].unique():\n",
    "            df_single_animal = self.df[self.df['animal'] == animal]\n",
    "            \n",
    "            # length should be modulo 256 = 0\n",
    "            df_single_animal = df_single_animal.iloc[:-(len(df_single_animal) % 256), :]\n",
    "\n",
    "\n",
    "            if window >= 1:\n",
    "                tensor_A = torch.tensor(df_single_animal[signal_A].values) # creating tensor from df \n",
    "                tensor_A = tensor_A.split(256)  # tensor shape (256, 1)\n",
    "                stack_A = torch.stack(tensor_A).unsqueeze(1)\n",
    "\n",
    "            if window >= 2:\n",
    "                tensor_B = torch.tensor(df_single_animal[signal_B].values) # creating tensor from df \n",
    "                tensor_B = tensor_B.split(256)  # tensor shape (256, 1)\n",
    "                stack_B = torch.stack(tensor_B).unsqueeze(1)\n",
    "\n",
    "            if window >= 3:\n",
    "                tensor_C = torch.tensor(df_single_animal[signal_C].values) # creating tensor from df \n",
    "                tensor_C = tensor_C.split(256)  # tensor shape (256, 1)\n",
    "                stack_C = torch.stack(tensor_C).unsqueeze(1)\n",
    "\n",
    "            if window == 4:\n",
    "                tensor_D = torch.tensor(df_single_animal[signal_D].values) # creating tensor from df \n",
    "                tensor_D = tensor_D.split(256)  # tensor shape (256, 1)\n",
    "                stack_D = torch.stack(tensor_D).unsqueeze(1)\n",
    "\n",
    "            target_tensor = torch.tensor(df_single_animal[target].values) # creating tensor from df \n",
    "            target_tensor = target_tensor.split(256)  # tensor shape (256, 1)\n",
    "            target_stack = torch.stack(target_tensor).unsqueeze(1)\n",
    "                \n",
    "            # The tensor of the first animal is added to self.tensor\n",
    "            if animal == self.df['animal'].unique()[0]:\n",
    "                self.tensor_A = stack_A\n",
    "                self.target = target_stack\n",
    "                if window >= 2:\n",
    "                    self.tensor_B = stack_B\n",
    "                if window >= 3:\n",
    "                    self.tensor_C = stack_C\n",
    "                if window >= 4:\n",
    "                    self.tensor_D = stack_D\n",
    "                \n",
    "            else: # The tensor of each following animal is concatenated to self.tensor\n",
    "                self.tensor_A = torch.cat((self.tensor_A, stack_A), 0)\n",
    "                self.target = torch.cat((self.target, target_stack), 0)\n",
    "                if window >= 2:\n",
    "                    self.tensor_B = torch.cat((self.tensor_B, stack_B), 0)\n",
    "                if window >= 3:\n",
    "                    self.tensor_C = torch.cat((self.tensor_C, stack_C), 0)\n",
    "                if window == 4:\n",
    "                    self.tensor_D = torch.cat((self.tensor_D, stack_D), 0)\n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        # all signals should have the same length\n",
    "        return len(self.tensor_A)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #return the signal at the given index \n",
    "        if self.window == 1: \n",
    "            return self.tensor_A[index], self.target[index] # , self.tensor_B[index], self.tensor_C[index], self.tensor_D[index]\n",
    "        if self.window == 2: \n",
    "            return self.tensor_A[index] , self.tensor_B[index], self.target[index]#, self.tensor_C[index], self.tensor_D[index], \n",
    "        if self.window == 3: \n",
    "            return self.tensor_A[index] , self.tensor_B[index], self.tensor_C[index], self.target[index] # self.tensor_D[index],\n",
    "        if self.window == 4: \n",
    "            return self.tensor_A[index] , self.tensor_B[index], self.tensor_C[index], self.tensor_D[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data should be of shape (Batch, channels = 2, Hight = 1, Width = 256)\n",
    "# Test Dataset includes data from a single animal\n",
    "\n",
    "# class TrainDataset(Dataset):\n",
    "#     def __init__(self, signal_A, signal_B, target,  df):\n",
    "#         self.df = df\n",
    "#         self.signal_A = self.df[signal_A]\n",
    "#         self.signal_B = self.df[signal_B]\n",
    "#         self.target = self.df[target]\n",
    "\n",
    "#         # length should be modulo 256 = 0\n",
    "#         self.df = self.df.iloc[:-(len(self.df) % 256), :]\n",
    "\n",
    "#         # creating tensor from df \n",
    "#         tensor_A = torch.tensor(self.df[signal_A].values)\n",
    "#         tensor_B = torch.tensor(self.df[signal_B].values)\n",
    "#         tensor_target = torch.tensor(self.df[target].values)\n",
    "\n",
    "#         # split tensor into tensors of size 256\n",
    "#         tensor_A = tensor_A.split(256)  # tensor shape (256, 1) \n",
    "#         tensor_B = tensor_B.split(256)   \n",
    "#         tensor_target = tensor_target.split(256)    \n",
    "\n",
    "#         # stack tensors\n",
    "#         self.tensor_A = torch.stack(tensor_A).unsqueeze(1) \n",
    "#         self.tensor_B = torch.stack(tensor_B).unsqueeze(1) \n",
    "#         self.tensor_target = torch.stack(tensor_target).unsqueeze(1)\n",
    "#         # print(self.tensor_A.shape, self.tensor_A.shape)\n",
    "\n",
    "#         self.tensor_A = self.tensor_A.unsqueeze(1)\n",
    "#         self.tensor_B = self.tensor_B.unsqueeze(1)\n",
    "#         self.tensor_target =self.tensor_target.unsqueeze(1)\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # signal_A and signal_B should have the same length\n",
    "#         return len(self.tensor_A)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # return the signal at the given index  # add data augmentation?\n",
    "#         return self.tensor_A[index], self.tensor_B[index], self.tensor_target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, signal_A, signal_B, target,  df):\n",
    "#         self.df = df\n",
    "#         self.signal_A = self.df[signal_A]\n",
    "#         self.signal_B = self.df[signal_B]\n",
    "#         self.target = self.df[target]\n",
    "\n",
    "#         # length should be modulo 256 = 0\n",
    "#         #self.df = self.df.iloc[:-(len(self.df) % 256), :]\n",
    "#         #print(self.df.shape)\n",
    "#         # creating tensor from df \n",
    "#         tensor_A = torch.tensor(self.df[signal_A].values)\n",
    "#         tensor_B = torch.tensor(self.df[signal_B].values)\n",
    "#         tensor_target = torch.tensor(self.df[target].values)\n",
    "\n",
    "#         # split tensor into tensors of size 256\n",
    "#         tensor_A = tensor_A.split(256)  # tensor shape (256, 1) \n",
    "#         tensor_B = tensor_B.split(256)   \n",
    "#         tensor_target = tensor_target.split(256)    \n",
    "\n",
    "#         # stack tensors\n",
    "#         self.tensor_A = torch.stack(tensor_A).unsqueeze(1) \n",
    "#         self.tensor_B = torch.stack(tensor_B).unsqueeze(1) \n",
    "#         self.tensor_target = torch.stack(tensor_target).unsqueeze(1)\n",
    "#         # print(self.tensor_A.shape, self.tensor_A.shape)\n",
    "\n",
    "#         self.tensor_A = self.tensor_A.unsqueeze(1)\n",
    "#         self.tensor_B = self.tensor_B.unsqueeze(1)\n",
    "#         self.tensor_target =self.tensor_target.unsqueeze(1)\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # signal_A and signal_B should have the same length\n",
    "#         return len(self.tensor_A)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # return the signal at the given index  # add data augmentation?\n",
    "#         return self.tensor_A[index], self.tensor_B[index], self.tensor_target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_test))\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels= 2, out_channels= 32, kernel_size = 3, stride = 2, padding =1)\n",
    "        self.conv2 = nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size = 3, stride = 2, padding =1)\n",
    "        self.conv3 = nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size = 3, stride = 2, padding =1)\n",
    "        self.conv4 = nn.Conv2d(in_channels= 128, out_channels= 1, kernel_size = 3, stride = 2, padding =1)\n",
    "        \n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        #x = self.conv5(x)\n",
    "        #x = self.conv6(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "x1 = torch.rand(512, 1, 1, 256)\n",
    "x2 = torch.rand(512, 1, 1, 256)\n",
    "combined_input = torch.cat((x1, x2), dim=1)\n",
    "print(combined_input.shape)\n",
    "discriminator = Discriminator()\n",
    "print(discriminator(combined_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "CHANNELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_pad(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.down_conv1 = double_conv_pad(CHANNELS, 32) \n",
    "        self.down_conv2 = double_conv_pad(32, 64) \n",
    "        self.down_conv3 = double_conv_pad(64, 128)\n",
    "        self.down_conv4 = double_conv_pad(128, 256)\n",
    "\n",
    "        self.up_trans1 = nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv1 = double_conv_pad(256, 128)\n",
    "        self.up_trans2 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = double_conv_pad(128, 64)\n",
    "        self.up_trans3 = nn.ConvTranspose1d(64, 32, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = double_conv_pad(64, 32)\n",
    "\n",
    "        self.out = nn.Conv1d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \n",
    "        # batch_size, channels, tensor_size\n",
    "        # downsampling\n",
    "        input = torch.cat([input1, input2], 1)\n",
    "        x1 = self.down_conv1(input)  \n",
    "        print(x1.size())   \n",
    "        x2 = self.maxpool(x1) \n",
    "        print(x2.size()) \n",
    "        x3 = self.down_conv2(x2)  \n",
    "        print(x3.size()) \n",
    "        x4 = self.maxpool(x3)\n",
    "        print(x4.size())  \n",
    "        x5 = self.down_conv3(x4)\n",
    "        print(x5.size())    \n",
    "        x6 = self.maxpool(x5)\n",
    "        print(x6.size())  \n",
    "        x7 = self.down_conv4(x6)\n",
    "        print(x7.size()) \n",
    "\n",
    "        # upsampling\n",
    "        x = self.up_trans1(x7)\n",
    "        x = self.up_conv1(torch.cat([x, x5], 1))\n",
    "        x = self.up_trans2(x)\n",
    "        x = self.up_conv2(torch.cat([x, x3], 1))\n",
    "        x = self.up_trans3(x)\n",
    "        x = self.up_conv3(torch.cat([x, x1], 1))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "x1 = torch.rand(512, 1, 256)\n",
    "x2 = torch.rand(512, 1, 256)\n",
    "model = UnetGenerator()\n",
    "print(model(x1, x2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_pad(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class Two_channel_UnetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Two_channel_UnetGenerator, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d((1,2))\n",
    "\n",
    "        self.down_conv1 = double_conv_pad(2, 32) \n",
    "        self.down_conv2 = double_conv_pad(32, 64) \n",
    "        self.down_conv3 = double_conv_pad(64, 128)\n",
    "        self.down_conv4 = double_conv_pad(128, 256)\n",
    "\n",
    "        self.up_trans1 = nn.ConvTranspose2d(256, 128, kernel_size=(1,2), stride=2, padding=0)\n",
    "        self.up_conv1 = double_conv_pad(128, 128)\n",
    "        self.up_trans2 = nn.ConvTranspose2d(128, 64, kernel_size=(1,2), stride=2, padding=0)\n",
    "        self.up_conv2 = double_conv_pad(64, 64)\n",
    "        self.up_trans3 = nn.ConvTranspose2d(64, 32, kernel_size=(1,2), stride=2, padding=0)\n",
    "        self.up_conv3 = double_conv_pad(32, 32)\n",
    "\n",
    "        self.out = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, input_A, input_B):\n",
    "        # [Batch size, Channels in, Height, Width]\n",
    "        #print(\"Input sizes: \", input_A.size(), input_B.size())\n",
    "        x1 = self.down_conv1(torch.cat([input_A, input_B], 1))\n",
    "        # print(x1.size())  \n",
    "        x2 = self.maxpool(x1) \n",
    "        # print(x2.size())\n",
    "        x3 = self.down_conv2(x2)  #\n",
    "        # print(x3.size())\n",
    "        x4 = self.maxpool(x3) \n",
    "        # print(x4.size()) \n",
    "        x5 = self.down_conv3(x4)  #\n",
    "        # print(x5.size()) \n",
    "        x6 = self.maxpool(x5)\n",
    "        # print(x6.size())  \n",
    "        x7 = self.down_conv4(x6)\n",
    "        # print(x7.size())\n",
    "\n",
    "        # # decoder\n",
    "        # print(\"Upsampling\")\n",
    "        x = self.up_trans1(x7)\n",
    "        # print(x.size())\n",
    "        x = self.up_conv1(x)\n",
    "        x = self.up_trans2(x)\n",
    "        # print(x.size())\n",
    "        x = self.up_conv2(x)\n",
    "        x = self.up_trans3(x)\n",
    "        # print(x.size())\n",
    "        x = self.up_conv3(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# x1 = torch.rand(512, 1, 1, 256)\n",
    "# x2 = torch.rand(512, 1, 1, 256)\n",
    "\n",
    "x1 = torch.rand(512, 1, 256)\n",
    "x2 = torch.rand(512, 1, 256)\n",
    "model = Two_channel_UnetGenerator()\n",
    "print(model(x1, x2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data we want to generate\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(df_train[SIG_A][:1000], label= SIG_A)\n",
    "plt.plot(df_train[SIG_B][:1000], label= SIG_B)\n",
    "plt.plot(df_train[TARGET][:1000], label= TARGET)\n",
    "plt.title('Comparison of {} and {}'.format(SIG_A, SIG_B))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator and discriminator\n",
    "gen_source = Two_channel_UnetGenerator().to(DEVICE)\n",
    "gen_target = Two_channel_UnetGenerator().to(DEVICE)\n",
    "disc_source = Discriminator().to(DEVICE)\n",
    "disc_target = Discriminator().to(DEVICE)\n",
    "\n",
    "# optimizers for discriminator and generator \n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_source.parameters()) + list(disc_target.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_source.parameters()) + list(gen_target.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# scheduler\n",
    "gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = 1,\n",
    "                                                    )\n",
    "disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = 1,\n",
    "                                                    )\n",
    "\n",
    "# run in float16\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "l1 = nn.L1Loss() # L1 loss for cycle consistency and identity loss\n",
    "mse = nn.MSELoss() # MSE loss for adversarial loss\n",
    "\n",
    "# create datasets \n",
    "dataset = TrainDataset(signal_A=SIG_A, signal_B=SIG_B, target=TARGET, df=df_train)\n",
    "test_dataset = TestDataset(signal_A=SIG_A, signal_B=SIG_B, target=TARGET, df=df_test)\n",
    "#gen_dataset = TestDataset(signal_A=SIG_A, signal_B=SIG_B, target=TARGET, df=df_gen)  \n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "#gen_loader = DataLoader(gen_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "\n",
    "# run in float16\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "train_losses = {\n",
    "            'Discrminator source loss' : [],\n",
    "            'Discrminator target loss' : [],\n",
    "            'Total Discrminator loss' : [],\n",
    "            'Adversaral loss source' : [],\n",
    "            'Adversaral loss target' : [],\n",
    "            'Cycle consistency loss source' : [],\n",
    "            'Cycle consistency loss target' : [],\n",
    "            'Total Generator loss' : [],\n",
    "        }\n",
    "\n",
    "test_losses = {\n",
    "            'Discrminator source loss' : [],\n",
    "            'Discrminator target loss' : [],\n",
    "            'Total Discrminator loss' : [],\n",
    "            'Adversaral loss source' : [],\n",
    "            'Adversaral loss target' : [],\n",
    "            'Cycle consistency loss source' : [],\n",
    "            'Cycle consistency loss target' : [],\n",
    "            'Total Generator loss' : [],\n",
    "            'L1 loss between real source signal and fake source signals' : [],\n",
    "            'L1 loss between real target signal and fake target signals' : [],\n",
    "        }\n",
    "\n",
    "B_reals = 0\n",
    "B_fakes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    for sig_A, sig_B, target in loader:\n",
    "        # convert to float16\n",
    "        sig_A = sig_A.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        sig_B = sig_B.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "        target = target.float()\n",
    "        \n",
    "        # move to GPU\n",
    "        sig_A = sig_A.to(DEVICE)\n",
    "        sig_B = sig_B.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        #  -------------------------------- #\n",
    "        #  ----- train discriminators ----- #\n",
    "        #  -------------------------------- #\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            fake_target = gen_target(sig_A, sig_B) # generate fake target signal\n",
    "            d_target_real = disc_target(torch.cat([sig_A, sig_B], 1).to(DEVICE)) # output of discriminator target for real target signal\n",
    "            \n",
    "            # combine fake target signal with zeros to match the input size of the discriminator\n",
    "            dummy_tensor = torch.zeros(len(sig_A), 1, 1, 256).to(DEVICE)\n",
    "            combined_fake_target = torch.cat([fake_target.detach(), dummy_tensor], 1).to(DEVICE)\n",
    "            d_target_fake = disc_target(combined_fake_target) # output of discriminator target for fake target signal \n",
    "                \n",
    "            B_reals += d_target_real.mean().item()\n",
    "            B_fakes += d_target_fake.mean().item()\n",
    "\n",
    "            # Loss between dicriminator (with real signal) output and 1 - The discriminator should output 1 for real signals\n",
    "            d_target_real_loss = mse(d_target_real, torch.ones_like(d_target_real))  \n",
    "            # Loss between dicriminator (with fake signal) output and 0 - The discriminator should output 0 for fake signals\n",
    "            d_target_fake_loss = mse(d_target_fake, torch.zeros_like(d_target_fake)) \n",
    "            # Total loss for discriminator B\n",
    "            d_target_loss = d_target_real_loss + d_target_fake_loss\n",
    "\n",
    "        \n",
    "            fake_source = gen_source(target, dummy_tensor) # generate fake source signal\n",
    "            combined_true_signals = torch.cat([sig_A, sig_B], 1).to(DEVICE)\n",
    "            d_source_real = disc_source(combined_true_signals) # output of discriminator source for real source signal\n",
    "            \n",
    "            combined_fake_source= torch.cat([fake_source, dummy_tensor], 1).to(DEVICE)\n",
    "            d_source_fake = disc_source(combined_fake_source.detach()) \n",
    "\n",
    "            d_source_real_loss = mse(d_source_real, torch.ones_like(d_source_real)) \n",
    "            d_source_fake_loss = mse(d_source_fake, torch.zeros_like(d_source_fake))  \n",
    "            d_source_loss = d_source_real_loss + d_source_fake_loss\n",
    "\n",
    "            # Total loss for discriminator A\n",
    "            d_loss = d_source_loss + d_target_loss  # in cycle GAN paper they halve the loss\n",
    "\n",
    "        # exit amp.auto_cast() context manager and backpropagate \n",
    "        opt_disc.zero_grad() \n",
    "        d_scaler.scale(d_loss).backward()  \n",
    "        d_scaler.step(opt_disc)  \n",
    "        d_scaler.update()\n",
    "\n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            # ----- adversarial loss for both generators ----- #\n",
    "            d_source_fake = disc_source(combined_fake_source) # disc_source should output 0 for fake source signal \n",
    "            d_target_fake = disc_target(combined_fake_target) # disc_target should output 0 for fake target signal\n",
    "            # loss between discriminator output and 0 - The discriminator should output 0 for fake signals\n",
    "            g_source_loss = mse(d_source_fake, torch.zeros_like(d_source_fake)) # was ones_like before  \n",
    "            g_target_loss = mse(d_target_fake, torch.zeros_like(d_target_fake)) # was ones_like before\n",
    "\n",
    "            # ----- cycle consistency loss ----- #\n",
    "            cycle_target = gen_target(fake_source, dummy_tensor)  \n",
    "            cycle_source = gen_source(fake_target, dummy_tensor) # fake_B = gen_A2B(sig_A)\n",
    "            cycle_target_loss = l1(sig_B, cycle_target)  # l1 loss: Mean absolute error between each element in the input x and target y.\n",
    "            cycle_source_loss = l1(sig_A, cycle_source)\n",
    "\n",
    "\n",
    "            # put it all together\n",
    "            g_loss = (\n",
    "                g_source_loss +\n",
    "                g_target_loss +\n",
    "                cycle_target_loss * LAMBDA_CYCLE +\n",
    "                cycle_source_loss * LAMBDA_CYCLE \n",
    "            )\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(g_loss).backward() \n",
    "        g_scaler.step(opt_gen) \n",
    "        g_scaler.update()\n",
    "\n",
    "    # save losses\n",
    "    train_losses['Discrminator source loss'].append(d_source_loss.item())\n",
    "    train_losses['Discrminator target loss'].append(d_target_loss.item())\n",
    "    train_losses['Total Discrminator loss'].append(d_loss.item())\n",
    "    train_losses['Adversaral loss source'].append(g_source_loss.item())\n",
    "    train_losses['Adversaral loss target'].append(g_target_loss.item())\n",
    "    train_losses['Cycle consistency loss source'].append(cycle_source_loss.item())\n",
    "    train_losses['Cycle consistency loss target'].append(cycle_target_loss.item())\n",
    "    train_losses['Total Generator loss'].append(g_loss.item())  \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    #  validation every 10 epochs\n",
    "    if (epoch+1) % 1 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_source.eval()  # set discriminator to evaluation mode\n",
    "            disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_target.eval()\n",
    "            gen_source.eval()\n",
    "\n",
    "            # store losses for testing\n",
    "            test_Discrminator_source_loss = 0 #\n",
    "            test_Discrminator_target_loss = 0 #\n",
    "            test_Total_Discrminator_loss = 0 #\n",
    "            test_Adversaral_loss_source = 0  #\n",
    "            test_Adversaral_loss_target = 0  #\n",
    "            test_Cycle_consistency_loss_source = 0 #\n",
    "            test_Cycle_consistency_loss_target = 0 #\n",
    "            test_Total_Generator_loss = 0 #\n",
    "            test_L1_real_fake_source = 0  # L1 loss between real signal A and fake signal A\n",
    "            test_L1_real_fake_target = 0  # L1 loss between real signal B and fake signal B\n",
    "\n",
    "            for sig_A, sig_B, target in test_loader:\n",
    "                # convert to float16\n",
    "                sig_A = sig_A.float()\n",
    "                sig_B = sig_B.float()\n",
    "                target = target.float()\n",
    "                # move to GPU\n",
    "                sig_A = sig_A.to(DEVICE)\n",
    "                sig_B = sig_B.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "\n",
    "                dummy_tensor = torch.zeros(len(sig_A), 1, 1, 256).to(DEVICE)\n",
    "                fake_target = gen_target(sig_A, sig_B)\n",
    "                fake_source = gen_source(target, dummy_tensor)\n",
    "        \n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                test_L1_real_fake_target = l1(target, fake_target)\n",
    "                test_L1_real_fake_source = l1(torch.cat([sig_A, sig_B], 1), fake_source)  # maybe switch sig_A and sig_B\n",
    "\n",
    "                # calculate adversarial loss\n",
    "                dummy_fake_target = torch.zeros(len(fake_target), 1, 1, 256).to(DEVICE)  # torch.Size([512, 1, 1, 256])\n",
    "                \n",
    "                disc_target_fake_target = disc_target(torch.cat([fake_target, dummy_fake_target],1))\n",
    "                disc_source_fake_source = disc_source(torch.cat([fake_source, dummy_tensor],1))\n",
    "                test_Adversaral_loss_target = mse(disc_target_fake_target, torch.zeros_like(disc_target_fake_target)) #was ones_like before\n",
    "                test_Adversaral_loss_source = mse(disc_source_fake_source, torch.zeros_like(disc_source_fake_source))\n",
    "\n",
    "                # ----- cycle loss ----- #\n",
    "                cycle_target = gen_target(fake_source, dummy_tensor)  \n",
    "                cycle_source = gen_source(fake_target, dummy_tensor)  \n",
    "                test_Cycle_consistency_loss_target = l1(target, cycle_target)\n",
    "                test_Cycle_consistency_loss_source = l1(torch.cat([sig_A, sig_B], 1), cycle_source)\n",
    "\n",
    "                # ----- discriminator loss ----- #\n",
    "                source = torch.cat([sig_A, sig_B], 1) # torch.Size([512, 2, 1, 256])\n",
    "                # print(disc_source(source).shape)  torch.Size([512, 1, 1, 16])\n",
    "                #print(fake_source.shape)   # torch.Size([512, 1, 1, 256])\n",
    "                dummy = torch.zeros(len(fake_source), 1, 1, 256).to(DEVICE)\n",
    "                cat = torch.cat([fake_source, dummy], 1) \n",
    "                cat_target = torch.cat([target, dummy], 1).to(DEVICE)\n",
    "                cat_fake_target = torch.cat([fake_target, dummy], 1).to(DEVICE)\n",
    "                # print(disc_target(torch.cat([target, dummy], 1)).shape)   # torch.Size([512, 1, 1, 16])\n",
    "                test_Discrminator_source_loss = mse(disc_source(source), torch.ones_like(disc_source(source))) + mse(disc_source(cat), torch.zeros_like(disc_source(cat)))\n",
    "                \n",
    "                # mse(disc_target(cat_target), torch.ones_like(disc_target(cat_target)))\n",
    "                # mse(disc_target(cat_fake_target), torch.zeros_like(disc_target(cat_fake_target)))\n",
    "\n",
    "                test_Discrminator_target_loss = mse(disc_target(cat_target), torch.ones_like(disc_target(cat_target))) + mse(disc_target(cat_fake_target), torch.zeros_like(disc_target(cat_fake_target)))\n",
    "                \n",
    "                # ----- total generator loss ----- #\n",
    "                test_Total_Generator_loss = test_Adversaral_loss_source + test_Adversaral_loss_target + test_Cycle_consistency_loss_target + test_Cycle_consistency_loss_source \n",
    "                \n",
    "                # ----- total discriminator loss ----- #\n",
    "                test_Total_Discrminator_loss = test_Discrminator_source_loss + test_Discrminator_target_loss\n",
    "\n",
    "                # save losses\n",
    "                test_losses['Discrminator source loss'].append(test_Discrminator_source_loss.item())\n",
    "                test_losses['Discrminator target loss'].append(test_Discrminator_target_loss.item())\n",
    "                test_losses['Total Discrminator loss'].append(test_Total_Discrminator_loss.item())\n",
    "                test_losses['Adversaral loss source'].append(test_Adversaral_loss_source.item())\n",
    "                test_losses['Adversaral loss target'].append(test_Adversaral_loss_target.item())\n",
    "                test_losses['Cycle consistency loss source'].append(test_Cycle_consistency_loss_source.item())\n",
    "                test_losses['Cycle consistency loss target'].append(test_Cycle_consistency_loss_target.item())\n",
    "                test_losses['Total Generator loss'].append(test_Total_Generator_loss.item())\n",
    "                test_losses['L1 loss between real source signal and fake source signals'].append(test_L1_real_fake_source.item())\n",
    "                test_losses['L1 loss between real target signal and fake target signals'].append(test_L1_real_fake_target.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- #\n",
    "# -------------- PLOT --------------- #\n",
    "# ----------------------------------- #\n",
    "\n",
    "# Plot training losses in different subplots\n",
    "\n",
    "fig, ax = plt.subplots(4, 1, figsize=(12, 24))\n",
    "ax[0].plot(train_losses['Discrminator source loss'], label= 'Discrminator source loss (Training)')\n",
    "ax[0].plot(train_losses['Discrminator target loss'], label= 'Discrminator target loss (Training)')\n",
    "ax[0].plot(train_losses['Total Discrminator loss'], label= 'Total Discrminator loss (Training)')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "#ax[0].set_title('Training Discriminator Loss')\n",
    "\n",
    "ax[1].plot(train_losses['Adversaral loss source'], label= 'Adversaral loss source (Training)')\n",
    "ax[1].plot(train_losses['Adversaral loss target'], label= 'Adversaral loss target (Training)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "#ax[1].set_title('Training Adversarial Loss')\n",
    "ax[2].plot(train_losses['Cycle consistency loss source'], label= 'Cycle consistency loss source (Training)')\n",
    "ax[2].plot(train_losses['Cycle consistency loss target'], label= 'Cycle consistency loss target (Training)')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend()\n",
    "#ax[2].set_title('Training Cycle Consistency Loss')\n",
    "ax[3].plot(train_losses['Total Generator loss'], label= 'Total Generator loss (Training)')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Loss')\n",
    "ax[3].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 1, figsize=(12, 24))\n",
    "ax[0].plot(test_losses['Discrminator source loss'], label= 'Discrminator source loss (Test)')\n",
    "ax[0].plot(test_losses['Discrminator target loss'], label= 'Discrminator target loss (Test)')\n",
    "ax[0].plot(test_losses['Total Discrminator loss'], label= 'Total Discrminator loss (Test)')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "#ax[4].set_title('Test Discriminator Loss')\n",
    "\n",
    "ax[1].plot(test_losses['Adversaral loss source'], label= 'Adversaral loss source (Test)')\n",
    "ax[1].plot(test_losses['Adversaral loss target'], label= 'Adversaral loss target (Test)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "#ax[5].set_title('Test Adversarial Loss')\n",
    "\n",
    "ax[2].plot(test_losses['Cycle consistency loss source'], label= 'Cycle consistency loss source (Test)')\n",
    "ax[2].plot(test_losses['Cycle consistency loss target'], label= 'Cycle consistency loss target (Test)')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend()\n",
    "#ax[6].set_title('Test Cycle Consistency Loss')\n",
    "\n",
    "ax[3].plot(test_losses['Total Generator loss'], label= 'Total Generator loss (Test)')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Loss')\n",
    "ax[3].legend()\n",
    "#ax[7].set_title('Test Total Generator Loss')\n",
    "\n",
    "ax[4].plot(test_losses['L1 loss between real source signal and fake source signals'], label= 'L1 loss between real source signal and fake source signals (Test)')\n",
    "ax[4].plot(test_losses['L1 loss between real target signal and fake target signals'], label= 'L1 loss between real target signal and fake target signals (Test)')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Loss')\n",
    "ax[4].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #  ------------------------------------- #   \n",
    "            #  ------- Generate fake signals ------- #\n",
    "            #  ------------------------------------- #\n",
    "            \n",
    "            # Generate fake signals after the last epoch\n",
    "            \n",
    "            if (epoch+1) % GENERATION_AFTER_EPOCH == 0:\n",
    "                print('Generate fake signals')\n",
    "                # generate fake signals 10 times\n",
    "                #utils.save_predictions(gen_loader, gen_B, gen_A, fake_A, fake_B, DEVICE, mse)\n",
    "\n",
    "                for sig_A, sig_B in gen_loader:\n",
    "                    \n",
    "                    sig_A = sig_A.float()\n",
    "                    sig_B = sig_B.float()\n",
    "                    sig_A = sig_A.to(DEVICE)\n",
    "                    sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "                    fake_B = gen_B(sig_A)\n",
    "                    fake_A = gen_A(sig_B)\n",
    "\n",
    "                    # plot generated signals and real signals\n",
    "                    #reshape to 1D\n",
    "                    fake_B = fake_B.reshape(-1)\n",
    "                    fake_A = fake_A.reshape(-1)\n",
    "                    sig_A = sig_A.reshape(-1)\n",
    "                    sig_B = sig_B.reshape(-1)\n",
    "\n",
    "                    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "                    ax[0].plot(sig_A.cpu().detach().numpy(), label= 'Real signal A')\n",
    "                    ax[0].plot(fake_A.cpu().detach().numpy(), label= 'Generated signal A')\n",
    "                    ax[0].set_xlabel('Epoch')\n",
    "                    ax[0].set_ylabel('Loss')\n",
    "                    ax[0].legend()\n",
    "\n",
    "                    ax[1].plot(sig_B.cpu().detach().numpy(), label= 'Real signal B')\n",
    "                    ax[1].plot(fake_B.cpu().detach().numpy(), label= 'Generated signal B')\n",
    "                    ax[1].set_xlabel('Epoch')\n",
    "                    ax[1].set_ylabel('Loss')\n",
    "                    ax[1].legend()\n",
    "\n",
    "                    # plot generated signals and real signals\n",
    "                    # plt.figure(figsize=(20, 5))\n",
    "                    # plt.plot(sig_A.cpu().detach().numpy(), label= 'Real signal A')\n",
    "                    # plt.plot(sig_B.cpu().detach().numpy(), label= 'Real signal B')\n",
    "                    # plt.plot(fake_A.cpu().detach().numpy(), label= 'Generated signal A')\n",
    "                    # plt.plot(fake_B.cpu().detach().numpy(), label= 'Generated signal B')\n",
    "                    # plt.title('Generated signals vs real signals')\n",
    "                    # plt.legend()\n",
    "\n",
    "                        \n",
    "                    # save generated signals\n",
    "                    # utils.save_predictions(sig_A, sig_B, fake_A, fake_B, epoch, mse, l1, DEVICE)\n",
    "\n",
    "        # activate training mode again\n",
    "        disc_A.train()  \n",
    "        disc_B.train()\n",
    "        gen_B.train()\n",
    "        gen_A.train()\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "        \n",
    "# ----------------------------------- #\n",
    "# -------------- PLOT --------------- #\n",
    "# ----------------------------------- #\n",
    "\n",
    "# Plot training losses in different subplots\n",
    "\n",
    "fig, ax = plt.subplots(9, 1, figsize=(12, 24))\n",
    "ax[0].plot(train_losses['Discrminator A loss'], label= 'Discrminator A loss (Training)')\n",
    "ax[0].plot(train_losses['Discrminator B loss'], label= 'Discrminator B loss (Training)')\n",
    "ax[0].plot(train_losses['Total Discrminator loss'], label= 'Total Discrminator loss (Training)')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "#ax[0].set_title('Training Discriminator Loss')\n",
    "\n",
    "ax[1].plot(train_losses['Adversaral loss A'], label= 'Adversaral loss A (Training)')\n",
    "ax[1].plot(train_losses['Adversaral loss B'], label= 'Adversaral loss B (Training)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "#ax[1].set_title('Training Adversarial Loss')\n",
    "ax[2].plot(train_losses['Cycle consistency loss A'], label= 'Cycle consistency loss A (Training)')\n",
    "ax[2].plot(train_losses['Cycle consistency loss B'], label= 'Cycle consistency loss B (Training)')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].legend()\n",
    "#ax[2].set_title('Training Cycle Consistency Loss')\n",
    "ax[3].plot(train_losses['Total Generator loss'], label= 'Total Generator loss (Training)')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('Loss')\n",
    "ax[3].legend()\n",
    "#ax[3].set_title('Training Total Generator Loss')\n",
    "\n",
    "# Plot test losses in different subplots\n",
    "\n",
    "ax[4].plot(test_losses['Discrminator A loss'], label= 'Discrminator A loss (Test)')\n",
    "ax[4].plot(test_losses['Discrminator B loss'], label= 'Discrminator B loss (Test)')\n",
    "ax[4].plot(test_losses['Total Discrminator loss'], label= 'Total Discrminator loss (Test)')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('Loss')\n",
    "ax[4].legend()\n",
    "#ax[4].set_title('Test Discriminator Loss')\n",
    "\n",
    "ax[5].plot(test_losses['Adversaral loss A'], label= 'Adversaral loss A (Test)')\n",
    "ax[5].plot(test_losses['Adversaral loss B'], label= 'Adversaral loss B (Test)')\n",
    "ax[5].set_xlabel('Epoch')\n",
    "ax[5].set_ylabel('Loss')\n",
    "ax[5].legend()\n",
    "#ax[5].set_title('Test Adversarial Loss')\n",
    "\n",
    "ax[6].plot(test_losses['Cycle consistency loss A'], label= 'Cycle consistency loss A (Test)')\n",
    "ax[6].plot(test_losses['Cycle consistency loss B'], label= 'Cycle consistency loss B (Test)')\n",
    "ax[6].set_xlabel('Epoch')\n",
    "ax[6].set_ylabel('Loss')\n",
    "ax[6].legend()\n",
    "#ax[6].set_title('Test Cycle Consistency Loss')\n",
    "\n",
    "ax[7].plot(test_losses['Total Generator loss'], label= 'Total Generator loss (Test)')\n",
    "ax[7].set_xlabel('Epoch')\n",
    "ax[7].set_ylabel('Loss')\n",
    "ax[7].legend()\n",
    "#ax[7].set_title('Test Total Generator Loss')\n",
    "\n",
    "ax[8].plot(test_losses['L1 loss between real signal A and fake signals A'], label= 'L1 loss between real signal A and fake signals A (Test)')\n",
    "ax[8].plot(test_losses['L1 loss between real signal B and fake signals B'], label= 'L1 loss between real signal B and fake signals B (Test)')\n",
    "ax[8].set_xlabel('Epoch')\n",
    "ax[8].set_ylabel('Loss')\n",
    "ax[8].legend()\n",
    "#ax[8].set_title('Test L1 Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Selected losses for the test dataset after the last epoch:\\n')\n",
    "print('\\nL1 loss between real signal A and fake signals A: ', test_losses['L1 loss between real signal A and fake signals A'][-1])\n",
    "print('\\nL1 loss between real signal B and fake signals B: ', test_losses['L1 loss between real signal B and fake signals B'][-1])\n",
    "print('\\nDiscrminator A loss: ', test_losses['Discrminator A loss'][-1])\n",
    "print('\\nDiscrminator B loss: ', test_losses['Discrminator B loss'][-1])\n",
    "print('\\nTotal Discriminator loss: ', test_losses['Total Discrminator loss'][-1])\n",
    "print('\\nAdversaral loss A: ', test_losses['Adversaral loss A'][-1])\n",
    "print('\\nAdversaral loss B: ', test_losses['Adversaral loss B'][-1])\n",
    "print('\\nCycle consistency loss A: ', test_losses['Cycle consistency loss A'][-1])\n",
    "print('\\nCycle consistency loss B: ', test_losses['Cycle consistency loss B'][-1])\n",
    "print('\\nTotal Generator loss: ', test_losses['Total Generator loss'][-1])\n",
    "\n",
    "\n",
    "print('Training finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
