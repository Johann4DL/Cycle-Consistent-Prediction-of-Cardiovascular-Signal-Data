{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Argument #6: Padding size should be less than the corresponding input dimension, but got: padding (1, 1) at dimension 1 of input 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 111\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mprint\u001b[39m(gen(x)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 111\u001b[0m     test()\n",
      "Cell \u001b[0;32mIn [14], line 107\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m12\u001b[39m, img_channels, img_size))  \u001b[39m# 12 is the batch size\u001b[39;00m\n\u001b[1;32m    106\u001b[0m gen \u001b[39m=\u001b[39m Generator(img_channels, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mprint\u001b[39m(gen(x)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 95\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial(x)\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n\u001b[0;32m---> 95\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres_blocks(x)\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_blocks:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 13\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode),\n\u001b[1;32m    457\u001b[0m                         weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                         _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #6: Padding size should be less than the corresponding input dimension, but got: padding (1, 1) at dimension 1 of input 3"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=32, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                img_channels,\n",
    "                num_features,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.InstanceNorm1d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    num_features, num_features * 2, kernel_size=3, stride=2, padding=1\n",
    "                ),\n",
    "                ConvBlock(\n",
    "                    num_features * 2,\n",
    "                    num_features * 4,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features * 4) for _ in range(num_residuals)]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    num_features * 4,\n",
    "                    num_features * 2,\n",
    "                    down=False,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                ConvBlock(\n",
    "                    num_features * 2,\n",
    "                    num_features * 1,\n",
    "                    down=False,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last = nn.Conv2d(\n",
    "            num_features * 1,\n",
    "            img_channels,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=3,\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n",
    "\n",
    "\n",
    "def test():\n",
    "    img_channels = 1\n",
    "    img_size = 128\n",
    "    x = torch.randn((12, img_channels, img_size))  # 12 is the batch size\n",
    "    gen = Generator(img_channels, 1)\n",
    "    print(gen(x).shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file semi-colon separated\n",
    "df_1 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_2 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_3 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_4 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_5 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_6 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_2_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_7 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_8 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_9 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_3_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_10 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_4_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_11 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_11_1_4_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_12 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_13 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_14 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_15 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_16 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_17 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_18 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_19 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_4_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_20 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_13_1_5_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_21 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_22 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_23 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_24 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_25 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_26 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_27 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_28 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_29 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_30 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_31 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_32 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_33 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_34 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_16_2_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_35 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_36 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_37 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_38 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_39 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_40 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_41 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_42 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_1_4_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_43 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_2_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_44 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_2_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_45 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_17_2_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_46 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_47 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_48 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_49 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_50 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_51 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_19_1_3_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_52 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_1_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_53 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_1_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_54 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_1_1_1_1_10_2.csv\", sep=\";\")\n",
    "df_55 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_2_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_56 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_2_1_1_1_3_2.csv\", sep=\";\")\n",
    "df_57 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_3_1_1_1_1_2.csv\", sep=\";\")\n",
    "df_58 = pd.read_csv(\"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/LeRntVAD_csv_exports/constant_speed_interventions/intervention_20_1_3_1_1_1_3_2.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate df_1 to df_58 into one dataframe\n",
    "df = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13, df_14, df_15, df_16,\n",
    "                df_17, df_18, df_19, df_20, df_21, df_22, df_23, df_24, df_25, df_26, df_27, df_28, df_29, df_30,\n",
    "                df_31, df_32, df_33, df_34, df_35, df_36, df_37, df_38, df_39, df_40, df_41, df_42, df_43, df_44,\n",
    "                df_45, df_46, df_47, df_48, df_49, df_50, df_51, df_52, df_53, df_54, df_55, df_56, df_57, df_58], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# access columns by name (e.g. df['LVP']) or by index (e.g. df.iloc[:, 0])\n",
    "\n",
    "\n",
    "# input = x\n",
    "# output/target = df['LVP']\n",
    "# drop columns that are not needed (all except 'LVP', 'AoP', 'AoQ', 'intervention', 'LVtot_kalibriert', 'LVtot', 'amimal')\n",
    "df = df.drop(columns=['Time', 'RVtot_kalibriert', 'RVP', 'PaP', 'PaQ',\n",
    "                        'VADspeed', 'VadQ', 'VADcurrent', 'Looperkennung', 'Phasenzuordnung', 'Extrasystolen',\n",
    "                        'Ansaugphase', 'ECGcond', 'ECG', 'RVtot', 'LVV1', 'LVV2', 'LVV3', 'LVV4', 'LVV5',\n",
    "                        'RVV1', 'RVV2', 'RVV3', 'RVV4', 'RVV5', 'Versuchsdatum', 'rep_an', 'rep_sect',\n",
    "                        'contractility', 'preload', 'afterload', 'controller'])\n",
    "\n",
    "# drop rows that contain NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hom many different animal ids are there?\n",
    "print('Different animal IDs',df['animal'].unique())\n",
    "\n",
    "# hom many different intervention ids are there?\n",
    "print('Different interventions',df['intervention'].unique())\n",
    "\n",
    "# length of data per animal\n",
    "print(df.groupby('animal').size())\n",
    "\n",
    "# randomly pick one animal as test data\n",
    "test_animal = df['animal'].sample(n=1, random_state=1).iloc[0]\n",
    "print('Test animal:', test_animal)\n",
    "\n",
    "# train data\n",
    "train = df[df['animal'] != test_animal]\n",
    "print('Train data shape:', train.shape)\n",
    "print('The test data is {} percent of the whole data'.format(100/train.shape[0]*test.shape[0]))\n",
    "\n",
    "df_test = test_animal\n",
    "df_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select only rows where 'intervention' is 1\n",
    "#df = df[df['intervention'] == 1]   # nach schweinen aufteilen? \n",
    "#print(df.shape) \n",
    "\n",
    "# split data into training and test set\n",
    "#df_train, df_test = train_test_split(df, test_size=0.15, shuffle=False, stratify = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the date by a factor of 0.1 to remove noise from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LVP = df_train['LVP']\n",
    "AoP = df_train['AoP']\n",
    "AoQ = df_train['AoQ']\n",
    "LVtot_kalibriert = df_train['LVtot_kalibriert']\n",
    "LVtot = df_train['LVtot']\n",
    "\n",
    "# take the mean of each 10 values in LVP\n",
    "print(LVP.shape)\n",
    "# instantiate as dataframe\n",
    "LVP_mean = pd.DataFrame(np.zeros((LVP.shape[0]//10, 1)))\n",
    "\n",
    "print(LVP_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample data by a factor of 10\n",
    "df_train = df_train.sample(frac=0.1, random_state=1)   # averaging over 10 signals and take the mean of the 10 signals\n",
    "df_test = df_test.sample(frac=0.1, random_state=1)\n",
    "\n",
    "# remove rows so that modulo 100 = 0\n",
    "df_train = df_train.iloc[:-(len(df_train) % 100), :]\n",
    "df_test = df_test.iloc[:-(len(df_test) % 100), :]\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "# averaging over 10 signals and take the mean of the 10 signals\n",
    "df_train = df_train.groupby(df_train.index // 10).mean()\n",
    "df_test = df_test.groupby(df_test.index // 10).mean()\n",
    "\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with sinus and cosinus and/or spike curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#generate sinusoidal signal with 100000 samples\u001b[39;00m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10000\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#generate sinusoidal signal with 100000 samples\n",
    "x = np.linspace(0, 100, 10000)\n",
    "sin = np.sin(x)\n",
    "\n",
    "# plot data\n",
    "plt.plot(x, sin)\n",
    "\n",
    "# generate cosine signal with 100000 samples\n",
    "x = np.linspace(0, 100, 10000)\n",
    "cos = np.cos(x)\n",
    "\n",
    "# plot data\n",
    "plt.plot(x, cos)\n",
    "\n",
    "#create a dataframe with the sine and cosine signals\n",
    "df = pd.DataFrame({'sin': sin, 'cos': cos})\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# split data into training and test set \n",
    "df_train, df_test = train_test_split(df, test_size=0.15, shuffle=False, stratify = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, signal_A, signal_B, df):\n",
    "        self.df = df\n",
    "        self.signal_A = self.df[signal_A]\n",
    "        self.signal_B = self.df[signal_B]\n",
    "\n",
    "        # creating tensor from df \n",
    "        self.tensor_A = torch.tensor(self.df[signal_A].values)\n",
    "        self.tensor_B = torch.tensor(self.df[signal_B].values)\n",
    "\n",
    "        # split tensor into tensors of size 100\n",
    "        self.tensor_A = self.tensor_A.split(100)   # change size to 128 - power of 2 \n",
    "        self.tensor_B = self.tensor_B.split(100)   # split in such a way, that only data from one pig is in one tensor\n",
    "                                                   # add zero padding to tensors that are smaller than 100\n",
    "\n",
    "        self.tensor_A = torch.stack(self.tensor_A).unsqueeze(1) #solves batch size problem\n",
    "        self.tensor_B = torch.stack(self.tensor_B).unsqueeze(1) #solves batch size problem\n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        # signal_A and signal_B should have the same length\n",
    "        return len(self.tensor_A)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return the signal at the given index  # add data augmentation?\n",
    "        return self.tensor_A[index], self.tensor_B[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m1\u001b[39m, channels, tensor_length))  \u001b[39m# 1 is the batch size\u001b[39;00m\n\u001b[1;32m     48\u001b[0m gen \u001b[39m=\u001b[39m Generator(channels, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[39mprint\u001b[39m(gen(x)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [17], line 42\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [17], line 24\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs): # **kwargs: keyword arguments\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose1d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3),   #, padding=1\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3),  #, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x) # skip connection \n",
    "\n",
    "# Generator class with 2 Residual blocks for data of shape (1, 1, 128) \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=32):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            ConvBlock(in_channels, features, kernel_size=7, stride=1, padding=3), #\n",
    "            ConvBlock(features, features * 2, kernel_size=3, stride=2, padding=1),\n",
    "            ConvBlock(features * 2, features * 4, kernel_size=3, stride=2, padding=1),\n",
    "            *[ResidualBlock(features * 4) for _ in range(9)],\n",
    "            ConvBlock(features * 4, features * 2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvBlock(features * 2, features, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ConvBlock(features, out_channels, use_act=False, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "    \n",
    "channels = 1\n",
    "tensor_length = 128\n",
    "x = torch.randn((1, channels, tensor_length))  # 1 is the batch size\n",
    "gen = Generator(channels, 1)\n",
    "print(gen(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "LAMBDA_IDENTITY = 0.0\n",
    "LAMBDA_CYCLE = 10.0\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 50\n",
    "GENERATION_AFTER_EPOCH = 50 # number of epochs after which the model generates a sample\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = False\n",
    "SIG_A = 'sin' #\"LVtot_kalibriert\"\n",
    "SIG_B = 'cos' #\"LVtot\"\n",
    "CHECKPOINT_GEN_A2B = \"Checkpoints/Generated_data/gen_{}.pth.tar\".format(SIG_B)\n",
    "CHECKPOINT_GEN_B2A = \"Checkpoints/Generated_data/gen_{}.pth.tar\".format(SIG_A)\n",
    "CHECKPOINT_DISC_A =  \"Checkpoints/Generated_data/disc{}.pth.tar\".format(SIG_A)\n",
    "CHECKPOINT_DISC_B =  \"Checkpoints/Generated_data/disc{}.pth.tar\".format(SIG_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network with supervised learning only (only gen A2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator and discriminator\n",
    "gen_A2B = Generator(in_channels=1, out_channels=1).to(DEVICE)\n",
    "gen_B2A = Generator(in_channels=1, out_channels=1).to(DEVICE)\n",
    "disc_A = Discriminator(in_channels=1).to(DEVICE)\n",
    "disc_B = Discriminator(in_channels=1).to(DEVICE)\n",
    "\n",
    "# optimizers for discriminator and generator \n",
    "opt_disc = torch.optim.AdamW(                                           # Adam statt AdamW \n",
    "    list(disc_A.parameters()) + list(disc_B.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    betas=(0.9, 0.999) # wofuer steht betas?  # betas auskommentieren? schau in wave unet \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_A2B.parameters()) + list(gen_B2A.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "l1 = nn.L1Loss() # L1 loss for cycle consistency and identity loss\n",
    "mse = nn.MSELoss() # MSE loss for adversarial loss\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    utils.load_checkpoint(\n",
    "        CHECKPOINT_GEN_B2A, gen_A2B, opt_gen, LEARNING_RATE,\n",
    "    )\n",
    "    utils.load_checkpoint(\n",
    "        CHECKPOINT_GEN_A2B, gen_B2A, opt_gen, LEARNING_RATE,\n",
    "    )\n",
    "    utils.load_checkpoint(\n",
    "        CHECKPOINT_DISC_A, disc_A, opt_disc, LEARNING_RATE,\n",
    "    )\n",
    "    utils.load_checkpoint(\n",
    "        CHECKPOINT_DISC_B, disc_B, opt_disc, LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "# train and test data\n",
    "print('Shape of train df: ', df_train.shape, '\\n') # df_train is a global variable that contains the data\n",
    "print('Shape of test dataframe: ', df_test.shape)\n",
    "\n",
    "# create datasets with class SignalDataset\n",
    "dataset = SignalDataset(signal_A=SIG_A, signal_B=SIG_B, df=df_train)\n",
    "test_dataset = SignalDataset(signal_A=SIG_A, signal_B=SIG_B, df=df_test)\n",
    "# Data loader to generate fake signals with batch size = 1. We use the same df as the test dataset\n",
    "# This is only to check if the model is able to generate signals that look like real signals\n",
    "# currently its mostly garbage\n",
    "gen_dataset = SignalDataset(signal_A=SIG_A, signal_B=SIG_B, df=df_test)\n",
    "\n",
    "# Data loader\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True,)\n",
    "gen_loader = DataLoader(gen_dataset, batch_size=1, shuffle=False, pin_memory=True,)\n",
    "\n",
    "# run in float16\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# store losses in dictionaries\n",
    "train_losses = {\n",
    "    'cycle_B_loss': [],\n",
    "    'cycle_A_loss': [],\n",
    "    'identity_A_loss': [],\n",
    "    'identity_B_loss': [],\n",
    "    'disc_A_loss': [],\n",
    "    'disc_B_loss': [],\n",
    "    'gen_A2B_loss': [],\n",
    "    'gen_B2A_loss': [],\n",
    "    'd_loss': [],\n",
    "    'g_loss': [],\n",
    "}\n",
    "\n",
    "test_losses = {\n",
    "    'cycle_B_loss': [],\n",
    "    'cycle_A_loss': [],\n",
    "    'disc_A_loss': [],\n",
    "    'disc_B_loss': [],\n",
    "    'gen_A2B_loss': [],\n",
    "    'gen_B2A_loss': [],\n",
    "    'd_loss': [],\n",
    "    'g_loss': [],\n",
    "    'mse_G_A2B': [],\n",
    "    'mse_G_B2A': [],\n",
    "}\n",
    "\n",
    "# --------------------------------- #\n",
    "# ------------ Training ----------- #\n",
    "# --------------------------------- #\n",
    "\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    for sig_A, sig_B in loader:\n",
    "        # convert to float16\n",
    "        sig_A = sig_A.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        sig_B = sig_B.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        sig_A = sig_A.to(DEVICE)\n",
    "        sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "        #  -------------------------------- #\n",
    "        #  ----- train discriminators ----- #\n",
    "        #  -------------------------------- #\n",
    "        with torch.cuda.amp.autocast(): # necessary for float16\n",
    "\n",
    "            fake_B = gen_A2B(sig_A) # generate fake signal B\n",
    "            d_B_real = disc_B(sig_B) # output of discriminator B for real signal B\n",
    "            d_B_fake = disc_B(fake_B.detach()) # output of discriminator B for fake signal B (detached from generator)\n",
    "\n",
    "            # Loss between dicriminator (with real signal) output and 1 - The discriminator should output 1 for real signals\n",
    "            d_B_real_loss = mse(d_B_real, torch.ones_like(d_B_real))  \n",
    "            # Loss between dicriminator (with fake signal) output and 0 - The discriminator should output 0 for fake signals\n",
    "            d_B_fake_loss = mse(d_B_fake, torch.zeros_like(d_B_fake)) \n",
    "            # Total loss for discriminator B\n",
    "            d_B_loss = d_B_real_loss + d_B_fake_loss\n",
    "\n",
    "            fake_A = gen_B2A(sig_B)\n",
    "            d_A_real = disc_A(sig_A)\n",
    "            d_A_fake = disc_A(fake_A.detach()) \n",
    "            d_A_real_loss = mse(d_A_real, torch.ones_like(d_A_real)) \n",
    "            d_A_fake_loss = mse(d_A_fake, torch.zeros_like(d_A_fake))  \n",
    "            d_A_loss = d_A_real_loss + d_A_fake_loss\n",
    "\n",
    "            # Total loss for discriminator A\n",
    "            d_loss = d_A_loss + d_B_loss  # in cycle GAN paper they halve the loss\n",
    "\n",
    "        # exit amp.auto_cast() context manager and backpropagate \n",
    "        opt_disc.zero_grad() \n",
    "        d_scaler.scale(d_loss).backward()  \n",
    "        d_scaler.step(opt_disc)  \n",
    "        d_scaler.update()\n",
    "        \n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            # ----- adversarial loss for both generators ----- #\n",
    "            d_A_fake = disc_A(fake_A) # disc_A should output 0 for fake signal A\n",
    "            d_B_fake = disc_B(fake_B) # disc_B should output 0 for fake signal B\n",
    "            # loss between discriminator output and 0 - The discriminator should output 0 for fake signals\n",
    "            g_A_loss = mse(d_A_fake, torch.zeros_like(d_A_fake)) # was ones_like before  \n",
    "            g_B_loss = mse(d_B_fake, torch.zeros_like(d_B_fake)) # was ones_like before\n",
    "\n",
    "            # ----- cycle consistency loss ----- #\n",
    "            cycle_B = gen_A2B(fake_A) # fake_A = gen_B2A(sig_B)  \n",
    "            cycle_A = gen_B2A(fake_B) # fake_B = gen_A2B(sig_A)\n",
    "            cycle_B_loss = l1(sig_B, cycle_B)  # l1 loss: Mean absolute error between each element in the input x and target y.\n",
    "            cycle_A_loss = l1(sig_A, cycle_A)\n",
    "\n",
    "            # ----- identity loss ----- #\n",
    "            id_B = gen_A2B(sig_B) \n",
    "            id_A = gen_B2A(sig_A)\n",
    "            id_B_loss = l1(sig_B, id_B)\n",
    "            id_A_loss = l1(sig_A, id_A)\n",
    "\n",
    "            # put it all together\n",
    "            g_loss = (\n",
    "                g_A_loss +\n",
    "                g_B_loss +\n",
    "                cycle_B_loss * LAMBDA_CYCLE +\n",
    "                cycle_A_loss * LAMBDA_CYCLE +\n",
    "                id_B_loss * LAMBDA_IDENTITY +  # LAMBDA_IDENTITY = 0.0 -> no identity loss \n",
    "                id_A_loss * LAMBDA_IDENTITY    # we could remove it to increase training speed\n",
    "            )\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(g_loss).backward() \n",
    "        g_scaler.step(opt_gen) \n",
    "        g_scaler.update()\n",
    "    \n",
    "    # save losses\n",
    "    train_losses['d_loss'].append(d_loss.item())\n",
    "    train_losses['g_loss'].append(g_loss.item())\n",
    "    train_losses['cycle_B_loss'].append(cycle_B_loss.item())\n",
    "    train_losses['cycle_A_loss'].append(cycle_A_loss.item())\n",
    "    #train_losses['identity_A_loss'].append(id_A_loss.item())\n",
    "    #train_losses['identity_B_loss'].append(id_B_loss.item())\n",
    "    train_losses['disc_A_loss'].append(d_A_loss.item())\n",
    "    train_losses['disc_B_loss'].append(d_B_loss.item())\n",
    "    train_losses['gen_A2B_loss'].append(g_A_loss.item())\n",
    "    train_losses['gen_B2A_loss'].append(g_B_loss.item())\n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    #  validation every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_A.eval()  # set discriminator to evaluation mode\n",
    "            disc_B.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_A2B.eval()\n",
    "            gen_B2A.eval()\n",
    "\n",
    "            # store losses for testing\n",
    "            test_mse_G_A2B = 0\n",
    "            test_mse_G_B2A = 0\n",
    "            test_g_A_loss = 0\n",
    "            test_g_B_loss = 0\n",
    "            test_cycle_B_loss = 0\n",
    "            test_cycle_A_loss = 0\n",
    "            test_d_A_loss = 0\n",
    "            test_d_B_loss = 0\n",
    "            test_g_loss = 0\n",
    "            test_d_loss = 0\n",
    "\n",
    "            for sig_A, sig_B in test_loader:\n",
    "                # convert to float16\n",
    "                sig_A = sig_A.float()\n",
    "                sig_B = sig_B.float()\n",
    "\n",
    "                # move to GPU\n",
    "                sig_A = sig_A.to(DEVICE)\n",
    "                sig_B = sig_B.to(DEVICE)\n",
    "\n",
    "                fake_B = gen_A2B(sig_A)\n",
    "                fake_A = gen_B2A(sig_B)\n",
    "        \n",
    "                # calculate mse loss of fake signals and real signals\n",
    "                test_mse_G_A2B = mse(sig_B, fake_B)\n",
    "                test_mse_G_B2A = mse(sig_A, fake_A)\n",
    "\n",
    "                # calculate Generator loss\n",
    "                test_g_A_loss = mse(disc_B(fake_B), torch.zeros_like(disc_B(fake_B))) #was ones_like before\n",
    "                test_g_B_loss = mse(disc_A(fake_A), torch.zeros_like(disc_A(fake_A)))\n",
    "\n",
    "                # ----- cycle loss ----- #\n",
    "                cycle_B = gen_A2B(fake_A)  # fake_A = gen_B2A(sig_B)\n",
    "                cycle_A = gen_B2A(fake_B)  # fake_B = gen_A2B(sig_A)\n",
    "                test_cycle_B_loss = l1(sig_B, cycle_B)\n",
    "                test_cycle_A_loss = l1(sig_A, cycle_A)\n",
    "\n",
    "                # ----- identity loss ----- #\n",
    "                # I am not tracking the identity loss because it is 0\n",
    "                #id_B = gen_A2B(sig_B)\n",
    "                #id_A = gen_B2A(sig_A)\n",
    "                #id_B_loss = l1(sig_B, id_B)\n",
    "                #id_A_loss = l1(sig_A, id_A)\n",
    "\n",
    "                # ----- discriminator loss ----- #\n",
    "                test_d_A_loss = mse(disc_A(sig_A), torch.ones_like(disc_A(sig_A))) + mse(disc_A(fake_A), torch.zeros_like(disc_A(fake_A)))\n",
    "                test_d_B_loss = mse(disc_B(sig_B), torch.ones_like(disc_B(sig_B))) + mse(disc_B(fake_B), torch.zeros_like(disc_B(fake_B)))\n",
    "                \n",
    "                # ----- generator loss ----- #\n",
    "                test_g_loss = test_g_A_loss + test_g_B_loss + test_cycle_B_loss + test_cycle_A_loss #+ id_B_loss + id_A_loss\n",
    "                test_d_loss = (test_d_A_loss + test_d_B_loss) / 2\n",
    "\n",
    "                # save losses\n",
    "                test_losses['d_loss'].append(test_d_loss.item())\n",
    "                test_losses['g_loss'].append(test_g_loss.item())\n",
    "                test_losses['cycle_B_loss'].append(test_cycle_B_loss.item())\n",
    "                test_losses['cycle_A_loss'].append(test_cycle_A_loss.item())\n",
    "                test_losses['disc_A_loss'].append(test_d_A_loss.item())\n",
    "                test_losses['disc_B_loss'].append(test_d_B_loss.item())\n",
    "                test_losses['gen_A2B_loss'].append(test_g_A_loss.item())\n",
    "                test_losses['gen_B2A_loss'].append(test_g_B_loss.item())\n",
    "                test_losses['mse_G_A2B'].append(test_mse_G_A2B.item()/len(test_loader))\n",
    "                test_losses['mse_G_B2A'].append(test_mse_G_B2A.item()/len(test_loader))\n",
    "\n",
    "            #  ------------------------------------- #   \n",
    "            #  ------- Generate fake signals ------- #\n",
    "            #  ------------------------------------- #\n",
    "            \n",
    "            # generate fake signals every 100 epochs\n",
    "            # Generation is in the validation loop because this was easier to implement\n",
    "            \n",
    "            if (epoch+1) % GENERATION_AFTER_EPOCH == 0:\n",
    "                print('Generate fake signals')\n",
    "                utils.save_predictions(gen_loader, gen_A2B, gen_B2A, fake_A, fake_B, DEVICE, mse)\n",
    "                print('Check')\n",
    "\n",
    "        # activate training mode again\n",
    "        disc_A.train()  \n",
    "        disc_B.train()\n",
    "        gen_A2B.train()\n",
    "        gen_B2A.train()\n",
    "\n",
    "\n",
    "# ----------------------------------- #\n",
    "# -------------- PLOT --------------- #\n",
    "# ----------------------------------- #\n",
    "\n",
    "# plot losses for each epoch in subplots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "ax[0].plot(train_losses['g_loss'], label='Generator loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Generator Loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(train_losses['d_loss'], label='Discriminator')\n",
    "ax[1].plot(train_losses['cycle_B_loss'], label='cycle B')\n",
    "ax[1].plot(train_losses['cycle_A_loss'], label='cycle A')\n",
    "ax[1].plot(train_losses['disc_A_loss'], label='Discriminator A')\n",
    "ax[1].plot(train_losses['disc_B_loss'], label='Discriminator B')\n",
    "ax[1].plot(train_losses['gen_A2B_loss'], label='Generator A2B')\n",
    "ax[1].plot(train_losses['gen_B2A_loss'], label='Generator B2A')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Training Losses')\n",
    "ax[1].legend()\n",
    "ax[2].plot(test_losses['d_loss'], label='Discriminator')\n",
    "ax[2].plot(test_losses['g_loss'], label='Generator')\n",
    "ax[2].plot(test_losses['cycle_B_loss'], label='cycle B')\n",
    "ax[2].plot(test_losses['cycle_A_loss'], label='cycle A')\n",
    "ax[2].plot(test_losses['disc_A_loss'], label='Discriminator A')\n",
    "ax[2].plot(test_losses['disc_B_loss'], label='Discriminator B')\n",
    "ax[2].plot(test_losses['gen_A2B_loss'], label='Generator A2B')\n",
    "ax[2].plot(test_losses['gen_B2A_loss'], label='Generator B2A')\n",
    "ax[2].plot(test_losses['mse_G_A2B'], label='MSE of Generator A2B')\n",
    "ax[2].plot(test_losses['mse_G_B2A'], label='MSE of Generator B2A')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Loss')\n",
    "ax[2].set_title('Test Losses')\n",
    "ax[2].legend()\n",
    "#plt.savefig(f'./losses/losses_{epoch}.png')\n",
    "\n",
    "\n",
    "# ----------------------------------- #\n",
    "# ----------- Save Model------------- #\n",
    "# ----------------------------------- #\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    utils.save_checkpoint(gen_A2B, opt_gen, path=CHECKPOINT_GEN_A2B)\n",
    "    utils.save_checkpoint(gen_B2A, opt_gen, path=CHECKPOINT_GEN_B2A)\n",
    "    utils.save_checkpoint(disc_A, opt_disc, path=CHECKPOINT_DISC_A)\n",
    "    utils.save_checkpoint(disc_B, opt_disc, path=CHECKPOINT_DISC_B)\n",
    "\n",
    "    # print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('generated_signals.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# plot df\n",
    "df.plot(figsize=(12, 6), title='Generated Signals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LVP to AoP\n",
    "\n",
    "Epoch [1/5], Loss D: 0.3012, loss G: 619.1838\n",
    "\n",
    "Epoch [2/5], Loss D: 0.3729, loss G: 1133.2758\n",
    "\n",
    "Epoch [3/5], Loss D: 0.2997, loss G: 818.2902\n",
    "\n",
    "Epoch [4/5], Loss D: 0.3086, loss G: 794.9103\n",
    "\n",
    "Epoch [5/5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AoP to AoQ\n",
    "\n",
    "Epoch [1/10], Loss D: 0.3331, loss G: 763.7483\n",
    "\n",
    "Epoch [2/10], Loss D: 0.3324, loss G: 528.8701\n",
    "\n",
    "Epoch [3/10], Loss D: 0.3328, loss G: 416.0652\n",
    "\n",
    "Epoch [4/10], Loss D: 0.3331, loss G: 541.8179\n",
    "\n",
    "Epoch [5/10], Loss D: 0.3312, loss G: 500.3121\n",
    "\n",
    "Epoch [6/10], Loss D: 0.3270, loss G: 490.1998\n",
    "\n",
    "Epoch [7/10], Loss D: 0.3597, loss G: 390.4794\n",
    "\n",
    "Epoch [8/10], Loss D: 0.3440, loss G: 403.8621\n",
    "\n",
    "Epoch [9/10], Loss D: 0.3366, loss G: 566.6377\n",
    "\n",
    "Epoch [10/10], Loss D: 0.3217, loss G: 452.6429"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
