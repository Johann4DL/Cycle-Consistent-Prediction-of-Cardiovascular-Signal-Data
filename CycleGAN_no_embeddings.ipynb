{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/johann/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975993/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import wandb\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import load_csv, drop_cols, remove_strings, groupedAvg, subsample, normalize\n",
    "from create_dataset import AnimalDataset, AnimalDatasetEmbedding, UnpairedDataset\n",
    "from generators import BasicGenerator, SkipConGenerator, OneHotGenerator, ResNetGenerator\n",
    "from discriminators import PatchDiscriminator, SampleDiscriminator\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receptive field of discriminator\n",
    "\n",
    "# def f(output_size, ksize, stride):\n",
    "#     return (output_size - 1) * stride + ksize\n",
    "\n",
    "# last_layer = f(output_size=1, ksize=4, stride=2)\n",
    "# print(last_layer)\n",
    "# # Receptive field: 3\n",
    "# fourth_layer = f(output_size=last_layer, ksize=3, stride=2)\n",
    "# print(fourth_layer)\n",
    "# # Receptive field: 7\n",
    "# third_layer = f(output_size=fourth_layer, ksize=4, stride=3)\n",
    "# print(third_layer)\n",
    "# # Receptive field: 15\n",
    "# second_layer = f(output_size=third_layer, ksize=4, stride=3)\n",
    "# print(second_layer)\n",
    "# # Receptive field: 31\n",
    "# first_layer = f(output_size=second_layer, ksize=4, stride=3)\n",
    "# # Receptive field: 63\n",
    "# print(first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "UNPAIRED = True \n",
    "PHASES = [1]\n",
    "SKIPCONNECTIONS = False\n",
    "\n",
    "UNET = True\n",
    "RESNET = False\n",
    "BLOCKS = 6\n",
    "\n",
    "PATCH = True\n",
    "\n",
    "# LR scheduler\n",
    "MultiStepLR = False\n",
    "GAMMA = 0.1\n",
    "\n",
    "ReduceLROnPlateau = False\n",
    "FACTOR = 1e-5\n",
    "PATIENCE = 2\n",
    "\n",
    "PolinomialLR = True\n",
    "power = 1\n",
    "LR_DECAY_AFTER_EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "# DISCRIMINATOR = MultiChannelDiscriminator\n",
    "# GENERATOR = BasicGenerator   # BasicGenerator, SkipConGenerator\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0002 #1e-2\n",
    "NUM_WORKERS = 16\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "SIG_A = \"AoP\"           # Drucksignal Hauptschlagader = Aortendruck\n",
    "SIG_B = \"VADcurrent\"    # VAD Strom [A] – Pumpemstrom in Ampere\n",
    "SIG_C = \"VadQ\"          # Fluss durch VAD (VAD = Ventrikular assistance device = Pumpe) = Pumpenfluss\n",
    "SIG_D = \"LVP\"           # Ventrikeldruck links = Drucksignal der linken Herzkammer\n",
    "TARGET = \"LVtot_kalibriert\"  # RVtot_kalibriert existiert auch\n",
    "source_signals = [SIG_D]\n",
    "CHANNELS = len(source_signals)\n",
    "WINDOW = 256\n",
    "\n",
    "# Use adversarial loss\n",
    "LAMBDA_DISC = 0.05\n",
    "GAN_LOSS = True   # adversarial loss\n",
    "LAMBDA_GAN = 1.0\n",
    "# Use cycle consistency loss\n",
    "CYCLE = True\n",
    "LAMBDA_CYCLE = 1.0\n",
    "# Use supervised loss\n",
    "SUPERVISED = False \n",
    "LAMBDA_SUPERVISED = 1.0\n",
    "# Use Identity loss\n",
    "IDENTITY = False\n",
    "LAMBDA_IDENTITY = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all data and preprocess\n",
    "\n",
    "Subsamplen, normalisieren pro Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6022044, 14)\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/johann/Desktop/Uni/Masterarbeit/Cycle_GAN/csv_export_files_alle_Daten/csv_export_files\" \n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "  \n",
    "df = pd.DataFrame()\n",
    "scaler = StandardScaler() \n",
    "# loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df_temp = pd.read_csv(f, sep=\";\")\n",
    "    df_temp = utils.drop_cols(df_temp)\n",
    "    df_temp = df_temp.dropna()\n",
    "    df_temp = utils.remove_strings(df_temp)\n",
    "    df_temp = utils.subsample(df_temp, 10)\n",
    "    df_temp = utils.normalize(df_temp, scaler, phase1 = True)  \n",
    "      \n",
    "    # print the content\n",
    "    df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase 1 preload\n",
    "phase 2 und 3 sind afterload\n",
    "phase 4 ist speedramp\n",
    "phase 10:\n",
    "        contractility = 1 -> nichts\n",
    "        contractility = 3 -> Dubotamin \n",
    "        contractility = 4 -> ischaemie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 3. 6. 2. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Phasenzuordnung'] == 1:\n",
    "        df.at[index, 'intervention'] = 0\n",
    "    elif row['intervention'] == 10:\n",
    "        if row['contractility'] == 1.0:\n",
    "            df.at[index, 'intervention'] = 0      # contractility = 1.0 - could be ignored? - phase 0?\n",
    "        if row['contractility'] == 3.0:\n",
    "            df.at[index, 'intervention'] = 5      # contractility = 3.0                                        \n",
    "        if row['contractility'] == 4.0:\n",
    "            df.at[index, 'intervention'] = 6    # contractility = 4.0\n",
    "\n",
    "#get unique intervention\n",
    "print(df['intervention'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select part of data to use in experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset after selecting Phasenzuordnung 1 (751120, 14)\n",
      "Size of the dataset with data from phase 1 (751120, 14)\n",
      "Size of Phase 1:  (751120, 14)\n",
      "Size of Phase 2:  (0, 14)\n",
      "Size of Phase 3:  (0, 14)\n",
      "Size of Phase 4:  (0, 14)\n",
      "SIze of Phase 5:  (0, 14)\n"
     ]
    }
   ],
   "source": [
    "# select Phasenzuordnung = 1 and 3\n",
    "df = df.loc[df['Phasenzuordnung'].isin(PHASES)]\n",
    "print('Size of the dataset after selecting Phasenzuordnung 1',df.shape)\n",
    "\n",
    "print('Size of the dataset with data from phase 1',df.shape)\n",
    "print('Size of Phase 1: ', df.loc[df['Phasenzuordnung'] == 1].shape)\n",
    "print('Size of Phase 2: ', df.loc[df['Phasenzuordnung'] == 2].shape)\n",
    "print('Size of Phase 3: ', df.loc[df['Phasenzuordnung'] == 3].shape)\n",
    "print('Size of Phase 4: ', df.loc[df['Phasenzuordnung'] == 4].shape)\n",
    "print('SIze of Phase 5: ', df.loc[df['Phasenzuordnung'] == 5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intervention'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop animals with less than 10 data points\n",
    "We drop animals from the dataframe, if they have less than 10 data points. Initially, we have 56 animals and after dropping those with close to no data points, we are left with 25 animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Number of animals after removing those with less than 10 data points:  25\n",
      "[  8  12  13 108   7  14  11   2  16  10 105 111  17  19   3  15   5  20\n",
      " 113   4 102   1   9 107 101]\n"
     ]
    }
   ],
   "source": [
    "print(len(df['animal'].unique()))\n",
    "# remove animals with less than 10 data points\n",
    "df = df.groupby('animal').filter(lambda x: len(x) > 10)\n",
    "print('Number of animals after removing those with less than 10 data points: ', len(df['animal'].unique()))\n",
    "\n",
    "# get all differnent animals\n",
    "animals = df['animal'].unique()\n",
    "print(animals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test split\n",
    "\n",
    "The 5 test animals represent 20.20924% of the whole data. If we don't use all phases, this number might be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test animal(s): [3, 4, 8, 11, 17]\n",
      "\n",
      "Different animal IDs after removing those that are in the test dataset:  20\n",
      "\n",
      "Train data shape: (591930, 14)\n",
      "\n",
      "Test data shape: (158976, 14)\n",
      "\n",
      "The test dataset is 21.171225160006713 percent of the whole data: \n"
     ]
    }
   ],
   "source": [
    "# select animals 3,4,8,11,17 as test animals\n",
    "test_animals = [3,4,8,11,17] \n",
    "\n",
    "print('\\nTest animal(s):', test_animals)\n",
    "\n",
    "all_animals = df['animal'].unique()\n",
    "# remove test animals from train animals\n",
    "train_animals =  [x for x in all_animals if x not in test_animals]\n",
    "\n",
    "# test data\n",
    "df_test = df[df['animal'].isin(test_animals)]\n",
    "\n",
    "# change the length of the test data to a multiple of the Window size\n",
    "df_test = df_test.iloc[:len(df_test) - (len(df_test) % WINDOW)]\n",
    "\n",
    "# train dataframe with only animals from train_animals\n",
    "df_train = df[df['animal'].isin(train_animals)]\n",
    "print('\\nDifferent animal IDs after removing those that are in the test dataset: ',len(df_train['animal'].unique()))\n",
    "\n",
    "\n",
    "print('\\nTrain data shape:', df_train.shape)\n",
    "print('\\nTest data shape:', df_test.shape)\n",
    "\n",
    "# lengt of df_train\n",
    "print('\\nThe test dataset is {} percent of the whole data: '.format((len(df_test)/(len(df_train) + len(df_test))) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #SEEDS \n",
    "# seed_1 = 1\n",
    "# seed_2 = 2\n",
    "# seed_3 = 3\n",
    "\n",
    "# #split train dataset into train and validation split\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=seed_2, shuffle=False)\n",
    "\n",
    "# print(df_train.shape)\n",
    "# print(df_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Generator and Discriminator\n",
    "\n",
    "We also initialize the 2 optimizers, the 2 Learning rate schedulers, the losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try out InstanceNorm1d instead of BatchNorm1d\n",
    "\n",
    "# def residual_block(in_channels, out_channels):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv1d(in_channels, out_channels, kernel_size=1, stride = 1, padding=0),\n",
    "#         nn.BatchNorm1d(out_channels),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Conv1d(out_channels, out_channels, kernel_size = 1, stride = 1, padding=0),\n",
    "#         nn.BatchNorm1d(out_channels),\n",
    "#     )\n",
    "\n",
    "# def single_conv_pad(in_channels, out_channels):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "#         nn.BatchNorm1d(out_channels),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Dropout1d(p=0.1, inplace=False),\n",
    "#     )\n",
    "\n",
    "# class ResNetGenerator(nn.Module):\n",
    "#     def __init__(self, INPUTCHANNELS, OUTPUTCHANNELS, blocks=6):\n",
    "#         super(ResNetGenerator, self).__init__()\n",
    "#         '''\n",
    "#         A Basic Unet Generator without skip connections\n",
    "#         '''\n",
    "#         self.blocks = blocks\n",
    "\n",
    "#         self.maxpool = nn.MaxPool1d((2))  \n",
    "\n",
    "#         self.down_conv1 = single_conv_pad(INPUTCHANNELS, 64) \n",
    "#         self.down_conv2 = single_conv_pad(64, 128)\n",
    "#         self.down_conv3 = single_conv_pad(128, 256)\n",
    "\n",
    "#         self.residual1 = residual_block(256, 256)\n",
    "#         self.residual2 = residual_block(256, 256)\n",
    "#         self.residual3 = residual_block(256, 256)\n",
    "#         self.residual4 = residual_block(256, 256)\n",
    "#         self.residual5 = residual_block(256, 256)\n",
    "#         self.residual6 = residual_block(256, 256)\n",
    "#         self.residual7 = residual_block(256, 256)\n",
    "#         self.residual8 = residual_block(256, 256)\n",
    "#         self.residual9 = residual_block(256, 256)\n",
    "\n",
    "#         self.up_trans1 = nn.ConvTranspose1d(256, 128, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv1 = single_conv_pad(128, 128)\n",
    "#         self.up_trans2 = nn.ConvTranspose1d(128, 64, kernel_size=(2), stride=2, padding=0)\n",
    "#         self.up_conv2 = single_conv_pad(64, 64)\n",
    "\n",
    "#         self.out = nn.Conv1d(64, OUTPUTCHANNELS, kernel_size=1) # kernel_size must be == 1\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # [Batch size, Channels in, Height, Width]\n",
    "#         x1 = self.down_conv1(input) \n",
    "#         x2 = self.maxpool(x1) \n",
    "#         x3 = self.down_conv2(x2)\n",
    "#         x4 = self.maxpool(x3) \n",
    "#         x5 = self.down_conv3(x4) \n",
    "\n",
    "#         # residual blocks\n",
    "#         x5 = x5 + self.residual1(x5)\n",
    "#         x5 = x5 + self.residual2(x5)\n",
    "#         x5 = x5 + self.residual3(x5)\n",
    "#         if self.blocks > 3:\n",
    "#             x5 = x5 + self.residual4(x5)\n",
    "#             x5 = x5 + self.residual5(x5)\n",
    "#             x5 = x5 + self.residual6(x5)           \n",
    "#         if self.blocks > 6:\n",
    "#             x5 = x5 + self.residual7(x5)\n",
    "#             x5 = x5 + self.residual8(x5)\n",
    "#             x5 = x5 + self.residual9(x5)  # x5.shape =  torch.Size([1, 256, 64])\n",
    "\n",
    "#         # # decoder\n",
    "#         x = self.up_trans1(x5)\n",
    "#         x = self.up_conv1(x)\n",
    "#         x = self.up_trans2(x)\n",
    "#         x = self.up_conv2(x)\n",
    "#         x = self.out(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "if UNET:\n",
    "    if not SKIPCONNECTIONS:\n",
    "        gen_target = BasicGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "        gen_source = BasicGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "    if SKIPCONNECTIONS:\n",
    "        gen_target = SkipConGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1).to(DEVICE)\n",
    "        gen_source = SkipConGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS).to(DEVICE)\n",
    "if RESNET:\n",
    "    gen_target = ResNetGenerator(INPUTCHANNELS = CHANNELS, OUTPUTCHANNELS = 1, blocks=BLOCKS).to(DEVICE)\n",
    "    gen_source = ResNetGenerator(INPUTCHANNELS = 1, OUTPUTCHANNELS = CHANNELS, blocks=BLOCKS).to(DEVICE)\n",
    "\n",
    "\n",
    "# gen_target receives source signals and generates a target signal\n",
    "# gen_source receives a target signal and generates source signals\n",
    "\n",
    "# Discriminator\n",
    "if PATCH:\n",
    "    disc_target = PatchDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = PatchDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "if not PATCH:\n",
    "    disc_target = SampleDiscriminator(CHANNELS = 1).to(DEVICE)\n",
    "    disc_source = SampleDiscriminator(CHANNELS = CHANNELS).to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = torch.optim.AdamW(                                         \n",
    "    list(disc_source.parameters()) + list(disc_target.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    ")\n",
    "opt_gen = torch.optim.AdamW(\n",
    "    list(gen_source.parameters()) + list(gen_target.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "# Schedulers\n",
    "if ReduceLROnPlateau:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_gen,\n",
    "                                                            factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                            min_lr=1e-6,\n",
    "                                                        )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = opt_disc,\n",
    "                                                                factor=FACTOR, patience=PATIENCE, threshold=1e-4,\n",
    "                                                                min_lr=1e-6,\n",
    "                                                        )\n",
    "if MultiStepLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_gen, milestones=[1], gamma=GAMMA)\n",
    "                                                        \n",
    "    disc_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer = opt_disc, milestones=[1], gamma=GAMMA)\n",
    "\n",
    "if PolinomialLR:\n",
    "    gen_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_gen,\n",
    "                                                      total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                      power = power,\n",
    "                                                    )\n",
    "    disc_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer = opt_disc,\n",
    "                                                       total_iters = NUM_EPOCHS-LR_DECAY_AFTER_EPOCH, \n",
    "                                                       power = power,\n",
    "                                                    )\n",
    "\n",
    "# losses\n",
    "l1 = nn.L1Loss() \n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT_GEN_target = 'Checkpoints/Basic_Generator/Epo200_Dec100/gen_target_BG_200_LRD100.pth.tar'\n",
    "# CHECKPOINT_GEN_source = 'Checkpoints/Basic_Generator/Epo200_Dec100/gen_source_BG_200_LRD100.pth.tar'\n",
    "\n",
    "# utils.load_checkpoint(CHECKPOINT_GEN_target, gen_target, opt_gen, LEARNING_RATE)\n",
    "# utils.load_checkpoint(CHECKPOINT_GEN_source, gen_source, opt_gen, LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UNPAIRED:\n",
    "    # create datasets without information of the phases and interventions (embedding information)\n",
    "    train_dataset = UnpairedDataset(df_train, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "    test_dataset = AnimalDataset(df_test, source_signals, target_name = TARGET, test = True, window_length = WINDOW)\n",
    "    #validation_dataset = UnpairedDataset(df_val, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "    \n",
    "\n",
    "if not UNPAIRED:\n",
    "    # create datasets without the embedding information\n",
    "    train_dataset = AnimalDataset(df_train, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "    test_dataset = AnimalDataset(df_test, source_signals, target_name = TARGET, test = True, window_length = WINDOW)\n",
    "    #validation_dataset = AnimalDataset(df_val, source_signals, target_name = TARGET, test = False, window_length = WINDOW)\n",
    "\n",
    "# DataLoader\n",
    "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)\n",
    "#validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohann4dl\u001b[0m (\u001b[33mjohannanton\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/johann/Desktop/GitHub repos/Master_thesis/wandb/run-20230803_023016-cp6fw967</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johannanton/Cycle_GAN/runs/cp6fw967' target=\"_blank\">exalted-dream-300</a></strong> to <a href='https://wandb.ai/johannanton/Cycle_GAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johannanton/Cycle_GAN' target=\"_blank\">https://wandb.ai/johannanton/Cycle_GAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johannanton/Cycle_GAN/runs/cp6fw967' target=\"_blank\">https://wandb.ai/johannanton/Cycle_GAN/runs/cp6fw967</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/johannanton/Cycle_GAN/runs/cp6fw967?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fecdfe02280>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Cycle_GAN\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc, reals, fakes):\n",
    "    # calculate how close reals are to being classified as real\n",
    "    real_loss = mse(disc(reals), torch.ones_like(disc(reals)))\n",
    "    # calculate how close fakes are to being classified as fake\n",
    "    fake_loss = mse(disc(fakes), torch.zeros_like(disc(fakes)))\n",
    "    # return the average of real and fake loss\n",
    "    return (real_loss + fake_loss)\n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_disc_loss_no_embedding(sig_A, sig_B,  \n",
    "                    disc_A, disc_B, gen_A, gen_B, fake_A, fake_B\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    \"\"\"\n",
    "    # discriminator loss\n",
    "    disc_B_loss = discriminator_loss(disc_B, sig_B, fake_B) * LAMBDA_DISC\n",
    "    disc_A_loss = discriminator_loss(disc_A, sig_A, fake_A) * LAMBDA_DISC\n",
    "    disc_loss = (disc_A_loss + disc_B_loss)  \n",
    "\n",
    "    return disc_loss, disc_A_loss, disc_B_loss\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def calc_gen_loss_no_embedding(source, target, \n",
    "                  gen_source, gen_target, disc_source, disc_target, fake_target, fake_source\n",
    "                  ):\n",
    "    loss = 0\n",
    "\n",
    "    if CYCLE:\n",
    "        rec_target = gen_target(fake_source)\n",
    "        rec_source = gen_source(fake_target)\n",
    "        cycle_target_loss = l1(target, rec_target)  # l1 loss in cycle GAN paper\n",
    "        cycle_source_loss = l1(source, rec_source)  \n",
    "\n",
    "        loss += cycle_target_loss * LAMBDA_CYCLE + cycle_source_loss * LAMBDA_CYCLE\n",
    "    else:\n",
    "        cycle_target_loss = torch.tensor(0)\n",
    "        cycle_source_loss = torch.tensor(0)\n",
    "\n",
    "    if GAN_LOSS:\n",
    "        g_source_loss = mse(disc_source(fake_source), torch.ones_like(disc_source(fake_source))) # optimally, disc outputs 1 for a fake signal\n",
    "        g_target_loss = mse(disc_target(fake_target), torch.ones_like(disc_target(fake_target))) \n",
    "\n",
    "        loss += g_source_loss * LAMBDA_GAN + g_target_loss * LAMBDA_GAN\n",
    "    else:\n",
    "        g_source_loss = torch.tensor(0)\n",
    "        g_target_loss = torch.tensor(0)\n",
    "\n",
    "    if SUPERVISED:\n",
    "        sup_source_loss = mse(source, fake_source)\n",
    "        sup_target_loss = mse(target, fake_target)\n",
    "\n",
    "        loss += sup_source_loss * LAMBDA_SUPERVISED + sup_target_loss * LAMBDA_SUPERVISED\n",
    "    else:\n",
    "        sup_source_loss = torch.tensor(0)\n",
    "        sup_target_loss = torch.tensor(0)\n",
    "\n",
    "    if IDENTITY:\n",
    "        id_target_loss = l1(target, gen_target(target))\n",
    "        id_source_loss = l1(source, gen_source(source))\n",
    "\n",
    "        loss += id_target_loss * LAMBDA_IDENTITY + id_source_loss * LAMBDA_IDENTITY\n",
    "    else:\n",
    "        id_target_loss = torch.tensor(0)\n",
    "        id_source_loss = torch.tensor(0)\n",
    "\n",
    "    return loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'L1 loss source' : [],\n",
    "    'L1 loss target' : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [3:52:13<00:00, 69.67s/it]  \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    for source, target in loader:\n",
    "        # convert to float16\n",
    "        source = source.float() # neccessary to prevent error: \"Input type (torch.cuda.DoubleTensor) \n",
    "        target = target.float() # and weight type (torch.cuda.HalfTensor) should be the same\"\n",
    "    \n",
    "        # move to GPU\n",
    "        source = source.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        #  ------------------------------- #\n",
    "        #  ----- train discriminators ---- #\n",
    "        #  ------------------------------- #\n",
    "        with torch.no_grad():\n",
    "            fake_target = gen_target(source).detach()\n",
    "            fake_source = gen_source(target).detach()\n",
    "\n",
    "        d_loss, disc_source_loss, disc_target_loss = get_disc_loss_no_embedding(source, target,\n",
    "                                                            disc_source, disc_target, gen_source, gen_target, \n",
    "                                                            fake_source, fake_target\n",
    "                                                        )\n",
    "\n",
    "        # update gradients of discriminator \n",
    "        opt_disc.zero_grad() \n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "        # d_scaler.scale(d_loss).backward()  \n",
    "               \n",
    "\n",
    "        # -------------------------------- #\n",
    "        # ------- train generators ------- #\n",
    "        # -------------------------------- # \n",
    "\n",
    "        out = calc_gen_loss_no_embedding(source, target,\n",
    "                                gen_source, gen_target, disc_source, disc_target, \n",
    "                                fake_target, fake_source\n",
    "                                )\n",
    "        g_loss, g_source_loss, g_target_loss, cycle_target_loss, cycle_source_loss, id_target_loss, id_source_loss, sup_source_loss, sup_target_loss = out\n",
    "\n",
    "        # update gradients of generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        # g_scaler.scale(g_loss).backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    # d_scaler.step(opt_disc)  \n",
    "    # d_scaler.update()\n",
    "\n",
    "    # g_scaler.step(opt_gen) \n",
    "    # g_scaler.update()\n",
    "\n",
    "    wandb.log({'Train/Discriminator A loss': disc_source_loss.item(),\n",
    "                'Train/Discriminator B loss': disc_target_loss.item(),\n",
    "                'Train/Total Discriminator loss': d_loss.item(),\n",
    "                'Train/Total Generator loss': g_loss.item(),\n",
    "                'Train/Adversarial loss A': g_source_loss.item(),\n",
    "                'Train/Adversarial loss B': g_target_loss.item(),\n",
    "                'Train/Cycle consistency loss A': cycle_source_loss.item(),\n",
    "                'Train/Cycle consistency loss B': cycle_target_loss.item(),\n",
    "                'Train/Supervised loss A': sup_source_loss.item(),\n",
    "                'Train/Supervised loss B': sup_target_loss.item(),\n",
    "                'Learning rate': opt_gen.param_groups[0][\"lr\"],\n",
    "                # 'Train/Identity loss A': id_A_loss.item(),\n",
    "                # 'Train/Identity loss B': id_B_loss.item()\n",
    "                })\n",
    "        \n",
    "\n",
    "    # ------------------------ #\n",
    "    # ------ Validation ------ #\n",
    "    # ------------------------ #\n",
    "\n",
    "    if (epoch+1) % 25 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set models to evaluation mode\n",
    "            disc_source.eval()  # set discriminator to evaluation mode\n",
    "            disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "            gen_target.eval()\n",
    "            gen_source.eval()\n",
    "\n",
    "            for source, target in test_loader:\n",
    "                # convert to float16\n",
    "                source = source.float()\n",
    "                target = target.float()\n",
    "\n",
    "                # move to GPU\n",
    "                source = source.to(DEVICE)\n",
    "                target = target.to(DEVICE)\n",
    "\n",
    "                fake_target = gen_target(source).detach() # already torch.no_grad()\n",
    "                fake_source = gen_source(target).detach()\n",
    "                \n",
    "\n",
    "                # generate signals during validation\n",
    "                #gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "                # calculate l1 loss of fake signals and real signals\n",
    "                test_real_fake_lossTarget = l1(target, fake_target)   # l1(sig_B, fake_B)\n",
    "                test_real_fake_lossSource = l1(source, fake_source)\n",
    "                # add to dict losses\n",
    "                losses['L1 loss source'].append(test_real_fake_lossSource.item())\n",
    "                losses['L1 loss target'].append(test_real_fake_lossTarget.item())\n",
    "\n",
    "                #  ------------------------------- #\n",
    "                #  ----- test discriminators ----- #\n",
    "                #  ------------------------------- #\n",
    "\n",
    "                test_d_loss, test_disc_source_loss, test_disc_target_loss = get_disc_loss_no_embedding(source, target, \n",
    "                                                                    disc_source, disc_target, gen_source, gen_target, \n",
    "                                                                    fake_source, fake_target\n",
    "                                                                )\n",
    "                \n",
    "                # -------------------------------- #\n",
    "                # ------- test generators -------- #\n",
    "                # -------------------------------- # \n",
    "\n",
    "                out = calc_gen_loss_no_embedding(source, target,\n",
    "                                        gen_source, gen_target, disc_source, disc_target, \n",
    "                                        fake_target, fake_source\n",
    "                                        )\n",
    "                g_lossT, g_source_lossT, g_target_lossT, cycle_target_lossT, cycle_source_lossT, id_target_lossT, id_source_lossT, sup_source_lossT, sup_target_lossT = out\n",
    "        \n",
    "                # gen_signals(fake_target, fake_source, target, source)\n",
    "\n",
    "            wandb.log({'Test/Generator loss': g_lossT.item(),\n",
    "                        'Test/Discriminator loss': test_d_loss.item(),\n",
    "                        'Test/L1 loss between real signal A and fake signals A': test_real_fake_lossSource.item(),\n",
    "                        'Test/L1 loss between real signal B and fake signals B': test_real_fake_lossTarget.item(),\n",
    "                        'Test/Discriminator A loss': test_disc_source_loss.item(),\n",
    "                        'Test/Discriminator B loss': test_disc_target_loss.item(),\n",
    "                        'Test/Adversarial or GAN loss A': g_source_lossT.item(),\n",
    "                        'Test/Adversarial or GAN loss B': g_target_lossT.item(),\n",
    "                        'Test/Cycle consistency loss A': cycle_source_lossT.item(),\n",
    "                        'Test/Cycle consistency loss B': cycle_target_lossT.item(),\n",
    "                        'Test/Supervised loss A': sup_source_lossT.item(),\n",
    "                        'Test/Supervised loss B': sup_target_lossT.item(),\n",
    "                        'Test/Epoch': epoch+1,\n",
    "                })\n",
    "            \n",
    "            \n",
    "    if ReduceLROnPlateau:  \n",
    "        disc_scheduler.step(d_loss) #d_loss # test_d_loss\n",
    "        gen_scheduler.step(g_loss)  # g_loss # g_lossT\n",
    "\n",
    "    if MultiStepLR:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n",
    "\n",
    "    # scheduler step if epoch > LR_DECAY_AFTER_EPOCH\n",
    "    if PolinomialLR == True and (epoch+1) >= LR_DECAY_AFTER_EPOCH:\n",
    "        disc_scheduler.step()\n",
    "        gen_scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get losses of the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average L1 loss of source signals: 1.6599957251587283\n",
      "Average L1 loss of target signals: 3.727595023654389\n",
      "Standard deviation of source losses 0.14800233531228296\n",
      "Standard deviation of target losses 0.7940028591145285\n"
     ]
    }
   ],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "\n",
    "disc_source.eval()  # set discriminator to evaluation mode\n",
    "disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "gen_target.eval()\n",
    "gen_source.eval()\n",
    "\n",
    "for source, target, in test_loader:                                  \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source)\n",
    "    fake_source = gen_source(target)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "#get stadard deviation of loses\n",
    "print('Standard deviation of source losses', np.std(source_losses))\n",
    "print('Standard deviation of target losses', np.std(target_losses))\n",
    "\n",
    "# # get min and max of losses\n",
    "# min_max_loss_source = np.(source_losses), np.max(source_losses)\n",
    "# min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "# print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "# print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# # get median of losses\n",
    "# median_loss_source = np.median(source_losses)\n",
    "# median_loss_target = np.median(target_losses)\n",
    "# print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "# print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate signals with the trained generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate signals\n",
    "idx = 0    \n",
    "\n",
    "disc_source.eval()  # set discriminator to evaluation mode\n",
    "disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "gen_target.eval()\n",
    "gen_source.eval()\n",
    "\n",
    "for source, target, in test_loader:\n",
    "    if idx == 5:\n",
    "        break                 \n",
    "                    \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source)\n",
    "    fake_source = gen_source(target)\n",
    "    # fake_target = gen_source(gen_target(source))\n",
    "    # fake_source = gen_target(gen_source(target))\n",
    "                        \n",
    "    fake_target = fake_target.reshape(-1)\n",
    "    fake_source = fake_source.reshape(-1)\n",
    "    source = source.reshape(-1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    ax[0].plot(source[:256].cpu().detach().numpy(), label= 'Real source signals')\n",
    "    ax[0].plot(fake_source[:256].cpu().detach().numpy(), label= 'Recreated source signals')\n",
    "    ax[0].set_xlabel('Signal length')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(target[:256].cpu().detach().numpy(), label= 'Real target signal')\n",
    "    ax[1].plot(fake_target[:256].cpu().detach().numpy(), label= 'Recreated target signal')\n",
    "    ax[1].set_xlabel('Signal length')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    idx += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify L1 loss of reconstructed and real signals, per phase and intervention\n",
    "# array([ 0.,  1.,  3., 10.,  2.,  4.,  9.])\n",
    "# df of phase 1\n",
    "df_phase_1 = df_test.loc[df_test['Phasenzuordnung'].isin([1])]\n",
    "df_phase_3 = df_test.loc[df_test['Phasenzuordnung'].isin([3])]\n",
    "\n",
    "# 1.,  3., 10.,  2.,  4.\n",
    "df_int_0 = df_test.loc[df_test['intervention'].isin([0])]\n",
    "df_int_1 = df_test.loc[df_test['intervention'].isin([1])]\n",
    "df_int_2 = df_test.loc[df_test['intervention'].isin([2])]\n",
    "df_int_3 = df_test.loc[df_test['intervention'].isin([3])]\n",
    "df_int_4 = df_test.loc[df_test['intervention'].isin([4])]\n",
    "df_int_9 = df_test.loc[df_test['intervention'].isin([9])]\n",
    "df_int_10 = df_test.loc[df_test['intervention'].isin([10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_dataset = AnimalDataset(df_phase_1, source_signals, target_name = TARGET, test = True, window_length = WINDOW)\n",
    "verify_loader = DataLoader(verify_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_losses = []\n",
    "target_losses = []\n",
    "\n",
    "disc_source.eval()  # set discriminator to evaluation mode\n",
    "disc_target.eval()  # turns off Dropouts Layers, BatchNorm Layers etc\n",
    "gen_target.eval()\n",
    "gen_source.eval()\n",
    "\n",
    "for source, target, in verify_loader:                                  \n",
    "    source = source.float()\n",
    "    target = target.float()\n",
    "    source = source.to(DEVICE)\n",
    "    target = target.to(DEVICE)\n",
    "\n",
    "    fake_target = gen_target(source)\n",
    "    fake_source = gen_source(target)\n",
    "    l1_source = l1(source, fake_source)\n",
    "    l1_target = l1(target, fake_target)\n",
    "    source_losses.append(l1_source.item())\n",
    "    target_losses.append(l1_target.item())\n",
    "\n",
    "print(f\"Average L1 loss of source signals: {np.mean(source_losses)}\")\n",
    "print(f\"Average L1 loss of target signals: {np.mean(target_losses)}\")\n",
    "\n",
    "# get min and max of losses\n",
    "min_max_loss_source = np.min(source_losses), np.max(source_losses)\n",
    "min_max_loss_target = np.min(target_losses), np.max(target_losses)\n",
    "print(f\"Min and max loss of source signals: {min_max_loss_source}\")\n",
    "print(f\"Min and max loss of target signals: {min_max_loss_target}\")\n",
    "\n",
    "# get median of losses\n",
    "median_loss_source = np.median(source_losses)\n",
    "median_loss_target = np.median(target_losses)\n",
    "print(f\"Median loss of source signals: {median_loss_source}\")\n",
    "print(f\"Median loss of target signals: {median_loss_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoints\n",
    "\n",
    "# utils.save_checkpoint(gen_source, opt_gen, path=\"phase_1_3_Basic_Gen.pth.tar\")\n",
    "# utils.save_checkpoint(gen_target, opt_gen, path=\"phase_1_3_Basic_Gen.pth.tar\")\n",
    "# utils.save_checkpoint(disc_source, opt_disc, path=\"phase_1_3_Basic_Gen.pth.tar\")\n",
    "# utils.save_checkpoint(disc_target, opt_disc, path=\"phase_1_3_Basic_Gen.pth.tar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9de897a1f02868636d0ac53130d687147b532c1438896437dda8e287739e6223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
